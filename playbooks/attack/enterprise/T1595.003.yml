name: T1595.003: Wordlist Scanning
id: a5b8c9d0-e1f2-4a3b-8c7d-6e5f4a3b2c1d
description: This playbook is designed to detect adversary reconnaissance techniques involving wordlist scanning against an organization's perimeter. It focuses on identifying several key indicators: inbound connections from IP addresses on threat intelligence feeds, the use of common scanning tool user-agents, anomalous rates of HTTP errors from a single source, high volumes of DNS queries for cloud storage endpoints, and systematic iteration of URL query parameters. The goal is to answer the primary investigative requirement 'Is the adversary performing reconnaissance against our perimeter using wordlist scanning?' by providing concrete queries and analytical methods.
type: technique
related:
- TA0043: Reconnaissance
contributors:
- Zachary Szewczyk
- Ask Sage
created: 2025-10-01
modified: 2025-10-01
version: 1.0
tags: none
questions:
- question: Are inbound connections originating from IP addresses known to be malicious based on threat intelligence feeds?
  context: |
    This question aims to identify initial contact from known adversaries, scanners, or botnets by cross-referencing inbound source IP addresses with a threat intelligence database. A match provides a high-confidence signal of malicious activity, allowing for rapid detection of reconnaissance efforts before a more significant attack occurs.
  answer_sources:
  - Zeek conn.log
  - Zeek http.log
  - Network egress/ingress points (e.g., Firewalls, Edge Routers)
  - Web Application Firewalls (WAFs)
  - Reverse Proxies
  range: last 90 days
  queries:
  - technology: pseudocode
    query: |
      FOR each event IN (zeek_conn_log, zeek_http_log):
        IF event.source_ip IS IN known_malicious_ips_feed:
          GENERATE high_severity_alert
- question: For an IP address already flagged by a threat feed, does its connection behavior exhibit an unusually high ratio of failed to successful HTTP requests?
  context: |
    This question adds a layer of behavioral analysis to a threat intelligence match. An adversary performing wordlist scanning will likely generate many failed requests (e.g., HTTP 404s) as they probe for non-existent files or directories. A high failure ratio for a known malicious IP strengthens the evidence that it is actively scanning, helping to prioritize the alert over more passive or erroneous threat feed matches.
  answer_sources:
  - Zeek conn.log
  - Zeek http.log
  - Network egress/ingress points (e.g., Firewalls, Edge Routers)
  - Web Application Firewalls (WAFs)
  - Reverse Proxies
  range: last 90 days
  queries:
  - technology: pseudocode
    query: |
      FOR each source_ip IN known_malicious_ips_feed:
        CALCULATE ratio = (count_failed_http_requests / count_successful_http_requests)
        CALCULATE percentile_threshold = 98th_percentile_of_ratio(all_external_traffic, last_30_days)
        IF ratio > percentile_threshold:
          ESCALATE alert_priority
- question: Can we classify an external IP address as malicious using a machine learning model based on its connection patterns and threat intelligence status?
  context: |
    This question proposes a more advanced, predictive approach. A machine learning model can learn complex patterns that distinguish malicious scanners from benign traffic, even for IPs not yet on a threat list. By analyzing features like connection frequency, data volume, and protocol usage in concert, the model can identify emerging threats and provide a probabilistic score of maliciousness, enabling automated and nuanced threat detection.
  answer_sources:
  - Zeek conn.log
  - Zeek http.log
  - Network egress/ingress points (e.g., Firewalls, Edge Routers)
  - Web Application Firewalls (WAFs)
  - Reverse Proxies
  range: last 90 days
  queries:
  - technology: pseudocode
    query: |
      FOR each new_connection from source_ip:
        EXTRACT features (on_threat_list, conn_frequency, bytes_transferred, protocol_dist)
        PREDICT probability = classification_model.predict(features)
        IF probability > 0.9:
          TRIGGER investigation
- question: Are inbound HTTP requests using User-Agent strings associated with known web scanning or content discovery tools?
  context: |
    This question seeks to identify attackers using off-the-shelf scanning tools. These tools often use default, identifiable User-Agent strings (e.g., 'gobuster', 'nikto', 'sqlmap'). Matching these strings in web server logs is a simple and effective method for detecting automated reconnaissance and vulnerability scanning activities.
  answer_sources:
  - Zeek http.log
  - Microsoft-IIS-Logging
  - Public-facing web servers
  - Application load balancers
  - API Gateways
  range: last 90 days
  queries:
  - technology: pseudocode
    query: |
      DEFINE scanner_ua_list = ['gobuster', 'dirb', 'ffuf', 'nikto', 'sqlmap']
      FOR each http_request IN (zeek_http_log, iis_logs):
        IF http_request.user_agent CONTAINS ANY IN scanner_ua_list:
          GENERATE medium_severity_alert
- question: Are we observing new, never-before-seen User-Agent strings associated with a high request rate from a single source IP?
  context: |
    This question helps detect attackers who modify their User-Agent string to evade simple signature-based detection. By flagging any new User-Agent string combined with a high request rate, this method can identify suspicious clients that are not behaving like normal web browsers or benign scripts, pointing to a potential scanning tool in use.
  answer_sources:
  - Zeek http.log
  - Microsoft-IIS-Logging
  - Public-facing web servers
  - Application load balancers
  - API Gateways
  range: last 90 days
  queries:
  - technology: pseudocode
    query: |
      MAINTAIN baseline_user_agents = unique_user_agents(last_30_days)
      FOR each http_request IN (zeek_http_log, iis_logs):
        IF http_request.user_agent NOT IN baseline_user_agents:
          CALCULATE request_rate = count_requests(http_request.source_ip, last_second)
          IF request_rate > 10:
            GENERATE alert
- question: Can a Natural Language Processing (NLP) model classify User-Agent strings as malicious even if they are not on a known-bad list?
  context: |
    This question explores using advanced ML to analyze the structure and characters of a User-Agent string itself. Attackers may randomize or craft custom User-Agents to appear legitimate. An NLP model can learn the subtle characteristics of malicious scanner agents versus benign ones, allowing for the detection of novel or obfuscated tools that would be missed by signature or frequency analysis.
  answer_sources:
  - Zeek http.log
  - Microsoft-IIS-Logging
  - Public-facing web servers
  - Application load balancers
  - API Gateways
  range: last 90 days
  queries:
  - technology: pseudocode
    query: |
      FOR each new_http_request IN (zeek_http_log, iis_logs):
        PREDICT classification = nlp_model.classify(new_http_request.user_agent)
        IF classification == 'malicious-scanner':
          GENERATE alert
- question: Is any single external IP address generating a high volume of HTTP 404 or 403 errors in a short time window?
  context: |
    This question aims to detect brute-force directory or file enumeration. A scanner using a wordlist will inevitably request many non-existent resources, resulting in a large number of 'Not Found' (404) or 'Forbidden' (403) errors. A simple threshold-based rule can effectively catch this common and noisy scanning behavior.
  answer_sources:
  - Zeek http.log
  - Microsoft-IIS-Logging
  - Public-facing web servers
  - API Gateways
  - Cloud Storage Buckets
  range: last 90 days
  queries:
  - technology: pseudocode
    query: |
      FOR each 1_minute_window:
        GROUP requests by source_ip
        FOR each source_ip:
          CALCULATE error_count = count_requests_where_status_is_(404, 403)
          IF error_count > 100:
            GENERATE alert
- question: Is the ratio of HTTP errors to total requests from a single source IP statistically anomalous compared to a dynamic, long-term baseline for the targeted server?
  context: |
    This question refines the simple error count by using a dynamic baseline. Different web applications have different normal error rates. By comparing a source IP's recent error ratio to the historical norm for that specific destination host, this method reduces false positives from naturally 'error-prone' applications and more accurately identifies behavior that deviates significantly (e.g., 3 standard deviations) from expected patterns.
  answer_sources:
  - Zeek http.log
  - Microsoft-IIS-Logging
  - Public-facing web servers
  - API Gateways
  - Cloud Storage Buckets
  range: last 90 days
  queries:
  - technology: pseudocode
    query: |
      FOR each destination_host:
        CALCULATE baseline_error_ratio_mean, baseline_error_ratio_stddev (last_30_days)
      FOR each source_ip over 5_minute_window:
        CALCULATE current_error_ratio
        IF current_error_ratio > (baseline_error_ratio_mean + 3 * baseline_error_ratio_stddev):
          GENERATE alert
- question: Can a time-series anomaly detection model identify unexpected spikes in HTTP 404 errors originating from a single source IP?
  context: |
    This question applies machine learning to model the 'normal' rhythm of errors over time for each source-destination pair. A time-series model can account for seasonality (e.g., time of day, day of week) and trends, making it highly effective at spotting sudden, sharp increases in errors that are characteristic of a scanning tool kicking off an operation. This is more robust than static thresholds or simple averages.
  answer_sources:
  - Zeek http.log
  - Microsoft-IIS-Logging
  - Public-facing web servers
  - API Gateways
  - Cloud Storage Buckets
  range: last 90 days
  queries:
  - technology: pseudocode
    query: |
      FOR each (source_ip, destination_host) pair:
        MODEL error_counts_over_time with time_series_anomaly_detection
        IF current_error_count IS flagged as anomalous by model:
          GENERATE alert
- question: Is an external IP performing a high volume of DNS lookups for cloud storage endpoints that contain our organization's keywords?
  context: |
    This question targets a specific reconnaissance technique where adversaries try to guess the names of cloud storage buckets or containers (e.g., S3 buckets, Azure blobs) by combining common organizational keywords with cloud service domains. A high frequency of such DNS queries from one source is a strong indicator of an attempt to discover publicly exposed cloud assets.
  answer_sources:
  - Zeek dns.log
  - Zeek conn.log
  - Cloud storage account boundaries (e.g., AWS account, Azure subscription)
  - Corporate DNS resolvers
  range: last 90 days
  queries:
  - technology: pseudocode
    query: |
      DEFINE org_keywords = ['acme-prod', 'acme-dev']
      DEFINE cloud_domains = ['*.s3.amazonaws.com', '*.blob.core.windows.net']
      FOR each 10_minute_window:
        COUNT dns_queries by source_ip WHERE query_name MATCHES (org_keywords, cloud_domains)
        IF count > 50:
          GENERATE alert
- question: Are we observing a source IP making sequential or slightly varied requests to cloud storage hostnames, as measured by a low Levenshtein distance between queried names?
  context: |
    This question aims to identify systematic enumeration by analyzing the *pattern* of names being queried, not just the volume. Scanners often iterate through a wordlist with minor variations (e.g., 'acme-dev', 'acme-dev1', 'acme-dev-test'). A low Levenshtein distance (a measure of string similarity) between successive queries from the same IP indicates this non-random, iterative behavior, which is a hallmark of a wordlist scan.
  answer_sources:
  - Zeek dns.log
  - Zeek conn.log
  - Cloud storage account boundaries (e.g., AWS account, Azure subscription)
  - Corporate DNS resolvers
  range: last 90 days
  queries:
  - technology: pseudocode
    query: |
      FOR each source_ip making > 20 requests to cloud_domains:
        EXTRACT list_of_queried_names
        CALCULATE avg_levenshtein_distance between successive names in list
        IF avg_levenshtein_distance < 3:
          GENERATE alert
- question: Can we identify cloud storage scanners as anomalous clusters by modeling DNS and connection log features?
  context: |
    This question proposes using unsupervised machine learning to group connection behavior and find outliers. By clustering sessions based on features like request frequency, character entropy of queried names, and connection success ratios, a model like DBSCAN can automatically group 'normal' user and system behavior. Scanners, with their unique combination of high-frequency, high-entropy, low-success-rate activity, will fail to fit into these normal clusters and be flagged as anomalous noise.
  answer_sources:
  - Zeek dns.log
  - Zeek conn.log
  - Cloud storage account boundaries (e.g., AWS account, Azure subscription)
  - Corporate DNS resolvers
  range: last 90 days
  queries:
  - technology: pseudocode
    query: |
      FOR all connections to cloud_domains:
        EXTRACT features (source_ip, freq, entropy, success_ratio)
        APPLY DBSCAN clustering model on features
        IDENTIFY points classified as noise/outliers
        GENERATE alert for outlier source_ips
- question: Is a source IP rapidly requesting the same URL path but using different, common administrative or debug query parameter names?
  context: |
    This question targets the discovery of hidden or unlinked parameters in a web application, a technique known as parameter fuzzing. An attacker might iterate through a list of common parameters like 'debug', 'admin', or 'test' on a specific endpoint to see if any of them elicit a different response. A rule that detects a high rate of requests to one path with many unique, known-suspicious parameter names can catch this.
  answer_sources:
  - Zeek http.log
  - Microsoft-IIS-Logging
  - Web applications with complex user inputs
  - API endpoints
  range: last 90 days
  queries:
  - technology: pseudocode
    query: |
      DEFINE fuzz_params = ['debug', 'test', 'admin', 'show', 'id']
      FOR each 5_minute_window, GROUP by (source_ip, uri_path):
        COUNT distinct_query_param_names WHERE name IN fuzz_params
        IF count > 50:
          GENERATE alert
- question: For a given URL path, is a source IP generating an anomalously high Shannon entropy in the set of query parameter keys it is using?
  context: |
    This question provides a statistical way to detect parameter fuzzing without relying on a static list of suspicious parameter names. Shannon entropy measures the randomness or diversity of a set of data. A normal user interacting with an application will use a small, predictable set of parameters (low entropy). A fuzzer trying many different parameter names will generate a highly diverse, random-looking set (high entropy). Comparing this entropy score to the 99th percentile for that specific URL path can reliably detect this behavior.
  answer_sources:
  - Zeek http.log
  - Microsoft-IIS-Logging
  - Web applications with complex user inputs
  - API endpoints
  range: last 90 days
  queries:
  - technology: pseudocode
    query: |
      FOR each uri_path:
        CALCULATE baseline_entropy_99th_percentile (last_30_days)
      FOR each 5_minute_window, GROUP by (source_ip, uri_path):
        CALCULATE shannon_entropy of query_parameter_keys
        IF shannon_entropy > baseline_entropy_99th_percentile:
          GENERATE alert
- question: Can an autoencoder neural network detect anomalous URLs that indicate parameter fuzzing by identifying high reconstruction errors?
  context: |
    This question proposes a sophisticated ML approach where a model learns the 'normal' structure of all URLs for an application. An autoencoder is trained to compress and then reconstruct legitimate URLs. When a fuzzed URL with an unusual parameter is fed into the model, it will struggle to reconstruct it accurately, resulting in a high 'reconstruction error'. This error score serves as a powerful anomaly signal, capable of detecting novel fuzzing attempts that other methods might miss.
  answer_sources:
  - Zeek http.log
  - Microsoft-IIS-Logging
  - Web applications with complex user inputs
  - API endpoints
  range: last 90 days
  queries:
  - technology: pseudocode
    query: |
      TRAIN autoencoder_model on legitimate_urls_for_each_app
      FOR each new_http_request:
        CALCULATE reconstruction_error = autoencoder_model.evaluate(request_url)
        IF reconstruction_error IS high:
          GENERATE alert