Directory structure:
└── sherlock/
    ├── generator.py
    ├── refiner.py
    ├── sherlock.py
    ├── config/
    │   ├── generator.yml
    │   └── refine.yml
    └── src/
        ├── attack_retriever.py
        ├── colorlog.py
        ├── formatting.py
        ├── llm.py
        └── processing.py

================================================
FILE: generator.py
================================================
#!/usr/bin/env python3
import logging

import sys
from pathlib import Path
# Add project root to path to allow src imports
sys.path.insert(0, str(Path(__file__).resolve().parent))

from src.colorlog import make_console_handler

# Define a module-level logger to be accessible by all functions
logger = logging.getLogger(__name__)

logger.info("Importing built-in modules.")
import json
from datetime import datetime, timezone
import os
import re
import requests
import urllib3
import time  # Added for retry delays
import argparse
from typing import Any, Dict, List, Optional

logger.info("Importing installed modules")
from asksageclient import AskSageClient
import yaml

logger.info("Importing project-specific modules.")
from src.attack_retriever import build_technique_dictionary
from refiner import refine_with_llm  # Reuse the LLM interaction pattern

BASE_PROMPT = """
I need you to generate an analytic playbook. The analytic playbook consists of the following components in a YAML format:

* Playbook Name [name]: A short, descriptive name for the playbook. This should be the "technique_id" and "technique_name" in the format "technique_id: technique_name" from the playbook.
* Playbook ID [id]: A unique identifier for the playbook. The identifier should use the UUID Version 4 format. 
* Playbook Description [description]: A longer description of the playbook. This description can include useful investigative context for the playbook that is not captured in the other fields. Derive this from the "information_requirement" key and the entirety of the "indicators" list.
* Playbook Type [type]: The category of playbook. For standalone playbooks, this can either be artifact, technique, phase, or malware. Since this playbook is based off of a MITRE ATT&CK technique (indicator), use "technique" for this field.
* Related Playbooks [related]: References to other playbooks that may be useful in investigating observations commonly tied to this playbook. Insert the "tactic_id" and "tactic_name" here.
* Playbook Contributors [contributors]: A list of people who contributed to the playbook, beginning with the original author. Derive this from a comma-joined list of "contributors" from the playbook.
* Created Date [created]: The date the playbook was initially created on. Use the date in YYYY-MM-DD format. Use 2025-10-01 for now.
* Last Modified Date [modified]: The most recent date when the playbook was added to or modified. Use the date in YYYY-MM-DD format. Use 2025-10-01 for now.
* Version [version]: The version of the playbook. Use 1.0 for now.
* Tags [tags]: Additional categorization properties. For now, leave this as "none".
* Investigative Questions [questions]: The investigative question that the play should help answer. A playbook may contain multiple questions. Each question has properties associated with it.
    * Question [question]: The investigative question written in plain but detailed language for human consumption, in the form of a question. Derive one question from each "action" element.
        * Context [context]: A detailed description of the question purpose or rationale. Use this field to describe why the question is meaningful or why the analyst should care about its answer. Expound upon the "action" element here with thorough, helpful detail.
        * Answering Data Sources [answer_sources]: The data sources that can be used to answer the question. Derive this from the "data_sources" and "nai" keys.
        * Relative Time Range [range]: The time range for which evidence data should be examined to answer the question. The range should be expressed in terms relative to the observed event time, if applicable. Default to the last 90 days unless that is infeasible or unless a different value is more appropriate.
        * Queries [queries]: Search queries analysts can use to gather evidence data to answer the question. Specify the search technology and the query. For now, output short pseudocode.

Note that you must output a distinct "question", "context", "answer_sources", "range", and "queries" series for each distinct action in the analytic plan. Based on these definitions, please generate an analytic playbook in plain, unstyled text in the YAML format based on the analytic plan below. 
"""

# ---------------------------
# Utilities
# ---------------------------

def setup_logging() -> tuple[str, Path]:
    """Initializes console and file logging."""
    run_ts = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")
    logs_dir = Path("logs")
    logs_dir.mkdir(parents=True, exist_ok=True)
    log_path = logs_dir / f"generator_{run_ts}.log"

    fmt = "%(asctime)s %(levelname)-8s %(name)s :: %(message)s"
    datefmt = "%Y-%m-%d %H:%M:%S"

    # Configure the root logger
    root = logging.getLogger()
    root.setLevel(logging.INFO)
    root.handlers.clear()
    root.addHandler(make_console_handler(fmt, datefmt))

    fh = logging.FileHandler(log_path, encoding="utf-8")
    fh.setFormatter(logging.Formatter(fmt=fmt, datefmt=datefmt))
    root.addHandler(fh)

    return run_ts, log_path

def load_config(path: Path) -> dict:
    """Loads the YAML configuration file."""
    if not path.exists():
        logger.critical(f"Configuration file not found at '{path}'. Exiting.")
        raise SystemExit(1)
    
    with open(path, "r", encoding="utf-8") as f:
        try:
            config = yaml.safe_load(f)
            logger.info(f"Successfully loaded configuration from '{path}'.")
            return config
        except yaml.YAMLError as e:
            logger.critical(f"Error parsing YAML configuration: {e}")
            raise SystemExit(1)

def _parse_cli_args() -> Path:
    ap = argparse.ArgumentParser(description="Generate analytic playbooks from Watson plans.")
    ap.add_argument(
        "-c", "--config",
        default="config/generator.yml",
        help="Path to generator.yml (default: config/generator.yml)"
    )
    args = ap.parse_args()
    return Path(args.config).expanduser().resolve()

def _read_json_file(path: Path) -> Optional[Any]:
    try:
        text = path.read_text(encoding="utf-8").strip()
        if text.startswith("```"):
            # Strip markdown fence if present
            text = re.sub(r"^```(?:json)?\s*", "", text)
            text = re.sub(r"\s*```$", "", text)
        return json.loads(text)
    except Exception as e:
        logger.warning(f"Failed to read/parse JSON from {path}: {e}")
        return None

def _extract_yaml_blob(text: str) -> Optional[str]:
    s = (text or "").strip()
    if not s:
        return None
    # Prefer fenced
    m = re.search(r"```(?:yaml|yml)?\s*([\s\S]*?)\s*```", s, flags=re.IGNORECASE)
    if m:
        return m.group(1).strip()
    # Otherwise return whole thing if it looks YAML-ish
    if ":" in s:
        return s
    return None

def _collect_plan_files(plan_paths: Dict[str, str]) -> List[Path]:
    files = []
    for k, v in (plan_paths or {}).items():
        p = Path(v)
        if p.is_dir():
            cand = sorted(p.glob("*.json"))
            logger.info(f"Plan dir [{k}] {p} -> {len(cand)} file(s)")
            files.extend(cand)
        else:
            logger.warning(f"Plan path [{k}] {p} is not a directory; skipping.")
    return files

def _index_attack_by_technique(technique_dict: Dict[str, Dict[str, str]]) -> Dict[str, Dict[str, str]]:
    out = {}
    for full_key, meta in technique_dict.items():
        tid = meta.get("technique_id")
        if tid and tid not in out:
            out[tid] = {"matrix": meta.get("matrix"), "name": meta.get("name"), "tactic": meta.get("tactic", "")}
    return out

def _first_indicator(plan_obj: Any) -> Optional[Dict[str, str]]:
    """
    Given a Watson plan (list of IR objects), try to extract a representative technique_id/name and tactic info.
    """
    if not isinstance(plan_obj, list):
        return None
    for ir in plan_obj:
        inds = ir.get("indicators") if isinstance(ir, dict) else None
        if isinstance(inds, list):
            for ind in inds:
                tid = (ind or {}).get("technique_id", "").strip()
                name = (ind or {}).get("name", "").strip()
                if tid:
                    return {
                        "technique_id": tid,
                        "technique_name": name,
                        "tactic_id": (ir or {}).get("tactic_id", "").strip(),
                        "tactic_name": (ir or {}).get("tactic_name", "").strip(),
                    }
    return None

# ---------------------------
# Main
# ---------------------------

def main():
    """Main script execution: read Watson plans ⇒ generate Sherlock playbooks (YAML)."""
    run_ts, log_path = setup_logging()
    logger.info(f"Run initialized at: {run_ts} | Logging to: {log_path}")

    # --- 0. Load keys for providers (same behavior as refiner.py) ---
    logger.info("Loading Gemini API key")
    try:
        with open(".GEMINI_API_KEY", "r") as fd:
            os.environ["GEMINI_API_KEY"] = fd.read().strip()
    except Exception:
        logger.info("Failed to import Gemini API key")

    logger.info("Loading Sage API key")
    try:
        with open("./credentials.json", "r") as file:
            credentials = json.load(file)
            if 'credentials' not in credentials or 'api_key' not in credentials['credentials']:
                logger.error("Missing required keys in the credentials file.")
                raise
        sage_api_key = credentials['credentials']['api_key']
        sage_email = credentials['credentials']['Ask_sage_user_info']['username']
    except FileNotFoundError:
        raise FileNotFoundError(f"Credentials file not found at ./credentials.json")
    except json.JSONDecodeError:
        raise ValueError(f"Invalid JSON format in the credentials file: ./credentials.json")

    # Disable SSL warnings (matches refiner)
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    old_request = requests.Session.request
    def new_request(self, method, url, **kwargs):
        kwargs['verify'] = False
        return old_request(self, method, url, **kwargs)
    requests.Session.request = new_request

    # Build AskSage client (same base URLs as refiner)
    ask_sage_client = AskSageClient(
        sage_email, 
        sage_api_key, 
        user_base_url="https://api.genai.army.mil/user/", 
        server_base_url="https://api.genai.army.mil/server/"
    )

    # --- 1. Load Configuration ---
    cfg_path = _parse_cli_args()
    logger.info(f"Loading configuration from: {cfg_path}")
    config = load_config(cfg_path)

    plan_paths: Dict[str, str] = config.get("plan_paths", {}) or {}
    output_dirs_map: Dict[str, str] = config.get("output_directories", {}) or {}
    matrices = config.get("matrices", ["enterprise"])
    filter_techniques = config.get("techniques", []) or []

    # LLM behavior (mirror refiner)
    model = config.get("model", "gemini-2.5-pro")
    max_retries = int(config.get("max_retries", 3))
    retry_delay = int(config.get("retry_delay", 1))
    llm_provider = (config.get("llm_provider") or "auto").strip().lower()
    llm_model = config.get("llm_model")  # may be None
    make_backup = bool(config.get("backup", True))
    num_cores = int(config.get("num_cores", 0) or 0)  # not used here; single-core is fine

    if not output_dirs_map:
        logger.critical("Configuration key 'output_directories' is missing or empty. Cannot determine where to save playbooks.")
        raise SystemExit(1)

    # --- 2. Technique dictionary (to map technique_id ⇒ matrix) ---
    logger.info("Building technique dictionary for ATT&CK → matrix resolution")
    technique_dict = build_technique_dictionary(matrices)
    tech_index = _index_attack_by_technique(technique_dict)

    # --- 3. Collect Watson plan files ---
    plan_files = _collect_plan_files(plan_paths)
    if not plan_files:
        logger.warning("No input plan files found in plan_paths.")
        logger.info("Script finished.")
        return

    if filter_techniques:
        filter_set = set(filter_techniques)
        logger.info(f"Technique filter is active with {len(filter_set)} IDs.")
    else:
        filter_set = None

    total = len(plan_files)
    generated = skipped = failed = 0

    for i, plan_path in enumerate(plan_files, start=1):
        logger.info(f"[{i}/{total}] Processing plan: {plan_path}")

        plan_obj = _read_json_file(plan_path)
        if not plan_obj:
            logger.warning(f"Skipping unreadable plan: {plan_path.name}")
            skipped += 1
            continue

        ind_info = _first_indicator(plan_obj)
        if not ind_info:
            logger.warning(f"No indicator/technique found in plan {plan_path.name}; skipping.")
            skipped += 1
            continue

        tech_id = ind_info["technique_id"]
        tech_name = ind_info["technique_name"] or tech_index.get(tech_id, {}).get("name", "")
        tactic_id = ind_info["tactic_id"]
        tactic_name = ind_info["tactic_name"]

        if filter_set and tech_id not in filter_set:
            logger.info(f"Technique {tech_id} not in filter list; skipping.")
            skipped += 1
            continue

        # Determine matrix/output dir
        matrix = tech_index.get(tech_id, {}).get("matrix", "enterprise")
        out_dir = Path(output_dirs_map.get(matrix) or output_dirs_map.get("default", "playbooks"))
        out_dir.mkdir(parents=True, exist_ok=True)
        out_path = out_dir / f"{tech_id}.yml"
        if out_path.exists():
            logger.info(f"Playbook already exists for {tech_id} at {out_path}; skipping.")
            skipped += 1
            continue

        # Build prompt
        prompt = (
            f"{BASE_PROMPT}\n\n"
            f"Technique: {tech_id} - {tech_name}\n"
            f"Tactic: {tactic_id} - {tactic_name}\n\n"
            f"EXISTING ANALYTIC PLAN (JSON):\n```json\n{json.dumps(plan_obj, indent=2)}\n```\n"
            f"Return ONLY the YAML document (no code fences, no commentary)."
        )

        try:
            llm_res = refine_with_llm(
                prompt=prompt,
                provider_pref=llm_provider,
                model=llm_model,
                ask_sage_client=ask_sage_client,
                max_retries=max_retries,
                retry_delay=retry_delay,
                gemini_primary_model=model,
            )
        except Exception as e:
            logger.error(f"LLM call failed for {tech_id}: {e}")
            failed += 1
            continue

        yaml_text = _extract_yaml_blob(llm_res.get("text", ""))
        if not yaml_text:
            logger.error(f"Could not extract YAML for {tech_id}")
            failed += 1
            continue

        # Optional backup of the source plan
        if make_backup:
            bdir = Path("backups") / f"generator_{run_ts}"
            bdir.mkdir(parents=True, exist_ok=True)
            (bdir / f"{tech_id}_{plan_path.name}").write_text(
                json.dumps(plan_obj, indent=2), encoding="utf-8"
            )

        try:
            out_path.write_text(yaml_text, encoding="utf-8")
            logger.info(f"Saved playbook to {out_path}")
            generated += 1
        except Exception as e:
            logger.error(f"Failed to write playbook {out_path}: {e}")
            failed += 1

    logger.info(f"Generation complete. Generated: {generated} | Skipped: {skipped} | Failed: {failed}")
    logger.info("Script finished successfully.")

if __name__ == "__main__":
    main()


================================================
FILE: refiner.py
================================================
#!/usr/bin/env python3
import logging
import sys
from pathlib import Path

# Add project root to path to allow src imports
sys.path.insert(0, str(Path(__file__).resolve().parent))

from src.colorlog import make_console_handler

# Module-level logger
logger = logging.getLogger(__name__)

logger.info("Importing built-in modules.")
import json
from datetime import datetime, timezone
import os
import re
import requests
import urllib3
import time
from typing import Any, Dict, List, Tuple, Optional
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing
import argparse

logger.info("Importing installed modules")
from asksageclient import AskSageClient
from google import genai
from google.genai import types
from google.genai import errors
import yaml

logger.info("Importing project-specific modules.")
from src.attack_retriever import build_technique_dictionary

BASE_PROMPT = """
I need you to refine an existing analytic plan. The analytic plan consists of the following components:

1.  Information Requirements (IRs): These identify the information that the commander considers most important. For example, 'Has the adversary gained initial access? (TA0001 - Initial Access)' (PIR) or 'What data is available for threat detection and modeling? (D3-D - Detect)' (FFIR). Note that PIRs are tagged with a MITRE ATT&CK tactic, and FFIRs are tagged with a MITRE D3FEND tactic. We call these "CCIR" generally.

2.  Indicators: These are positive or negative evidence of threat activity pertaining to one or more information requirements. They are observable clues related to a specific information requirement. For the IR above, indicators might include:
    * T1078 - Valid Accounts
    For the FFIR above, indicators might include:
    * D3-NTA - Network Traffic Analysis
    Note that indicators for PIRs are tagged with MITRE ATT&CK techniques, and FFIRs are tagged with MITRE D3FEND techniques.

3.  Evidence: This is the concrete information that supports or refutes an indicator. It provides the 'proof' and can vary in complexity. For the indicator 'T1078 - Valid Accounts', evidence could be 'A valid account login exhibits multiple anomalous characteristics simultaneously, such as originating from a rare geographic location, using an unfamiliar device, and occurring outside of normal working hours.' For the indicator 'D3-NTA', evidence could be 'Logs generated from network activity such as network flow metadata and network traffic content'.

4.  Data: This describes the precise data necessary to identify evidence. Specificity here is key (e.g., Zeek Conn logs, Sysmon event ID 4624, Active Directory security logs). For the evidence, focus your plan on the following data sources: network logs, specifically Zeek logs; host logs, specifically Windows Event IDs. Write only the data name. For example, Windows Event ID 4688, Zeek conn.log

5. Data Source (Platform): Use a dummy value here of "TBD".

6. Named Areas of Interest (NAIs): These are areas where data that will satisfy a specific information requirement can be collected. For the IR above, NAIs could include 'Our organization's internet gateway', 'Authentication servers', 'Servers hosting sensitive data', and 'Endpoint devices of high-value targets'.

7.  Actions: These are high-level instructions that guide the analysts' search for evidence. For the evidence associated with the indicator 'T1078 - Valid Accounts' and the associated PIR 'Has the adversary gained initial access? (TA0001 - Initial Access)', an action could be: 'For each successful login (Windows Event ID 4624), enrich with geolocation data from the source IP (Zeek conn.log). Establish a multi-faceted baseline for each user account including typical login times, source countries/ISPs, and devices used. Use a scoring system where deviations from the baseline (e.g., rare country, login at 3 AM, new device) add to a risk score. A high cumulative risk score, identified using statistical models or descriptive statistics (e.g., multiple metrics exceeding 2 standard deviations from the norm), indicates a likely compromised account.' For the evidence associated with the indicator 'D3-NTA' and the associated FFIR 'What data is available for threat detection and modeling? (D3-D - Detect)', an action could be: 'Inventory available network log sources (e.g., networking appliances, Zeek, PCAP). For each source, perform a time series analysis to visualize data volume over at least the last 30 days to identify collection gaps or anomalies. Use descriptive statistics to summarize key fields like protocol distribution in Zeek conn.log and the frequency of top requested domains in dns.log to establish a cursory understanding of network activity. Compare across data sources to validate collection consistency and identify individual sensor blind spots.' Focus mostly on simple detections, but also look for opportunities to incorporate basic data science techniques here, such as percentiles, entropy scores, statistics, and other, similar methods. Generally speaking, you should have one symbolic logic (such as an IOC match), one statistical method (such as a percentile threshold), and machine learning application (such as classification or time series analysis) action.

Based on these definitions, please refine an existing analytic plan in plain, unstyled text in the JSON format below. Provide specific and relevant examples for each component within this format.

[
  {
    "information_requirement": "Insert CCIR here",
    "tactic_id": "Insert MITRE ATT&CK or MITRE D3FEND tactic T-code here",
    "tactic_name": "Insert the tactic name here",
    "indicators": [
      {
        "technique_id": "Insert MITRE technique T-code here",
        "name": "Insert the technique name here",
        "evidence": [
          {
            "description": "Describe the evidence here",
            "data_sources": [
              "First data source",
              "Second data source"
            ],
            "data_platforms": [
              "TBD"
            ],
            "nai": "Insert site-specific NAI here",
            "action": [
              "Describe the symbolic logic action here, such as an IOC match",
              "Describe one statistical acton, such as a percentile threshold",
              "Describe one machine learning action, such as classification, regression, time series analysis, clustering, etc."
            ]
          }
        ]
      }
    ]
   }
]

Based on that format, improve an analytic plan for the following technique. If you are given an offensive technique, a T-code, then only generate PIRs; if you are given a defensive technique, a D-code, then only generate FFIRs. Pay extremely close attention to the type of matrix the technique references (enterprise, ICS, mobile), which will have a significant impact on how you build this plan.
"""

# Additional prompt for playbook refinement
PLAYBOOK_REFINE_PROMPT = """
You are given an existing analytic playbook in YAML. Refine it while preserving the YAML schema and improving clarity, specificity, and operational utility.

Requirements:
- Preserve the top-level keys: name, id, description, type, related, contributors, created, modified, tags, questions.
- Keep "type" = "technique".
- Ensure each question has: question, context, answer_sources (list), range (string), queries (list of {system, query}).
- Keep content executable by SOC teams (SIEM/SQL/Zeek pseudo-queries are fine).
- Avoid stylistic markup. No code fences. Return only a single YAML document.

Return only the refined YAML (no commentary).
"""

# ---------------------------
# Utilities
# ---------------------------

def setup_logging() -> tuple[str, Path]:
    run_ts = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")
    logs_dir = Path("logs")
    logs_dir.mkdir(parents=True, exist_ok=True)
    log_path = logs_dir / f"refiner_{run_ts}.log"

    fmt = "%(asctime)s %(levelname)-8s %(name)s :: %(message)s"
    datefmt = "%Y-%m-%d %H:%M:%S"

    root = logging.getLogger()
    root.setLevel(logging.INFO)
    root.handlers.clear()
    root.addHandler(make_console_handler(fmt, datefmt))

    fh = logging.FileHandler(log_path, encoding="utf-8")
    fh.setFormatter(logging.Formatter(fmt=fmt, datefmt=datefmt))
    root.addHandler(fh)
    return run_ts, log_path

def load_config(path: Path) -> dict:
    if not path.exists():
        logger.critical(f"Configuration file not found at '{path}'. Exiting.")
        raise SystemExit(1)
    with open(path, "r", encoding="utf-8") as f:
        try:
            config = yaml.safe_load(f)
            logger.info(f"Successfully loaded configuration from '{path}'.")
            return config
        except yaml.YAMLError as e:
            logger.critical(f"Error parsing YAML configuration: {e}")
            raise SystemExit(1)

def parse_date(date_str: str) -> Optional[datetime]:
    if not date_str:
        return None
    try:
        return datetime.strptime(date_str, "%Y-%m-%d").replace(tzinfo=timezone.utc)
    except Exception:
        return None

def semver_tuple(v: str) -> Tuple[int, ...]:
    if not isinstance(v, str):
        return (0, )
    parts = v.strip().split(".")
    out = []
    for p in parts[:3]:
        try:
            out.append(int(p))
        except ValueError:
            try:
                fp = float(p)
                out.append(int(round(fp)))
            except Exception:
                out.append(0)
    return tuple(out) if out else (0, )

def compare_versions(a: str, b: str) -> int:
    ta, tb = semver_tuple(a), semver_tuple(b)
    la, lb = len(ta), len(tb)
    if la < lb:
        ta = ta + (0,) * (lb - la)
    elif lb < la:
        tb = tb + (0,) * (la - lb)
    return (ta > tb) - (ta < tb)

def increment_version(v: str) -> str:
    if not isinstance(v, str) or not v:
        return "1.1"
    parts = v.split(".")
    nums = [int(p) if p.isdigit() else 0 for p in parts]
    if len(nums) == 1:
        return f"{nums[0] + 1}"
    if len(nums) == 2:
        return f"{nums[0]}.{nums[1] + 1}"
    nums[2] += 1
    return ".".join(str(x) for x in nums[:3])

def extract_json(blob: str) -> Optional[str]:
    s = blob.strip()
    if s.startswith("[") or s.startswith("{"):
        return s
    m = re.search(r"```(?:json)?\s*([\s\S]*?)\s*```", s)
    if m:
        return m.group(1).strip()
    first_bracket = min((i for i in [s.find("["), s.find("{")] if i != -1), default=-1)
    if first_bracket != -1:
        last_square = s.rfind("]")
        last_curly = s.rfind("}")
        end_index = max(last_square, last_curly)
        if end_index > first_bracket:
            return s[first_bracket:end_index + 1]
    return None

def extract_yaml(blob: str) -> Optional[str]:
    s = (blob or "").strip()
    if not s:
        return None
    m = re.search(r"```(?:yaml|yml)?\s*([\s\S]*?)\s*```", s, flags=re.IGNORECASE)
    if m:
        return m.group(1).strip()
    # crude heuristic
    if ":" in s:
        return s
    return None

def compute_plan_metadata(plan: List[dict]) -> Tuple[Optional[str], Optional[str]]:
    max_dt = None
    max_ver = None
    for ir in plan:
        lu = parse_date(ir.get("last_updated", ""))
        if lu and (max_dt is None or lu > max_dt):
            max_dt = lu
        ver = ir.get("version", "")
        if ver and (max_ver is None or compare_versions(ver, max_ver) > 0):
            max_ver = ver
    dt_iso = max_dt.strftime("%Y-%m-%d") if max_dt else None
    return dt_iso, max_ver

def _core_tag() -> str:
    pid = os.getpid()
    core = None
    try:
        core = os.sched_getcpu()  # type: ignore[attr-defined]
    except Exception:
        core = None
    if core is not None:
        return f"core={core} pid={pid}"
    try:
        proc_name = multiprocessing.current_process().name
        return f"pid={pid} proc={proc_name}"
    except Exception:
        return f"pid={pid}"

def refine_with_llm(
    prompt: str,
    provider_pref: Optional[str],
    model: Optional[str],
    ask_sage_client: AskSageClient,
    max_retries: int = 3,
    retry_delay: int = 1,
    gemini_primary_model: Optional[str] = None,  # for auto mode fallback to legacy `model` key
) -> Dict[str, str]:
    """
    Returns: {"text": <response>, "endpoint": "gemini"|"asksage", "model_used": <model_name>}

    Behavior:
      - If provider_pref in {"gemini","asksage"} AND model provided: use ONLY that provider+model.
      - Else: AUTO mode = try Gemini (gemini_primary_model or model) then AskSage fallback.
    """
    if not prompt or not isinstance(prompt, str):
        raise ValueError("Prompt must be a non-empty string")

    tag = _core_tag()

    def _call_gemini(gem_model: str) -> Dict[str, str]:
        api_key = os.environ.get("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY environment variable is not set")
        logger.info(f"[LLM] [{tag}] Gemini call model={gem_model}")
        client = genai.Client(api_key=api_key)
        resp = client.models.generate_content(
            model=gem_model,
            contents=[types.Content(role="user", parts=[types.Part.from_text(text=prompt)])],
            config=types.GenerateContentConfig(temperature=0.7),
        )
        if not resp or not getattr(resp, "text", None):
            raise ValueError("Invalid/empty response from Gemini")
        return {"text": resp.text, "endpoint": "gemini", "model_used": gem_model}

    def _call_asksage(as_model: str) -> Dict[str, str]:
        logger.info(f"[LLM] [{tag}] AskSage call model={as_model}")
        resp = ask_sage_client.query(
            prompt,
            persona="default",
            dataset="none",
            limit_references=0,
            temperature=0.7,
            live=0,
            model=as_model,
            system_prompt=None,
        )
        if not resp or not isinstance(resp, dict) or not resp.get("message"):
            raise ValueError("Invalid/empty response from AskSage")
        return {"text": resp["message"], "endpoint": "asksage", "model_used": as_model}

    if provider_pref in {"gemini", "asksage"} and model:
        logger.info(f"[LLM] [{tag}] Forced LLM mode: provider={provider_pref} model={model}")
        if provider_pref == "gemini":
            return _call_gemini(model)
        else:
            return _call_asksage(model)

    primary_gemini_model = gemini_primary_model or (model if model else "gemini-2.5-pro")
    logger.info(f"[LLM] [{tag}] AUTO mode: try Gemini primary={primary_gemini_model}, fallback=AskSage")

    for attempt in range(max_retries):
        try:
            return _call_gemini(primary_gemini_model)
        except (errors.ClientError, errors.APIError, ValueError) as e:
            msg = str(e)
            code = getattr(e, 'status_code', None) if hasattr(e, 'status_code') else None
            retriable = (
                code == 429 or
                "429" in msg or
                "RESOURCE_EXHAUSTED" in msg or
                "quota" in msg.lower() or
                "rate" in msg.lower()
            )
            if retriable:
                logger.error(f"[LLM] [{tag}] Gemini rate/quota: {msg}. Breaking to AskSage fallback.")
                break
            delay = retry_delay * (2 ** attempt)
            m = re.search(r'retry in (\d+(?:\.\d+)?)', msg.lower())
            if m:
                try:
                    delay = min(float(m.group(1)) + 1, 120)
                except Exception:
                    pass
            if attempt < max_retries - 1:
                logger.warning(f"[LLM] [{tag}] Gemini error: {msg}. Retrying in {delay:.1f}s ...")
                time.sleep(delay)
            else:
                logger.warning(f"[LLM] [{tag}] Gemini attempts exhausted; switching to AskSage fallback.")

    fallback_model = "google-gemini-2.5-pro"
    logger.info(f"[LLM] [{tag}] Fallback AskSage model={fallback_model}")
    return _call_asksage(fallback_model)

# ---------------------------
# Worker (plans mode)
# ---------------------------

def _worker_refine_one(job: Dict[str, Any]) -> Dict[str, Any]:
    """
    Worker process function: refine a single technique plan. Returns a summary dict.
    Logs:
      - Skip reasons
      - Pre-processing endpoint/model (with core/worker tag)
      - Post-processing completion with endpoint/model (with core/worker tag)
    """
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    old_request = requests.Session.request
    def new_request(self, method, url, **kwargs):
        kwargs['verify'] = False
        return old_request(self, method, url, **kwargs)
    requests.Session.request = new_request

    gemini_api_key = job.get("gemini_api_key")
    if gemini_api_key:
        os.environ["GEMINI_API_KEY"] = gemini_api_key

    ask_sage_client = AskSageClient(
        job["sage_email"],
        job["sage_api_key"],
        user_base_url="https://api.genai.army.mil/user/",
        server_base_url="https://api.genai.army.mil/server/",
    )

    tag = _core_tag()

    try:
        full_key: str = job["full_key"]
        tech_data: Dict[str, Any] = job["tech_data"]

        matrix_type = tech_data.get("matrix")
        output_dirs_map = job["output_dirs_map"]
        default_output_dir = Path(job["default_output_dir"])
        if matrix_type and matrix_type in output_dirs_map:
            output_dir = Path(output_dirs_map[matrix_type])
        else:
            output_dir = default_output_dir

        try:
            technique_id = full_key.split(" - ")[0].strip()
        except Exception:
            logger.warning(f"[{full_key}] [{tag}] SKIP: unable to parse technique_id from key.")
            return {"technique": full_key, "status": "fail", "reason": "parse_id"}

        file_path = output_dir / f"{technique_id}.json"
        if not file_path.exists():
            logger.info(f"[{technique_id}] [{tag}] SKIP: plan file missing at '{file_path}'.")
            return {"technique": technique_id, "status": "missing"}

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                existing_plan = json.load(f)
            if not isinstance(existing_plan, list):
                logger.info(f"[{technique_id}] [{tag}] SKIP: plan JSON is not a list.")
                return {"technique": technique_id, "status": "skip", "reason": "not_list"}
        except Exception as e:
            logger.info(f"[{technique_id}] [{tag}] SKIP: read/parse error: {e}")
            return {"technique": technique_id, "status": "skip", "reason": f"read_error: {e}"}

        plan_last_updated_max, plan_version_max = compute_plan_metadata(existing_plan)

        if job.get("skip_if_updated_after"):
            cutoff_dt = parse_date(job["skip_if_updated_after"])
            plan_dt = parse_date(plan_last_updated_max) if plan_last_updated_max else None
            if cutoff_dt and plan_dt and plan_dt > cutoff_dt:
                logger.info(f"[{technique_id}] [{tag}] SKIP: last_updated {plan_last_updated_max} > cutoff {job['skip_if_updated_after']}.")
                return {"technique": technique_id, "status": "skip", "reason": "updated_after_cutoff"}

        if job.get("skip_if_version_gt") and plan_version_max:
            if compare_versions(plan_version_max, str(job["skip_if_version_gt"])) > 0:
                logger.info(f"[{technique_id}] [{tag}] SKIP: version {plan_version_max} > {job['skip_if_version_gt']}.")
                return {"technique": technique_id, "status": "skip", "reason": "version_gt"}

        refine_guidance = (job.get("refine_guidance") or "").strip()
        matrix_banner = f"Matrix: MITRE ATT&CK for {tech_data.get('matrix','').upper()}"
        refine_block = (
            f"{BASE_PROMPT}\n\n"
            f"You are given an EXISTING analytic plan to refine. Keep the JSON schema identical and retain "
            f"the top-level array-of-objects structure. Improve clarity, specificity, and operational utility, "
            f"but do not remove required fields.\n\n"
            f"Technique: {full_key}\n"
            f"{matrix_banner}\n"
            f"Tactic(s): {tech_data.get('tactic','')}\n\n"
            f"Description: {tech_data.get('description','')}\n\n"
            f"Detection: {tech_data.get('detection','')}\n\n"
            f"Existing plan JSON:\n```json\n{json.dumps(existing_plan, indent=2)}\n```\n\n"
        )
        if refine_guidance:
            refine_block += f"REFINEMENT GUIDANCE (apply carefully):\n{refine_guidance}\n\n"
        refine_block += "Return ONLY the refined JSON array (no commentary, no markdown, no code fences)."

        prov = (job.get("llm_provider") or "auto").lower()
        mdl = job.get("llm_model")
        base_model = mdl or job.get("model")

        logger.info(f"[{technique_id}] [{tag}] START: provider={prov} model={mdl or base_model}")

        llm_res = refine_with_llm(
            prompt=refine_block,
            provider_pref=prov,
            model=mdl,
            ask_sage_client=ask_sage_client,
            max_retries=job["max_retries"],
            retry_delay=job["retry_delay"],
            gemini_primary_model=base_model,
        )

        if llm_res["endpoint"] != "gemini":
            logger.info(f"[{technique_id}] [{tag}] INFO: fell back to endpoint={llm_res['endpoint']} model={llm_res['model_used']}")

        json_str = extract_json(llm_res["text"])
        if not json_str:
            logger.error(f"[{technique_id}] [{tag}] FAIL: could not extract JSON from model output.")
            return {"technique": technique_id, "status": "fail", "reason": "no_json"}

        try:
            refined_plan = json.loads(json_str)
            if not isinstance(refined_plan, list):
                logger.error(f"[{technique_id}] [{tag}] FAIL: refined JSON is not a list.")
                return {"technique": technique_id, "status": "fail", "reason": "not_list_refined"}
        except json.JSONDecodeError as e:
            logger.error(f"[{technique_id}] [{tag}] FAIL: JSON decode error: {e}")
            return {"technique": technique_id, "status": "fail", "reason": f"json_error: {e}"}

        today_iso = job["today_iso"]
        _, base_version_max = compute_plan_metadata(existing_plan)
        new_version = increment_version(base_version_max or "1.0")

        existing_dates = [ir.get("date_created") for ir in existing_plan if ir.get("date_created")]
        min_created = min(existing_dates) if existing_dates else None
        contrib_union = []
        seen = set()
        for ir in existing_plan:
            for c in (ir.get("contributors") or []):
                if c not in seen:
                    seen.add(c)
                    contrib_union.append(c)
        if not contrib_union:
            contrib_union = ["Zachary Szewczyk"]

        for ir in refined_plan:
            ir["last_updated"] = today_iso
            ir["version"] = new_version
            if "date_created" not in ir and min_created:
                ir["date_created"] = min_created
            if "contributors" not in ir or not isinstance(ir["contributors"], list) or not ir["contributors"]:
                ir["contributors"] = contrib_union
            else:
                existing_c = set(ir["contributors"])
                for c in contrib_union:
                    if c not in existing_c:
                        ir["contributors"].append(c)

        try:
            output_dir = Path(job["resolved_output_dir"])
            output_dir.mkdir(parents=True, exist_ok=True)
            if job["make_backup"]:
                backups_dir = Path(job["backups_dir"])
                backups_dir.mkdir(parents=True, exist_ok=True)
                backup_path = backups_dir / f"{technique_id}_{job['run_ts']}.json"
                with open(backup_path, "w", encoding="utf-8") as bf:
                    json.dump(existing_plan, bf, indent=2)

            file_path = Path(job["resolved_output_dir"]) / f"{technique_id}.json"
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(refined_plan, f, indent=2)
        except Exception as e:
            logger.error(f"[{technique_id}] [{tag}] FAIL: write error: {e}")
            return {"technique": technique_id, "status": "fail", "reason": f"write_error: {e}"}

        logger.info(f"[{technique_id}] [{tag}] DONE: processing complete with endpoint={llm_res['endpoint']} model={llm_res['model_used']}")
        return {"technique": technique_id, "status": "ok", "endpoint": llm_res["endpoint"], "model_used": llm_res["model_used"]}

    except Exception as e:
        logger.error(f"[{job.get('full_key','unknown')}] [{tag}] FAIL: unexpected error: {e}")
        return {"technique": job.get("full_key", "unknown"), "status": "fail", "reason": f"unexpected: {e}"}

def _parse_cli_args() -> Tuple[Path, str]:
    parser = argparse.ArgumentParser(
        description="Refine existing analytic plans or analytic playbooks using LLMs."
    )
    parser.add_argument(
        "-c", "--config",
        default="config/refine.yml",
        help="Path to refine.yml (default: config/refine.yml)"
    )
    parser.add_argument(
        "--mode",
        choices=["plans", "playbooks"],
        default=None,
        help="Refinement mode. If omitted, uses 'mode' from config (default: plans)."
    )
    args = parser.parse_args()
    return Path(args.config).expanduser().resolve(), (args.mode or "")

# ---------------------------
# Playbook refinement (YAML)
# ---------------------------

def _refine_one_playbook(job: Dict[str, Any]) -> Dict[str, Any]:
    """
    Refines a single playbook YAML file and writes it back.
    """
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    old_request = requests.Session.request
    def new_request(self, method, url, **kwargs):
        kwargs['verify'] = False
        return old_request(self, method, url, **kwargs)
    requests.Session.request = new_request

    if job.get("gemini_api_key"):
        os.environ["GEMINI_API_KEY"] = job["gemini_api_key"]

    ask_sage_client = AskSageClient(
        job["sage_email"],
        job["sage_api_key"],
        user_base_url="https://api.genai.army.mil/user/",
        server_base_url="https://api.genai.army.mil/server/",
    )

    tag = _core_tag()
    path: Path = Path(job["file_path"])
    try:
        raw_yaml = path.read_text(encoding="utf-8")
    except Exception as e:
        logger.info(f"[{path.name}] [{tag}] SKIP: read error: {e}")
        return {"file": str(path), "status": "skip", "reason": f"read_error: {e}"}

    try:
        pb_obj = yaml.safe_load(raw_yaml)
        if not isinstance(pb_obj, dict) or "questions" not in pb_obj:
            logger.info(f"[{path.name}] [{tag}] SKIP: not a playbook dict or missing 'questions'.")
            return {"file": str(path), "status": "skip", "reason": "bad_schema"}
    except Exception as e:
        logger.info(f"[{path.name}] [{tag}] SKIP: parse error: {e}")
        return {"file": str(path), "status": "skip", "reason": f"yaml_parse_error: {e}"}

    refine_block = (
        f"{PLAYBOOK_REFINE_PROMPT}\n\n"
        f"EXISTING PLAYBOOK YAML:\n```yaml\n{raw_yaml}\n```\n\n"
        f"Return ONLY the refined YAML (no commentary, no fences)."
    )

    prov = (job.get("llm_provider") or "auto").lower()
    mdl = job.get("llm_model")
    base_model = mdl or job.get("model")

    logger.info(f"[{path.name}] [{tag}] START (playbook): provider={prov} model={mdl or base_model}")

    llm_res = refine_with_llm(
        prompt=refine_block,
        provider_pref=prov,
        model=mdl,
        ask_sage_client=ask_sage_client,
        max_retries=job["max_retries"],
        retry_delay=job["retry_delay"],
        gemini_primary_model=base_model,
    )

    ytxt = extract_yaml(llm_res.get("text", ""))
    if not ytxt:
        logger.error(f"[{path.name}] [{tag}] FAIL: could not extract YAML response.")
        return {"file": str(path), "status": "fail", "reason": "no_yaml"}

    # Best-effort validation
    try:
        refined = yaml.safe_load(ytxt)
        if not isinstance(refined, dict) or "questions" not in refined:
            logger.error(f"[{path.name}] [{tag}] FAIL: refined YAML not dict/has no 'questions'.")
            return {"file": str(path), "status": "fail", "reason": "bad_refined_schema"}
        # update modified date if present
        today_iso = job["today_iso"]
        refined["modified"] = today_iso
        if "created" not in refined or not refined["created"]:
            refined["created"] = today_iso
        ytxt = yaml.safe_dump(refined, sort_keys=False)
    except Exception as e:
        logger.error(f"[{path.name}] [{tag}] FAIL: YAML validation failed: {e}")
        return {"file": str(path), "status": "fail", "reason": f"yaml_validation: {e}"}

    try:
        backup_dir = Path(job["backups_dir"])
        if job["make_backup"]:
            backup_dir.mkdir(parents=True, exist_ok=True)
            (backup_dir / f"{path.stem}_{job['run_ts']}.yml").write_text(raw_yaml, encoding="utf-8")
        path.write_text(ytxt, encoding="utf-8")
    except Exception as e:
        logger.error(f"[{path.name}] [{tag}] FAIL: write error: {e}")
        return {"file": str(path), "status": "fail", "reason": f"write_error: {e}"}

    logger.info(f"[{path.name}] [{tag}] DONE (playbook): endpoint={llm_res['endpoint']} model={llm_res['model_used']}")
    return {"file": str(path), "status": "ok", "endpoint": llm_res["endpoint"], "model_used": llm_res["model_used"]}

# ---------------------------
# Main
# ---------------------------

def main():
    run_ts, log_path = setup_logging()
    logger.info(f"Run initialized at: {run_ts} | Logging to: {log_path}")

    # --- 0. Keys ---
    logger.info("Loading Gemini API key")
    gemini_api_key = None
    try:
        with open(".GEMINI_API_KEY", "r") as fd:
            gemini_api_key = fd.read().strip()
            os.environ["GEMINI_API_KEY"] = gemini_api_key
    except Exception:
        logger.info("Failed to import Gemini API key")

    logger.info("Loading Sage API key")
    try:
        with open("./credentials.json", "r") as file:
            credentials = json.load(file)
            if 'credentials' not in credentials or 'api_key' not in credentials['credentials']:
                logger.error("Missing required keys in the credentials file.")
                raise
        sage_api_key = credentials['credentials']['api_key']
        sage_email = credentials['credentials']['Ask_sage_user_info']['username']
    except FileNotFoundError:
        raise FileNotFoundError("Credentials file not found at ./credentials.json")
    except json.JSONDecodeError:
        raise ValueError("Invalid JSON format in the credentials file: ./credentials.json")

    # --- 1. Config ---
    cfg_path, cli_mode = _parse_cli_args()
    logger.info(f"Loading configuration from: {cfg_path}")
    config = load_config(cfg_path)

    mode = (cli_mode or config.get("mode") or "plans").strip().lower()
    logger.info(f"Refiner mode: {mode}")

    # Shared knobs
    max_retries = int(config.get("max_retries", 3))
    retry_delay = int(config.get("retry_delay", 1))
    llm_provider = (config.get("llm_provider") or "auto").strip().lower()
    llm_model = config.get("llm_model")  # may be None
    model = config.get("model", "gemini-2.5-pro")
    make_backup = bool(config.get("backup", True))
    num_cores = config.get("num_cores", 0)

    today_iso = datetime.now(timezone.utc).strftime("%Y-%m-%d")
    backups_dir = Path("backups") / f"refiner_{run_ts}"

    if mode == "playbooks":
        # Gather playbook files from config
        pmap = config.get("playbook_directories", {}) or {}
        files: List[Path] = []
        for k, v in pmap.items():
            d = Path(v)
            if d.is_dir():
                found = sorted(d.glob("*.yml"))
                logger.info(f"Playbook dir [{k}] {d} -> {len(found)} file(s)")
                files.extend(found)
            else:
                logger.warning(f"Configured playbook directory missing [{k}]: {d}")

        if not files:
            logger.warning("No playbook files found to refine.")
            logger.info("Script finished.")
            return

        # Normalize workers
        try:
            if num_cores is None:
                workers = 1
            else:
                workers = int(num_cores)
        except Exception:
            workers = 1
        max_cpu = os.cpu_count() or 1
        workers = max(1, min(workers, max_cpu))

        jobs = []
        for f in files:
            jobs.append({
                "file_path": str(f),
                "today_iso": today_iso,
                "run_ts": run_ts,
                "backups_dir": str(backups_dir),
                "make_backup": make_backup,
                "llm_provider": llm_provider,
                "llm_model": llm_model,
                "model": model,
                "max_retries": max_retries,
                "retry_delay": retry_delay,
                "sage_email": sage_email,
                "sage_api_key": sage_api_key,
                "gemini_api_key": gemini_api_key,
            })

        ok = skip = fail = 0
        tag = _core_tag()
        if workers <= 1:
            logger.info(f"[MAIN {tag}] Running in single-core mode (playbooks).")
            for jb in jobs:
                res = _refine_one_playbook(jb)
                st = res.get("status")
                if st == "ok":
                    ok += 1
                elif st == "skip":
                    skip += 1
                else:
                    fail += 1
                logger.info(f"[MAIN {tag}] {res.get('file')}: {st}" + (f" ({res.get('reason')})" if res.get('reason') else ""))
        else:
            logger.info(f"[MAIN {tag}] Running in multi-core mode (playbooks) with {workers} workers.")
            with ProcessPoolExecutor(max_workers=workers) as ex:
                futs = {ex.submit(_refine_one_playbook, jb): jb for jb in jobs}
                for fut in as_completed(futs):
                    try:
                        res = fut.result()
                    except Exception as e:
                        logger.error(f"[MAIN {tag}] Worker crashed: {e}")
                        fail += 1
                        continue
                    st = res.get("status")
                    if st == "ok":
                        ok += 1
                    elif st == "skip":
                        skip += 1
                    else:
                        fail += 1
                    logger.info(f"[MAIN {tag}] {res.get('file')}: {st}" + (f" ({res.get('reason')})" if res.get('reason') else ""))
        logger.info(f"[MAIN {tag}] Playbook refinement done. OK: {ok} | Skip: {skip} | Fail: {fail}")
        logger.info("[MAIN] Script finished successfully.")
        return

    # ---------------- Plans mode (existing behavior) ----------------
    output_dirs_map = config.get("output_directories", {})
    default_output_dir = Path(output_dirs_map.get("default", "techniques"))
    matrices = config.get("matrices", ["enterprise"])
    filter_techniques = config.get("techniques", [])
    refine_guidance = config.get("refine_guidance", "").strip()
    skip_if_updated_after = config.get("skip_if_updated_after")
    skip_if_version_gt = config.get("skip_if_version_gt")

    if not output_dirs_map:
        logger.critical("Configuration key 'output_directories' is missing or empty. Cannot determine where to save files.")
        raise SystemExit(1)

    logger.info("Building technique dictionary")
    technique_dict = build_technique_dictionary(matrices)

    if filter_techniques:
        logger.info(f"Filtering for {len(filter_techniques)} specific techniques from config.")
        wanted = set(filter_techniques)
        target_techniques = {k: v for k, v in technique_dict.items() if v['technique_id'] in wanted}
    else:
        target_techniques = technique_dict

    logger.info(f"Will attempt to refine plans for up to {len(target_techniques)} techniques (existing files only).")

    jobs: List[Dict[str, Any]] = []
    for full_key, tech_data in target_techniques.items():
        matrix_type = tech_data.get("matrix")
        if matrix_type and matrix_type in output_dirs_map:
            resolved_output_dir = Path(output_dirs_map[matrix_type])
        else:
            resolved_output_dir = default_output_dir

        jobs.append({
            "full_key": full_key,
            "tech_data": tech_data,
            "output_dirs_map": output_dirs_map,
            "default_output_dir": str(default_output_dir),
            "resolved_output_dir": str(resolved_output_dir),
            "refine_guidance": refine_guidance,
            "max_retries": max_retries,
            "retry_delay": retry_delay,
            "skip_if_updated_after": skip_if_updated_after,
            "skip_if_version_gt": skip_if_version_gt,
            "make_backup": make_backup,
            "backups_dir": str(backups_dir),
            "run_ts": run_ts,
            "today_iso": today_iso,
            "sage_email": sage_email,
            "sage_api_key": sage_api_key,
            "gemini_api_key": gemini_api_key,
            "llm_provider": llm_provider,
            "llm_model": llm_model,
            "model": model,
        })

    refined_count = skipped_count = missing_count = failed_count = 0

    try:
        if num_cores is None:
            num_workers = 1
        else:
            num_workers = int(num_cores)
    except Exception:
        num_workers = 1

    main_tag = _core_tag()

    if num_workers <= 1:
        logger.info(f"[MAIN {main_tag}] Running in single-core mode.")
        for job in jobs:
            res = _worker_refine_one(job)
            status = res.get("status")
            if status == "ok":
                refined_count += 1
            elif status == "skip":
                skipped_count += 1
            elif status == "missing":
                missing_count += 1
            else:
                failed_count += 1
            logger.info(f"[MAIN {main_tag}] {res.get('technique')}: {status}" + (f" ({res.get('reason')})" if res.get('reason') else ""))
    else:
        max_cpu = os.cpu_count() or 1
        workers = max(1, min(num_workers, max_cpu))
        logger.info(f"[MAIN {main_tag}] Running in multi-core mode with {workers} workers.")
        with ProcessPoolExecutor(max_workers=workers) as ex:
            future_map = {ex.submit(_worker_refine_one, jb): jb for jb in jobs}
            for fut in as_completed(future_map):
                try:
                    res = fut.result()
                except Exception as e:
                    logger.error(f"[MAIN {main_tag}] Worker crashed: {e}")
                    failed_count += 1
                    continue
                status = res.get("status")
                if status == "ok":
                    refined_count += 1
                elif status == "skip":
                    skipped_count += 1
                elif status == "missing":
                    missing_count += 1
                else:
                    failed_count += 1
                logger.info(f"[MAIN {main_tag}] {res.get('technique')}: {status}" + (f" ({res.get('reason')})" if res.get('reason') else ""))

    logger.info(f"[MAIN {main_tag}] Refinement complete. Refined: {refined_count} | Skipped: {skipped_count} | Missing: {missing_count} | Failed: {failed_count}")
    logger.info("[MAIN] Script finished successfully.")

if __name__ == "__main__":
    main()


================================================
FILE: sherlock.py
================================================
#!/usr/bin/env python3
import logging
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Any

import pandas as pd
import yaml

# Add project root to path to allow src imports
import sys
sys.path.insert(0, str(Path(__file__).resolve().parent))

# Import local modules
from src.colorlog import make_console_handler

def setup_logging() -> tuple[str, Path]:
    """Initializes console and file logging."""
    run_ts = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")
    logs_dir = Path("logs")
    logs_dir.mkdir(parents=True, exist_ok=True)
    log_path = logs_dir / f"sherlock_{run_ts}.log"

    fmt = "%(asctime)s %(levelname)-8s %(name)s :: %(message)s"
    datefmt = "%Y-%m-%d %H:%M:%S"

    root = logging.getLogger()
    root.setLevel(logging.INFO)
    root.handlers.clear()

    # Colored console handler
    root.addHandler(make_console_handler(fmt, datefmt))

    # Plain file handler
    fh = logging.FileHandler(log_path, encoding="utf-8")
    fh.setFormatter(logging.Formatter(fmt=fmt, datefmt=datefmt))
    root.addHandler(fh)

    return run_ts, log_path

def _discover_playbooks(base_map: Dict[str, str]) -> List[Path]:
    files: List[Path] = []
    for k, v in (base_map or {}).items():
        d = Path(v)
        if d.is_dir():
            found = sorted(d.glob("*.yml"))
            logging.getLogger("main").info(f"Playbook dir [{k}] {d} -> {len(found)} file(s)")
            files.extend(found)
        else:
            logging.getLogger("main").warning(f"Configured playbook directory missing [{k}]: {d}")
    return files

def _load_playbook(path: Path) -> Dict[str, Any] | None:
    try:
        txt = path.read_text(encoding="utf-8")
        obj = yaml.safe_load(txt)
        if isinstance(obj, dict) and "questions" in obj:
            return obj
        return None
    except Exception as e:
        logging.getLogger("main").warning(f"Failed to parse playbook {path}: {e}")
        return None

def _flatten_playbooks(files: List[Path]) -> pd.DataFrame:
    rows: List[Dict[str, Any]] = []
    for p in files:
        obj = _load_playbook(p)
        if not obj:
            continue
        name = obj.get("name", "")
        pid = obj.get("id", "")
        desc = obj.get("description", "")
        ptype = obj.get("type", "")
        created = obj.get("created", "")
        modified = obj.get("modified", "")
        tags = obj.get("tags", []) or []
        tags_str = "; ".join([str(t) for t in tags])

        related = obj.get("related", []) or []
        related_str = "; ".join([str(r) for r in related])

        contributors = obj.get("contributors", []) or []
        contributors_str = "; ".join([str(c) for c in contributors])

        qlist = obj.get("questions", []) or []
        if not isinstance(qlist, list) or not qlist:
            # still emit a row to track the playbook header
            rows.append({
                "Playbook Name": name,
                "Playbook ID": pid,
                "Type": ptype,
                "Created": created,
                "Modified": modified,
                "Contributors": contributors_str,
                "Related": related_str,
                "Tags": tags_str,
                "Question #": "",
                "Question": "",
                "Context": "",
                "Answer Sources": "",
                "Range": "",
                "Queries": "",
                "File": str(p),
            })
            continue

        for idx, q in enumerate(qlist, start=1):
            question = (q or {}).get("question", "")
            context = (q or {}).get("context", "")
            ans_sources = (q or {}).get("answer_sources", []) or []
            ans_sources_str = "; ".join([str(s) for s in ans_sources])
            rng = (q or {}).get("range", "")
            queries = (q or {}).get("queries", []) or []
            if isinstance(queries, list):
                queries_str = "; ".join(
                    [f"{str(d.get('system',''))}: {str(d.get('query',''))}" if isinstance(d, dict) else str(d)
                     for d in queries]
                )
            else:
                queries_str = str(queries)

            rows.append({
                "Playbook Name": name,
                "Playbook ID": pid,
                "Type": ptype,
                "Created": created,
                "Modified": modified,
                "Contributors": contributors_str,
                "Related": related_str,
                "Tags": tags_str,
                "Question #": idx,
                "Question": question,
                "Context": context,
                "Answer Sources": ans_sources_str,
                "Range": rng,
                "Queries": queries_str,
                "File": str(p),
            })

    cols = [
        "Playbook Name","Playbook ID","Type","Created","Modified","Contributors",
        "Related","Tags","Question #","Question","Context","Answer Sources","Range","Queries","File"
    ]
    return pd.DataFrame(rows, columns=cols)

def main():
    """Aggregate and export Sherlock playbooks to Excel."""
    run_ts, log_path = setup_logging()
    logger = logging.getLogger("main")
    logger.info(f"Run initialized at: {run_ts} | Logging to: {log_path}")

    # We load generator.yml to discover where playbooks are written
    cfg_path = Path("config/generator.yml")
    if not cfg_path.exists():
        logger.critical("Missing config/generator.yml; cannot discover playbook directories.")
        return

    with open(cfg_path, "r", encoding="utf-8") as f:
        gen_cfg = yaml.safe_load(f) or {}

    playbook_dirs = gen_cfg.get("output_directories", {}) or {}
    if not playbook_dirs:
        logger.critical("'output_directories' is missing in config/generator.yml.")
        return

    files = _discover_playbooks(playbook_dirs)
    if not files:
        logger.warning("No playbooks found. Nothing to export.")
        logger.info("Script finished.")
        return

    df = _flatten_playbooks(files)
    if df.empty:
        logger.warning("Parsed 0 rows from playbooks.")
        logger.info("Script finished.")
        return

    out_dir = Path("outputs")
    out_dir.mkdir(parents=True, exist_ok=True)
    ts_suffix = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_xlsx = out_dir / f"playbooks_{ts_suffix}.xlsx"
    out_csv = out_dir / f"playbooks_{ts_suffix}.csv"

    logger.info(f"Writing Excel: {out_xlsx}")
    with pd.ExcelWriter(out_xlsx, engine="openpyxl") as writer:
        df.to_excel(writer, index=False, sheet_name="Playbooks")

    logger.info(f"Writing CSV: {out_csv}")
    df.to_csv(out_csv, index=False)

    logger.info("Script finished successfully.")

if __name__ == "__main__":
    main()


================================================
FILE: config/generator.yml
================================================
# config/generator.yml

# --- Input Settings ---
plan_paths:
  enterprise: "../watson/techniques/attack/enterprise"
  ics: "../watson/techniques/attack/ics"
  mobile: "../watson/techniques/attack/mobile"
  default: "../watson/techniques/misc"

# --- Output Settings ---
# Directories where the generated analytic plan JSON files will be saved.
# The script will use the key that matches the technique's matrix
# (e.g., 'enterprise', 'ics', 'mobile') to determine the save location.
output_directories:
  enterprise: "playbooks/attack/enterprise"
  ics: "playbooks/attack/ics"
  mobile: "playbooks/attack/mobile"
  default: "playbooks/misc"

# --- ATT&CK Matrix Selection ---
# List of ATT&CK matrices to download and process.
# Valid options: "enterprise", "mobile", "ics"
matrices:
  # - "enterprise"
  # - "mobile"
  - "ics"

# --- Technique Filtering (Optional) ---
# A list of specific technique IDs (e.g., "T1548", "T1078.001") to generate plans for.
# If the list is empty or commented out, the script will generate plans for ALL techniques
# found in the selected matrices.
techniques:
  # - "T0817"
  # - "T0846"
  # - "T0882"

model: "gemini-2.5-pro"

# Skip rules (optional)
skip_if_updated_after: "2025-09-30"   # e.g., "2025-09-01" or null to disable
skip_if_version_gt: null      # e.g., "1.2" or null to disable

# File safety
backup: true

# # Force Gemini only:
# llm_provider: gemini
# llm_model: gemini-2.5-pro

# Force AskSage only:
llm_provider: asksage
llm_model: google-gemini-2.5-pro

# # Auto (legacy behavior):
# llm_provider: auto
# llm_model: null

# LLM call behavior
max_retries: 3
retry_delay: 1

# Parallelism:
# - Set to 0, 1, or leave null to run single-core.
# - Set to 2 or more to enable multi-core; will cap at machine CPU count.
num_cores: 8


================================================
FILE: config/refine.yml
================================================
# Mode: choose whether to refine Watson-style plans (JSON) or Sherlock playbooks (YAML)
# Valid values: plans, playbooks
mode: playbooks

# ----- Plans mode (legacy Watson plan refinement) -----
# Output directories for each matrix (align these with your existing generator for plans)
output_directories:
  default: techniques
  enterprise: techniques/attack/enterprise
  ics: techniques/attack/ics
  mobile: techniques/attack/mobile

# Which matrices to consider when resolving technique metadata via attack_retriever
matrices:
  - enterprise
  # - ics
  # - mobile

# Optional: restrict to these technique IDs only (e.g., T1078, T1059, D3-NTA). Leave empty to process all found.
techniques:
  # - T0800
  # - T1078
  # - T1671

# Guidance appended to the prompt that the model MUST apply during refinement (plans mode only).
refine_guidance: |
  * Improve evidence descriptions to be measurable and testable; avoid vague phrasing.
  * Ensure data sources list precise artifacts (e.g., "Windows Event ID 4624", "Zeek conn.log", "Zeek dns.log").
  * Include at least one symbolic, one statistical, and one ML-oriented action where reasonable. Improve existing  
  * Keep actions executable by SOC analysts; be explicit but concise.
  * Do not remove required fields; preserve the top-level array-of-objects JSON schema.

# Skip rules (plans mode only)
skip_if_updated_after: null   # e.g., "2025-09-01" or null to disable
skip_if_version_gt: null      # e.g., "1.2" or null to disable

# ----- Playbooks mode (Sherlock) -----
# Directories containing Sherlock playbooks to refine
playbook_directories:
  enterprise: playbooks/attack/enterprise
  ics: playbooks/attack/ics
  mobile: playbooks/attack/mobile
  default: playbooks/misc

# File safety
backup: true

# LLM provider selection
# # Force Gemini only:
# llm_provider: gemini
# llm_model: gemini-2.5-pro

# Force AskSage only:
llm_provider: asksage
llm_model: google-gemini-2.5-pro

# # Auto (legacy behavior):
# llm_provider: auto
# llm_model: null

# Primary model (Gemini family) to try first in AUTO mode
model: gemini-2.5-pro

# LLM call behavior
max_retries: 3
retry_delay: 1

# Parallelism:
# - Set to 0, 1, or leave null to run single-core.
# - Set to 2 or more to enable multi-core; will cap at machine CPU count.
num_cores: 8


================================================
FILE: src/attack_retriever.py
================================================
import json
import logging
import os
from functools import lru_cache

import requests
from mitreattack.stix20 import MitreAttackData

logger = logging.getLogger(__name__)

MATRIX_URLS = {
    "enterprise": "https://raw.githubusercontent.com/mitre/cti/master/enterprise-attack/enterprise-attack.json",
    "mobile": "https://raw.githubusercontent.com/mitre/cti/master/mobile-attack/mobile-attack.json",
    "ics": "https://raw.githubusercontent.com/mitre/cti/master/ics-attack/ics-attack.json",
}

def _download_matrix(matrix_name: str, filename: str) -> bool:
    """Downloads and saves a MITRE ATT&CK matrix if it doesn't exist locally."""
    if os.path.exists(filename):
        logger.info(f"Using cached version of '{matrix_name}' matrix from '{filename}'.")
        return True

    url = MATRIX_URLS.get(matrix_name)
    if not url:
        logger.error(f"Invalid matrix name '{matrix_name}'. No URL found.")
        return False

    logger.info(f"Downloading '{matrix_name}' matrix from {url}...")
    try:
        response = requests.get(url, timeout=60)
        response.raise_for_status()
        with open(filename, "w", encoding='utf-8') as f:
            json.dump(response.json(), f)
        logger.info(f"Successfully saved '{matrix_name}' matrix to '{filename}'.")
        return True
    except requests.exceptions.RequestException as e:
        logger.error(f"Error downloading ATT&CK data for '{matrix_name}': {e}")
    except (json.JSONDecodeError, IOError) as e:
        logger.error(f"Error processing or saving data for '{matrix_name}': {e}")
    
    return False

@lru_cache(maxsize=None)
def _get_mitre_attack_data(matrix_name: str) -> MitreAttackData | None:
    """Initializes and caches a MitreAttackData object for a given matrix."""
    filename = f"{matrix_name}-attack.json"
    if _download_matrix(matrix_name, filename):
        try:
            return MitreAttackData(filename)
        except Exception as e:
            logger.error(f"Error initializing MitreAttackData from '{filename}': {e}")
    return None

def _extract_attack_id_from_stix(stix_obj):
    """Extracts the MITRE ATT&CK ID (e.g., T1548) from a STIX object."""
    for ref in stix_obj.get("external_references", []):
        if ref.get("source_name") == "mitre-attack":
            return ref.get("external_id")
    return None

def build_technique_dictionary(matrices: list[str]) -> dict:
    """
    Builds a comprehensive dictionary of techniques from the specified ATT&CK matrices.
    """
    technique_dict = {}
    
    for matrix_name in matrices:
        logger.info(f"Processing matrix: {matrix_name}")
        mad = _get_mitre_attack_data(matrix_name)
        if not mad:
            logger.warning(f"Skipping matrix '{matrix_name}' due to download/initialization failure.")
            continue

        tactic_name_to_id = {t.name.lower(): _extract_attack_id_from_stix(t) for t in mad.get_tactics()}
        all_techniques = mad.get_techniques(remove_revoked_deprecated=True) + \
                         mad.get_subtechniques(remove_revoked_deprecated=True)

        for tech in all_techniques:
            tid = _extract_attack_id_from_stix(tech)
            if not tid:
                continue

            name = tech.get("name", "").strip().replace("/", "-")
            full_key = f"{tid} - {name}"
            
            # Avoid overwriting if a technique (e.g., from enterprise) is already present
            if full_key in technique_dict:
                continue

            tactic_names = []
            for phase in tech.get("kill_chain_phases", []):
                # Ensure the phase belongs to a mitre-attack kill chain for the correct matrix
                if phase.get("kill_chain_name") in (f"mitre-{matrix_name}-attack", "mitre-attack"):
                    phase_name_lookup = phase.get("phase_name", "").lower().replace("-", " ")
                    tactic_id = tactic_name_to_id.get(phase_name_lookup)
                    if tactic_id:
                        tactic_display_name = phase_name_lookup.title()
                        tactic_names.append(f"{tactic_id} - {tactic_display_name}")
            
            technique_dict[full_key] = {
                "technique_id": tid,
                "name": name,
                "matrix": matrix_name,
                "tactic": ", ".join(sorted(set(tactic_names))),
                "description": tech.get("description", "").strip(),
                "detection": tech.get("x_mitre_detection", "").strip(),
            }

    logger.info(f"Built dictionary with {len(technique_dict)} unique techniques across {len(matrices)} matrices.")
    return technique_dict


================================================
FILE: src/colorlog.py
================================================
# src/colorlog.py
from __future__ import annotations
import logging
import os
import sys
from typing import Dict, Any, Optional

# Optional: enable ANSI colors on Windows if colorama is present.
try:
    import colorama  # type: ignore
    colorama.just_fix_windows_console()
except Exception:
    pass

ANSI_CODES = {
    "reset": "\x1b[0m",
    "bold": "\x1b[1m",
    "dim": "\x1b[2m",
    "underline": "\x1b[4m",
    "black": "\x1b[30m",
    "red": "\x1b[31m",
    "green": "\x1b[32m",
    "yellow": "\x1b[33m",
    "blue": "\x1b[34m",
    "magenta": "\x1b[35m",
    "cyan": "\x1b[36m",
    "white": "\x1b[37m",
    "bg_black": "\x1b[40m",
    "bg_red": "\x1b[41m",
    "bg_green": "\x1b[42m",
    "bg_yellow": "\x1b[43m",
    "bg_blue": "\x1b[44m",
    "bg_magenta": "\x1b[45m",
    "bg_cyan": "\x1b[46m",
    "bg_white": "\x1b[47m",
}

LEVEL_COLORS = {
    logging.DEBUG:  ("blue", None, False),
    logging.INFO:   ("green", None, False),
    logging.WARNING:("yellow", None, True),
    logging.ERROR:  ("red", None, True),
    logging.CRITICAL:("white", "bg_red", True),
}

def supports_color(stream) -> bool:
    try:
        # Only color when writing to a real TTY
        return hasattr(stream, "isatty") and stream.isatty()
    except Exception:
        return False

def c(text: str, *, fg: Optional[str]=None, bg: Optional[str]=None, bold: bool=False, underline: bool=False, dim: bool=False) -> str:
    """
    Manually colorize a substring. Use in your message like:
      logger.info("hello %s", c("world", fg="cyan", bold=True))
    """
    parts = []
    if bold: parts.append(ANSI_CODES["bold"])
    if dim: parts.append(ANSI_CODES["dim"])
    if underline: parts.append(ANSI_CODES["underline"])
    if fg and fg in ANSI_CODES: parts.append(ANSI_CODES[fg])
    if bg and bg in ANSI_CODES: parts.append(ANSI_CODES[bg])
    start = "".join(parts)
    end = ANSI_CODES["reset"] if parts else ""
    return f"{start}{text}{end}"

class ConsoleColoredFormatter(logging.Formatter):
    """
    Colors console output. File logs should use a plain formatter.

    How to color specific lines:
      logger.info("Message", extra={"msg_color":"cyan","msg_bold":True,"msg_underline":False,"msg_bg":"bg_black"})
    If not provided, it colors level name based on LEVEL_COLORS and leaves message plain.
    """
    def __init__(self, fmt: str, datefmt: str | None = None, *, colorize: bool = True, color_levelnames: bool = True):
        super().__init__(fmt=fmt, datefmt=datefmt)
        self.colorize = colorize
        self.color_levelnames = color_levelnames

    def format(self, record: logging.LogRecord) -> str:
        # Save originals
        orig_levelname = record.levelname
        orig_msg = record.getMessage()

        # Apply colors only if enabled and stream supports it (checked when handler is created)
        if self.colorize:
            # 1) Colorize LEVELNAME
            if self.color_levelnames:
                fg, bg, bold = LEVEL_COLORS.get(record.levelno, ("white", None, False))
                record.levelname = c(orig_levelname, fg=fg, bg=bg, bold=bold)

            # 2) Colorize message if requested via extra
            fg = getattr(record, "msg_color", None)
            bg = getattr(record, "msg_bg", None)
            bold = bool(getattr(record, "msg_bold", False))
            underline = bool(getattr(record, "msg_underline", False))
            dim = bool(getattr(record, "msg_dim", False))

            # If user supplied any styling flag, colorize whole message
            if any([fg, bg, bold, underline, dim]):
                colored_msg = c(orig_msg, fg=fg, bg=bg, bold=bold, underline=underline, dim=dim)
            else:
                # Otherwise leave message as-is; user can still embed c("...") manually.
                colored_msg = orig_msg

            # Inject a temporary attribute for the format string
            record.__dict__["_colored_message"] = colored_msg
            # Use %(message)s normally (we'll swap below)
            out = super().format(record)
            # Reset modified fields
            record.levelname = orig_levelname
            return out.replace(orig_msg, colored_msg, 1)
        else:
            # No color
            record.levelname = orig_levelname
            return super().format(record)

def make_console_handler(fmt: str, datefmt: str) -> logging.Handler:
    stream = sys.stdout
    colorize = supports_color(stream)
    handler = logging.StreamHandler(stream)
    handler.setLevel(logging.INFO)
    handler.setFormatter(ConsoleColoredFormatter(fmt=fmt, datefmt=datefmt, colorize=colorize))
    return handler



================================================
FILE: src/formatting.py
================================================
# src/formatting.py
import logging
from typing import Dict, List, Tuple, Iterable, Optional
import pandas as pd
from openpyxl.styles import Alignment

logger = logging.getLogger(__name__)

def export_simple_excel(df: pd.DataFrame, path, column_widths_pixels: Optional[Dict[str, int]] = None) -> None:
    with pd.ExcelWriter(path, engine="openpyxl") as writer:
        df.to_excel(writer, index=False, sheet_name="ASOM")
        ws = writer.sheets["ASOM"]
        if column_widths_pixels:
            _apply_pixel_widths(ws, list(df.columns), column_widths_pixels)

def export_with_merged_cells(df: pd.DataFrame, path, column_widths_pixels: Optional[Dict[str, int]] = None) -> None:
    """
    Merge hierarchy spans. If some expected columns are missing (e.g., due to filtering),
    gracefully skip those merge levels with a warning.
    """
    if df.empty:
        logger.warning("export_with_merged_cells: empty DataFrame.")
        with pd.ExcelWriter(path, engine="openpyxl") as writer:
            df.to_excel(writer, index=False, sheet_name="ASOM")
        return

    with pd.ExcelWriter(path, engine="openpyxl") as writer:
        df.to_excel(writer, index=False, sheet_name="ASOM")
        ws = writer.sheets["ASOM"]
        headers = list(df.columns)

        def merge_by_group(group_cols: List[str], merge_cols: List[str], level_name: str) -> None:
            present_group_cols = [c for c in group_cols if c in df.columns]
            present_merge_cols = [c for c in merge_cols if c in df.columns]

            missing_g = [c for c in group_cols if c not in df.columns]
            missing_m = [c for c in merge_cols if c not in df.columns]
            if missing_g or missing_m:
                logger.warning(
                    f"Merging: {level_name}: skipping missing columns "
                    f"(group keys missing={missing_g}, merge cols missing={missing_m})."
                )

            # If there are no group cols or no merge cols present, nothing to do.
            if not present_group_cols or not present_merge_cols:
                return

            # Iterate contiguous spans
            start = 0
            prev = tuple(df.iloc[0][present_group_cols]) if len(df) > 0 else None

            def flush(s: int, e: int):
                if e <= s:
                    return
                top = 2 + s  # header row = 1
                bot = 2 + e
                for mc in present_merge_cols:
                    c = headers.index(mc) + 1
                    ws.merge_cells(start_row=top, start_column=c, end_row=bot, end_column=c)

            for i in range(len(df)):
                key = tuple(df.iloc[i][present_group_cols])
                if key != prev:
                    flush(start, i - 1)
                    start = i
                    prev = key
            flush(start, len(df) - 1)

        # Merge levels (expected full hierarchy)
        merge_by_group(["CCIR Index"], ["CCIR Index", "CCIR"], "CCIR-level")
        merge_by_group(["CCIR Index", "Tactic ID", "Tactic Name"], ["Tactic ID", "Tactic Name"], "Tactic-level")
        merge_by_group(
            ["CCIR Index", "Tactic ID", "Tactic Name", "Indicator Index"],
            ["Indicator Index", "Indicator", "Technique ID", "Technique Name"],
            "Indicator-level"
        )
        merge_by_group(
            ["CCIR Index", "Tactic ID", "Tactic Name", "Indicator Index", "Evidence Index"],
            ["Evidence Index", "Evidence Description", "Data Sources", "Data Platforms", "NAI"],
            "Evidence-level"
        )

        if column_widths_pixels:
            _apply_pixel_widths(ws, headers, column_widths_pixels)

        # Enable wrap text for all populated cells (merged sheet only)
        max_row = ws.max_row
        max_col = ws.max_column
        wrap_align = Alignment(wrap_text=True, vertical="top")
        for r in range(1, max_row + 1):         # include header row if you want header wrapping
            for c in range(1, max_col + 1):
                cell = ws.cell(row=r, column=c)
                cell.alignment = wrap_align

def _apply_pixel_widths(ws, headers: List[str], column_widths_pixels: Dict[str, int]) -> None:
    PIXELS_PER_CHAR = 7.0
    from openpyxl.utils import get_column_letter
    for col_name, px in column_widths_pixels.items():
        if col_name not in headers:
            continue
        col_letter = get_column_letter(headers.index(col_name) + 1)
        ch = max(6, int(round(px / PIXELS_PER_CHAR)))
        ws.column_dimensions[col_letter].width = ch

def _col_letter(ws, col_index_1based: int) -> str:
    from openpyxl.utils import get_column_letter
    return get_column_letter(col_index_1based)


def _merge_spans(
    ws,
    df: pd.DataFrame,
    group_on: List[str],
    merge_cols: List[str],
    headers: List[str]
) -> None:
    """
    Find contiguous blocks where 'group_on' fields are identical and merge each of the 'merge_cols'
    across the corresponding row span.
    """
    if any(col not in df.columns for col in group_on):
        return

    # We'll iterate contiguous spans by comparing rows with previous row
    start = 0  # df index (0-based)
    prev_key = tuple(df.iloc[0][group_on]) if not df.empty else None

    def flush_span(s: int, e: int):
        # Merge each column in merge_cols across Excel rows 2+s .. 2+e (header = row 1)
        if e <= s:  # single row - no merge needed
            return
        top_row = 2 + s
        bot_row = 2 + e
        for col in merge_cols:
            if col not in headers:
                continue
            c = headers.index(col) + 1
            ws.merge_cells(start_row=top_row, start_column=c, end_row=bot_row, end_column=c)

    for i in range(len(df)):
        cur_key = tuple(df.iloc[i][group_on])
        if cur_key != prev_key:
            # close previous span [start, i-1]
            flush_span(start, i - 1)
            start = i
            prev_key = cur_key
    # flush last span
    flush_span(start, len(df) - 1)



================================================
FILE: src/llm.py
================================================
# src/llm.py
import logging, os, re, time
from typing import Dict, Optional
from google import genai
from google.genai import types, errors

logger = logging.getLogger(__name__)

def refine_with_llm(
    prompt: str,
    ask_sage_client,
    *,
    provider_pref: Optional[str],
    model: Optional[str],
    max_retries: int = 3,
    retry_delay: int = 1,
    gemini_primary_model: Optional[str] = None,
) -> Dict[str, str]:
    """
    Shared LLM call surface. Returns:
      {"text": <output>, "endpoint": "gemini" | "asksage", "model_used": <name>}
    """
    if not prompt or not isinstance(prompt, str):
        raise ValueError("Prompt must be a non-empty string")

    def _call_gemini(gem_model: str) -> Dict[str, str]:
        api_key = os.environ.get("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY not set")
        client = genai.Client(api_key=api_key)
        resp = client.models.generate_content(
            model=gem_model,
            contents=[types.Content(role="user", parts=[types.Part.from_text(text=prompt)])],
            config=types.GenerateContentConfig(temperature=0.7),
        )
        if not resp or not getattr(resp, "text", None):
            raise ValueError("Invalid/empty response from Gemini")
        return {"text": resp.text, "endpoint": "gemini", "model_used": gem_model}

    def _call_asksage(as_model: str) -> Dict[str, str]:
        resp = ask_sage_client.query(
            prompt, persona="default", dataset="none",
            limit_references=0, temperature=0.7, live=0,
            model=as_model, system_prompt=None,
        )
        if not resp or not isinstance(resp, dict) or not resp.get("message"):
            raise ValueError("Invalid/empty response from AskSage")
        return {"text": resp["message"], "endpoint": "asksage", "model_used": as_model}

    if provider_pref in {"gemini", "asksage"} and model:
        return _call_gemini(model) if provider_pref == "gemini" else _call_asksage(model)

    primary_gemini_model = gemini_primary_model or (model if model else "gemini-2.5-pro")
    for attempt in range(max_retries):
        try:
            return _call_gemini(primary_gemini_model)
        except (errors.ClientError, errors.APIError, ValueError) as e:
            msg = str(e); code = getattr(e, "status_code", None)
            retriable = code == 429 or "429" in msg or "RESOURCE_EXHAUSTED" in msg or "quota" in msg.lower() or "rate" in msg.lower()
            if retriable:
                break
            delay = retry_delay * (2 ** attempt)
            m = re.search(r'retry in (\d+(?:\.\d+)?)', msg.lower())
            if m:
                try: delay = min(float(m.group(1)) + 1, 120)
                except: pass
            if attempt < max_retries - 1:
                time.sleep(delay)
    return _call_asksage("google-gemini-2.5-pro")



================================================
FILE: src/processing.py
================================================
import copy
import json
import logging
import re
from pathlib import Path
from typing import Any, Dict, List, Set, Tuple

import pandas as pd

logger = logging.getLogger(__name__)

TECHNIQUE_ID_PATTERN = re.compile(r"^(?P<tech_id>T\d{4}(?:\.\d{3})?|D3-[A-Z]+)")

def _normalize_tactic_key(tactic: str) -> Tuple[str, str]:
    if " - " in tactic:
        tid, name = tactic.split(" - ", 1)
        return tid.strip(), name.strip()
    return tactic.strip(), ""

def _normalize_technique_id(tech: str) -> str:
    m = TECHNIQUE_ID_PATTERN.match(tech.strip())
    return m.group("tech_id") if m else ""

def _load_json_safely(path: Path) -> Any:
    text = path.read_text(encoding="utf-8").strip()
    if text.startswith("```"):
        text = re.sub(r"^```(?:json)?\s*", "", text)
        text = re.sub(r"\s*```$", "", text)
    return json.loads(text)

def _is_new_schema_object(obj: dict) -> bool:
    required = {"information_requirement", "tactic_id", "tactic_name", "indicators"}
    return isinstance(obj, dict) and required.issubset(obj.keys())

def _filter_indicators(ir_obj: dict, allowed_ids: Set[str]) -> dict:
    if not allowed_ids:
        return ir_obj

    new_obj = copy.deepcopy(ir_obj)
    new_indicators = [
        ind for ind in new_obj.get("indicators", [])
        if _normalize_technique_id(ind.get("technique_id", "")) in allowed_ids
    ]
    new_obj["indicators"] = new_indicators
    return new_obj

def _append_suffix_if_missing(base: str, suffix_id: str, suffix_name: str) -> str:
    """
    Append ' (ID - Name)' to base if both are present and the exact suffix
    isn't already there. Returns the possibly-modified string.
    """
    base = (base or "").strip()
    sid = (suffix_id or "").strip()
    sname = (suffix_name or "").strip()
    if not sid or not sname:
        return base
    suffix = f" ({sid} - {sname})"
    # Avoid double-appending if it's already present (anywhere in the string)
    if suffix in base:
        return base
    return f"{base}{suffix}"

def build_asom(
    detect_chain: Dict[str, List[str]],
    attack_chain: Dict[str, List[str]],
    directories: List[Path],
    filter_indicators: bool = True,
    deduplicate: bool = True
) -> List[dict]:
    """
    Builds an ASOM by processing detect and attack chains against analytic plan files.

    The function processes all JSON files in the given list of directories, filters
    them based on the combined tactics and techniques from both chains, and then sorts
    the results to ensure that items from the 'detect_chain' appear first.
    """
    # Combine chains and create a master map of all tactics and techniques
    full_chain = {**detect_chain, **attack_chain}
    tactic_map: Dict[str, Set[str]] = {}
    for tactic_str, technique_list in full_chain.items():
        tactic_id, _ = _normalize_tactic_key(tactic_str)
        norm_tecs = {_normalize_technique_id(t) for t in technique_list}
        norm_tecs = {t for t in norm_tecs if t}
        tactic_map.setdefault(tactic_id, set()).update(norm_tecs)

    # This list preserves the order of detect tactics for final sorting
    detect_tactic_ids_ordered = [_normalize_tactic_key(t)[0] for t in detect_chain.keys()]

    # Gather all .json files from all specified directories
    all_files: List[Path] = []
    for directory in directories:
        if directory.is_dir():
            all_files.extend(directory.glob("*.json"))
        else:
            logger.warning(f"Directory '{directory}' specified in config does not exist. Skipping.")

    # Process all JSON files and collect all matching IR objects
    all_results: List[dict] = []
    for path in sorted(all_files):
        try:
            data = _load_json_safely(path)
        except Exception as e:
            logger.warning(f"Could not parse {path.name}: {e}")
            continue

        if not isinstance(data, list):
            continue

        for obj in data:
            if not _is_new_schema_object(obj):
                logger.debug(f"Object in {path.name} does not conform to schema. Skipping.")
                continue

            tactic_id = obj.get("tactic_id", "").strip()
            if tactic_id not in tactic_map:
                continue

            if filter_indicators:
                filtered_obj = _filter_indicators(copy.deepcopy(obj), tactic_map[tactic_id])
                if tactic_map[tactic_id] and not filtered_obj.get("indicators"):
                    continue
                all_results.append(filtered_obj)
            else:
                all_results.append(copy.deepcopy(obj))

    # Sort results: detect chain tactics first (in order), then attack chain tactics
    def sort_key(ir_obj):
        tactic_id = ir_obj.get("tactic_id")
        if tactic_id in detect_tactic_ids_ordered:
            # Primary sort key 0 for detect, secondary is its position in the config
            return (0, detect_tactic_ids_ordered.index(tactic_id))
        else:
            # Primary sort key 1 for attack, secondary is alphabetical by ID
            return (1, tactic_id)

    all_results.sort(key=sort_key)
    
    # Deduplicate after sorting to ensure the first-occurring item is kept
    if deduplicate:
        seen = set()
        unique_results: List[dict] = []
        for obj in all_results:
            key = (obj.get("information_requirement"), obj.get("tactic_id"))
            if key in seen:
                continue
            seen.add(key)
            unique_results.append(obj)
        return unique_results
    
    return all_results


ORDINAL_LABELS = ["First action", "Second action", "Third action"]

def _distinct_preserve_order(xs: List[str]) -> List[str]:
    seen = set()
    out = []
    for x in xs:
        if x not in seen:
            seen.add(x)
            out.append(x)
    return out

def _normalize_actions_no_pad(action_field: Any, *, context: str) -> List[str]:
    """
    Return up to three DISTINCT actions, preserving order. Do NOT pad with blanks.
    """
    if isinstance(action_field, list):
        actions = [str(a).strip() for a in action_field if a is not None and str(a).strip() != ""]
        actions = _distinct_preserve_order(actions)
        if len(actions) > 3:
            logger.warning(f"{context}: 'action' had {len(actions)} entries; truncating to 3.")
            actions = actions[:3]
        return actions
    elif isinstance(action_field, str):
        s = action_field.strip()
        return [s] if s else []
    elif action_field is None:
        return []
    else:
        logger.warning(f"{context}: 'action' not a list/string; skipping.")
        return []

def format_asom(raw_asom: List[Dict[str, Any]]) -> pd.DataFrame:
    """
    Flatten nested ASOM (with 'action' as a list) into one row per ACTION (1..3).
    No blank padding; an evidence with 1 or 2 actions produces 1 or 2 rows respectively.

    Columns:
      - CCIR Index
      - CCIR
      - Tactic ID
      - Tactic Name
      - Indicator Index   (TEMP, will be re-numbered later)
      - Indicator
      - Technique ID
      - Technique Name
      - Evidence Index    (TEMP, will be re-numbered later)
      - Evidence Description
      - Data Sources
      - Data Platforms
      - NAI
      - Action Label
      - Action
    """
    rows: List[Dict[str, Any]] = []

    if not isinstance(raw_asom, list):
        logger.error("format_asom: expected a list at the top level; returning empty frame.")
        return pd.DataFrame(columns=[
            "CCIR Index","CCIR","Tactic ID","Tactic Name","Indicator Index","Indicator",
            "Technique ID","Technique Name","Evidence Index","Evidence Description",
            "Data Sources","Data Platforms","NAI","Action Label","Action"
        ])

    for ccir_idx, ccir_obj in enumerate(raw_asom, start=1):
        ccir = str(ccir_obj.get("information_requirement", "")).strip()
        tactic_id = str(ccir_obj.get("tactic_id", "")).strip()
        tactic_name = str(ccir_obj.get("tactic_name", "")).strip()

        # Enrich CCIR with "(TacticID - Tactic Name)" if not already present
        ccir_display = _append_suffix_if_missing(ccir, tactic_id, tactic_name)
        
        indicators = ccir_obj.get("indicators", []) or []
        if not isinstance(indicators, list):
            logger.warning(f"CCIR {ccir_idx}: 'indicators' not a list; skipping.")
            continue

        for ind_temp_idx, ind in enumerate(indicators, start=1):
            # Schema shows: technique_id + name at indicator level
            tech_id = str(ind.get("technique_id", "")).strip()
            indicator_name = str(ind.get("name", "")).strip()
            tech_name = str(ind.get("name", "")).strip()  # If you have a separate technique name, swap here

            # Enrich Indicator with "(TechniqueID - Technique Name)" if not already present
            indicator_display = _append_suffix_if_missing(indicator_name, tech_id, tech_name)
            
            evidence_list = ind.get("evidence", []) or []
            if not isinstance(evidence_list, list):
                logger.warning(f"CCIR {ccir_idx} Indicator {ind_temp_idx}: 'evidence' not a list; skipping.")
                continue

            for ev_temp_idx, ev in enumerate(evidence_list, start=1):
                description = str(ev.get("description", "")).strip()

                data_sources = ev.get("data_sources", []) or []
                if not isinstance(data_sources, list):
                    data_sources = [str(data_sources)]
                data_sources_str = "; ".join([str(s) for s in data_sources])

                data_platforms = ev.get("data_platforms", []) or []
                if not isinstance(data_platforms, list):
                    data_platforms = [str(data_platforms)]
                data_platforms_str = "; ".join([str(p) for p in data_platforms])

                nai = str(ev.get("nai", "")).strip()

                actions = _normalize_actions_no_pad(
                    ev.get("action"),
                    context=f"CCIR {ccir_idx}, IND {ind_temp_idx}, EVID {ev_temp_idx}"
                )

                # If no actions, still emit a single row so the evidence is not lost (Action columns blank)
                if not actions:
                    rows.append({
                        "CCIR Index": ccir_idx,
                        "CCIR": ccir_display,
                        "Tactic ID": tactic_id,
                        "Tactic Name": tactic_name,
                        "Indicator Index": ind_temp_idx,   # temp
                        "Indicator": indicator_display,
                        "Technique ID": tech_id,
                        "Technique Name": tech_name,
                        "Evidence Index": ev_temp_idx,     # temp
                        "Evidence Description": description,
                        "Data Sources": data_sources_str,
                        "Data Platforms": data_platforms_str,
                        "NAI": nai,
                        "Action Index": 1,
                        "Action": "",
                    })
                else:
                    for a_i, a_text in enumerate(actions, start=1):
                        rows.append({
                            "CCIR Index": ccir_idx,
                            "CCIR": ccir_display,
                            "Tactic ID": tactic_id,
                            "Tactic Name": tactic_name,
                            "Indicator Index": ind_temp_idx,   # temp; renumbered later
                            "Indicator": indicator_display,
                            "Technique ID": tech_id,
                            "Technique Name": tech_name,
                            "Evidence Index": ev_temp_idx,     # temp; renumbered later
                            "Evidence Description": description,
                            "Data Sources": data_sources_str,
                            "Data Platforms": data_platforms_str,
                            "NAI": nai,
                            "Action Index": a_i,               # <— integer index, 1..N
                            "Action": a_text,
                        })

    df = pd.DataFrame(rows, columns=[
        "CCIR Index","CCIR","Tactic ID","Tactic Name",
        "Indicator Index","Indicator","Technique ID","Technique Name",
        "Evidence Index","Evidence Description","Data Sources","Data Platforms","NAI",
        "Action Index","Action"   # <— updated
    ])
    return df

def renumber_formatted_df(df: pd.DataFrame) -> pd.DataFrame:
    """
    Sorts rows and re-numbers:
      - Indicator Index: 1..N within each (CCIR Index, Tactic ID, Tactic Name) for each distinct (Indicator, Technique ID, Technique Name) triplet in order of first appearance.
      - Evidence Index:  1..M within each (CCIR Index, Tactic ID, Tactic Name, Indicator Index) for each distinct (Evidence Description, Data Sources, Data Platforms, NAI).

    Ensures contiguity so merges work across consecutive rows.
    """
    if df.empty:
        return df

    # Re-number Indicator Index
    def renumber_indicator(group: pd.DataFrame) -> pd.Series:
        # Key per indicator within the tactic
        keys = list(zip(group["Indicator"], group["Technique ID"], group["Technique Name"]))
        key_to_num = {}
        next_id = 1
        mapped = []
        for k in keys:
            if k not in key_to_num:
                key_to_num[k] = next_id
                next_id += 1
            mapped.append(key_to_num[k])
        return pd.Series(mapped, index=group.index)

    # Re-number Evidence Index within each Indicator Index block
    def renumber_evidence(group: pd.DataFrame) -> pd.Series:
        keys = list(zip(group["Evidence Description"], group["Data Sources"], group["Data Platforms"], group["NAI"]))
        key_to_num = {}
        next_id = 1
        mapped = []
        for k in keys:
            if k not in key_to_num:
                key_to_num[k] = next_id
                next_id += 1
            mapped.append(key_to_num[k])
        return pd.Series(mapped, index=group.index)
    
    # initial deterministic sort (stable)
    df = df.sort_values(
        by=[
            "CCIR Index", "Tactic ID", "Tactic Name",
            "Indicator", "Technique ID", "Technique Name",
            "Evidence Description", "Data Sources", "Data Platforms", "NAI", "Action Index"
        ],
        kind="mergesort"
    ).reset_index(drop=True)
    
    # Re-number Indicator Index (silence FutureWarning with include_groups=False)
    df["Indicator Index"] = (
        df.groupby(["CCIR Index","Tactic ID","Tactic Name"], sort=False)
          .apply(renumber_indicator, include_groups=False)
          .reset_index(level=[0,1,2], drop=True)
    )
    
    # Re-number Evidence Index inside each Indicator Index
    df["Evidence Index"] = (
        df.groupby(["CCIR Index","Tactic ID","Tactic Name","Indicator Index"], sort=False)
          .apply(renumber_evidence, include_groups=False)
          .reset_index(level=[0,1,2,3], drop=True)
    )
    
    # Final contiguous sort
    df = df.sort_values(
        by=[
            "CCIR Index", "Tactic ID", "Tactic Name",
            "Indicator Index", "Evidence Index", "Action Index"
        ],
        kind="mergesort"
    ).reset_index(drop=True)

    return df

