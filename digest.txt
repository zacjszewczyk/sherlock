Directory structure:
└── analytic-plans/
    ├── Analytic Plan Generator.py
    ├── ASOM Generator.py
    └── Edit Plans.py

================================================
FILE: Analytic Plan Generator.py
================================================
#!/usr/bin/env python
# coding: utf-8

# # Analytic Plan Generator
# 
# This notebook uses artificial intelligence to generate analytic plans.
# 
# ## Background
# 
# As described in TC 3-12.2.4.1, "The Analytic Scheme of Maneuver is the plan to collect and analyze technical data to meet specific information requirements. It identifies what data to analyze, how to analyze it, and why it is being analyzed." The analytic scheme of maneuver, or ASOM, consists of the following components:
# 
# * **Priority information requirement**:
# * **Indicator**:
# * **Evidence**:
# * **Data**:
# * **NAI**:
# * **Analytic**:
# 
# 
# 
# ## Environment Setup
# 
# This section sets up the environment. It installs packages necessary to generate the analytic plans, imports modules, initializes helper functions, and finally defines global variables. This section also mounts Google Drive to the runtime and moves into the project folder.
# 
# ### Install Packages

# In[ ]:


get_ipython().system('pip install -U -q "google" 1> /dev/null')
get_ipython().system('pip install -U -q "google.genai" 1> /dev/null')
get_ipython().system('pip install mitreattack-python 1> /dev/null')


# ### Import Modules

# In[ ]:


import os
from google.colab import userdata
from google.colab import drive
import requests
from mitreattack.stix20 import MitreAttackData
import json
import base64
from google import genai
from google.genai import types
import datetime
import time
import sys
import multiprocessing


# ### Initialize Helper Functions

# In[ ]:


def log(message, end="\n", flush = True):
    """
    Logs a message to the console, prepended with the current timestamp
    in ISO 8601 format.

    Args:
    message (str): The string message to log.
    """

    # Access the global flag controlling verbosity
    global verbose

    # Get the current date and time
    current_time = datetime.datetime.now()

    # Format the timestamp in ISO 8601 format
    timestamp = current_time.isoformat()

    # Construct the final log string using an f-string for clean formatting
    log_string = f"[{timestamp}] {message}"

    # Print the log string to the console if logging is turned on (verbose = True)
    if (verbose == True):
        print(log_string, end = end, flush = flush)


# In[ ]:


def extract_attack_id_from_stix(stix_obj):
    """Extracts the MITRE ATT&CK ID (e.g., T1548, TA0002) from a STIX object.

    Iterates through the 'external_references' list of a STIX object (like a technique,
    tactic, or software object) and returns the 'external_id' associated with the
    'mitre-attack' source name.

    Args:
        stix_obj (dict): A dictionary representing a STIX object, expected to have
                         an 'external_references' key.

    Returns:
        str | None: The MITRE ATT&CK external ID if found, otherwise None.
    """
    # Ensure 'external_references' exists and is iterable; default to empty list if not found.
    for ref in stix_obj.get("external_references", []):
        # Check if the reference source is specifically 'mitre-attack'.
        if ref.get("source_name") == "mitre-attack":
            # Return the associated external ID.
            return ref.get("external_id")
    # Return None if no 'mitre-attack' external reference was found.
    return None


# ### Define Global Variables

# In[ ]:


# Toggle logging on (verbose = True)/off (verbose = False)
verbose = True
# verbose = False


# In[ ]:


# Rate limits: https://ai.google.dev/gemini-api/docs/rate-limits
# Pricing: https://ai.google.dev/gemini-api/docs/pricing
# Usage: https://console.cloud.google.com/apis/api/generativelanguage.googleapis.com/metrics?project=gen-lang-client-0497172401
# Note that this notebook is designed to be run in Google Colab. The line below reads the Gemini API key for AI Studio,
# which is configured in the Secrets tab on the left side of the Colab window.
os.environ["GEMINI_API_KEY"] = userdata.get("GOOGLE_API_KEY")
log("Gemii API key loaded.")


# ### Mount Google Drive

# In[ ]:


# Mount Google Drive and move into the Google AI Studio folder
DRIVE_PATH = "/content/drive"
TECHNIQUES_PATH = "/content/drive/MyDrive/Google AI Studio/techniques"

drive.mount(DRIVE_PATH)
log(f"Google Drive mounted to {DRIVE_PATH}")

os.chdir(TECHNIQUES_PATH)
log(f"Changed directory to {TECHNIQUES_PATH}")


# ## ATT&CK Matrix Retrieval
# 
# This section first retrieves the latest MITRE ATT&CK Matrix for Enterprise from MITRE's GitHub repository. Then, it builds a data structure that stores each technique as they key with it's parent tactic, the technique's description, and detection suggestions as a nested dictionary as the value.

# In[ ]:


# Define the local filename for the MITRE ATT&CK data
attack_data_filename = "enterprise-attack.json"
# Define the source URL for the MITRE ATT&CK Enterprise data
attack_data_url = "https://raw.githubusercontent.com/mitre/cti/master/enterprise-attack/enterprise-attack.json"

# Check if the MITRE ATT&CK data file already exists locally.
# This avoids redundant downloads on subsequent runs, saving time and bandwidth.
# It acts as a simple caching mechanism.
if not os.path.exists(attack_data_filename):
    log(f"Local file '{attack_data_filename}' not found.")
    log (f"Downloading MITRE ATT&CK data from {attack_data_url}")
    try:
        # Attempt to download the latest enterprise ATT&CK framework definition from the official MITRE CTI GitHub repository.
        attack_response = requests.get(attack_data_url)
        # Raise an exception if the download request was unsuccessful (e.g., 404 Not Found, 500 Server Error).
        attack_response.raise_for_status()
        # Get the text content of the successful response.
        attack_text = attack_response.text

        # Parse the downloaded JSON text into a Python dictionary/list structure.
        # This allows programmatic access to the ATT&CK data.
        attack_json = json.loads(attack_text)

        # Save the downloaded and parsed data to the local file.
        # Using 'w' mode ensures that if the file somehow existed but os.path.exists returned False (e.g., race condition, though unlikely here),
        # it would be overwritten. Opening with 'utf-8' encoding is generally good practice for JSON.
        # The 'indent=4' argument makes the saved JSON file human-readable (pretty-printed).
        with open(attack_data_filename, "w", encoding='utf-8') as outfile:
            json.dump(attack_json, outfile, indent=4)
        log(f"Successfully downloaded and saved data to '{attack_data_filename}'.")

    except requests.exceptions.RequestException as e:
        # Handle potential network errors during download (e.g., connection error, timeout, bad HTTP status).
        log(f"Error downloading ATT&CK data: {e}")
        # Depending on the application's needs, might want to exit, retry, or proceed without the data.
        # For now, we'll just print the error and potentially fail later when MitreAttackData is initialized.
    except json.JSONDecodeError as e:
        # Handle cases where the downloaded content is not valid JSON.
        log(f"Error parsing downloaded JSON data: {e}")
    except IOError as e:
        # Handle potential errors during file writing (e.g., permissions denied, disk full).
        log(f"Error writing data to file '{attack_data_filename}': {e}")

# Initialize the MitreAttackData object using the local JSON file.
# This object provides methods to easily query and navigate the ATT&CK framework data
# (e.g., find techniques, tactics, mitigations, relationships).
# It encapsulates the logic for interacting with the raw JSON data structure.
# Ensure the file exists before attempting to initialize, or handle potential FileNotFoundError within MitreAttackData.
if os.path.exists(attack_data_filename):
    try:
        mad = MitreAttackData(attack_data_filename)
        log(f"MitreAttackData object initialized successfully from '{attack_data_filename}'.")
        # Now the 'mad' object can be used to work with the ATT&CK data.
        # Example (assuming methods exist):
        # techniques = mad.get_techniques()
        # tactic_details = mad.get_tactic_by_id('TA0001')
    except Exception as e:
        # Catch any potential errors during MitreAttackData initialization (e.g., file format issues not caught earlier, internal errors).
        log(f"Error initializing MitreAttackData from '{attack_data_filename}': {e}")
        # Set mad to None or handle appropriately to indicate failure.
        mad = None
else:
    # This branch is reached if the download/file writing failed in the 'if not os.path.exists' block.
    log(f"Cannot initialize MitreAttackData: File '{attack_data_filename}' is missing and could not be created.")
    sys.exit(1)


# Now that the script has retrieved the MITRE ATT&CK Matrix for Enterprise, transform it into a new data structure for ease of reference later.

# In[ ]:


# Create a dictionary mapping lowercase tactic names (e.g., "initial access")
# to their corresponding MITRE ATT&CK Tactic IDs (e.g., "TA0001").
# This provides a quick lookup for tactic IDs based on names found in technique data.
# It assumes the first external reference in a tactic object contains the desired ID.
tactic_name_to_id = {
    # Convert tactic name to lowercase for case-insensitive matching later.
    tactic.name.lower(): tactic.external_references[0]["external_id"]
    # Iterate through all tactic objects provided by the MitreAttackData instance.
    for tactic in mad.get_tactics()
}

# Initialize the dictionary that will store the processed technique/sub-technique information.
# The keys will be formatted strings like "T1548.002 - Unsecured Credentials",
# and the values will be dictionaries containing tactic, description, and detection details.
technique_dict = {}

# Retrieve both techniques and sub-techniques from the MitreAttackData instance.
# Exclude any techniques/sub-techniques marked as revoked or deprecated in the source data.
# Combine them into a single list for uniform processing.
all_techniques = mad.get_techniques(remove_revoked_deprecated=True) + \
                 mad.get_subtechniques(remove_revoked_deprecated=True)

# Iterate through each technique or sub-technique object in the combined list.
for tech in all_techniques:
    # Extract the primary ATT&CK ID (e.g., "T1548" or "T1548.002") using the helper function.
    tid = extract_attack_id_from_stix(tech)
    # Skip processing if a valid ATT&CK ID couldn't be extracted.
    if not tid:
        continue # Or add logging/error handling

    # Extract the name, remove leading/trailing whitespace, and replace slashes
    # (which might cause issues in file paths or keys) with hyphens.
    name = tech.get("name", "").strip().replace("/", "-")

    # Create a unique, human-readable key for the technique_dict.
    full_key = f"{tid} - {name}"

    # Extract the description and detection details, removing leading/trailing whitespace.
    # Use .get() with a default empty string to prevent errors if keys are missing.
    description = tech.get("description", "").strip()
    # 'x_mitre_detection' is a custom STIX property used by MITRE for detection guidance.
    detection = tech.get("x_mitre_detection", "").strip()

    tactic_names = []
    # Iterate through the kill chain phases associated with the technique/sub-technique.
    # Default to an empty list if 'kill_chain_phases' is missing.
    for phase in tech.get("kill_chain_phases", []):
        # Ensure we are looking at phases belonging to the 'mitre-attack' framework
        # (as opposed to potential other frameworks like 'mitre-pre-attack').
        if phase.get("kill_chain_name") == "mitre-attack":
            # Extract the tactic's phase name (e.g., "initial-access").
            # Convert it to lowercase and replace hyphens with spaces (e.g., "initial access")
            # to match the keys created for the 'tactic_name_to_id' dictionary.
            phase_name_lookup_key = phase.get("phase_name", "").lower().replace("-", " ")

            # Look up the Tactic ID (e.g., "TA0001") using the prepared lookup key.
            tactic_id = tactic_name_to_id.get(phase_name_lookup_key)

            # If a corresponding tactic ID was found...
            if tactic_id:
                # Format the tactic information string (e.g., "TA0001 - Initial Access").
                # Use .title() for consistent capitalization of the tactic name.
                tactic_display_name = phase.get("phase_name", "").replace("-", " ").title()
                tactic_names.append(f"{tactic_id} - {tactic_display_name}")

    # Join the collected tactic strings into a single comma-separated string.
    # Use set() to remove potential duplicates and sorted() for consistent ordering.
    tactic_str = ", ".join(sorted(set(tactic_names)))

    # Store the extracted and formatted information in the main dictionary.
    technique_dict[full_key] = {
        "tactic": tactic_str,
        "description": description,
        "detection": detection,
    }


# In[ ]:


# Debugging block
# 'technique_dict' now contains the processed data, ready for further use
# (e.g., writing to a file, displaying in a UI, further analysis).
# print(json.dumps(technique_dict["T1133 - External Remote Services"], indent=4))


# ## Build Prompts to Generate the Initial Analytic Plans
# 
# This section generates a prompt to build the initial analytic plan for each technique in the MITRE ATT&CK matrix. It first defines a base prompt that provides context to the model, and then generates a specific prompt crafted for each individual technique. The complete prompts are stored in `prompt_library`, where the key is the technique ID and name in the form "T1190 - Exploit Public-Facing Application".

# In[ ]:


# This prompt generates the initial analytic plan.
base_prompt = """\
Generate an Analytic Scheme of Maneuver (ASOM) based on the following definitions of its components. For each component, provide specific examples relevant to a scenario where we suspect a sophisticated external threat is attempting to gain unauthorized access to our organization's sensitive data or negatively impact the network.

ASOM Component Definitions:

1.  Information Requirement (IR): These identify the information about the enemy or the terrain that the commander considers most important. For example, "Has the adversary gained initial access to the network?" These should be tagged with MITRE ATT&CK tactic numbers; for example, (TA0001 - Initial Access). A complete PIR looks like this: "Has the adversary gained initial access to the network? (TA0001 - Initial Access)".

2.  Indicators: These are positive or negative evidence of threat activity pertaining to one or more information requirements. They are observable clues related to a specific information requirement. For the IR "Has the adversary gained initial access to the network? (TA0001 - Initial Access)", the indicator should be a technique within that tactic; for example, "T1190 - Exploit Public Facing Application".

3.  Evidence: This is the concrete information that supports or refutes an indicator. It provides the "proof" and can vary in complexity. For the IR "Has the adversary gained initial access to the network?" and the indicator "T1190 - Exploit Public Facing Application" beneath it, appropriate evidence could be:
    * "Anomalous login attempts from unusual geographic locations."
    * "Network traffic involving known malicious command and control (C2) infrastructure."

4.  Data: This describes the precise data necessary to identify evidence. Specificity here is key (e.g., Zeek Conn logs, Sysmon event ID 4624, Active Directory security logs). For the evidence, focus your ASOM on the following data sources: network logs, specifically Zeek logs; host logs, specifically Windows Event IDs. Write only the data name. For example, Windows Event ID 4688; Zeek conn.log

5. Data Platform: This describes the type of system from which the data can be collected. It should be one of the following options: "Endpoints", "Servers", "Network devices".

6.  Named Areas of Interest (NAIs): These are areas where data that will satisfy a specific information requirement can be collected. For all NAIs, use a dummy value of "Insert site-specific NAI here".

7.  Actions: These are high-level instructions that guide the analysts' search for evidence. Focus mostly on simple detections, but also look for opportunities to incorporate basic statistical methods data science techniques here, such as descriptive statistics, Inter‑quartile range & box‑plots, entropy measures, correlation analysis, linear regression, time series analysis, and other, similar methods. For the evidence above, appropriate actions could be:
    * "Identify all source IP addresses for failed and successful login events. Geolocate logain sources, then identify rare source countries by low frequency of occurence or percentiles to flag anomalous login events from unusual geographic locations."
    * "Inner join a list of source and destination IP addresses from public-facing devices with a list of known malicious command and control (C2) servers from a cyber threat intelligence provider updated within at least 30 days. Investigate all sessions where connections involved C2 servers."

Based on these definitions, please generate a detailed ASOM in the JSON format. The keys in the JSON object should correspond to the following ASOM components in this order: IR, Indicators, Evidence, Data Platform, Data, NAIs, Actions. Note that the key for IR should be replaced by the actual IR based on the description above. Also note that the key for evidence should be replaced with the actual form of evidence based on the description above. Here is an example template:

{
    "(Insert IR here)": {
        "Indicators": {
            "(Insert form of evidence here)": {
                "Data": "",
                "Data Platform": "",
                "NAI": "",
                "Action": ""
            },
            "(Insert form of evidence here)": {
                "Data": "",
                "Data Platform": "",
                "NAI": "",
                "Action": ""
            }
        }
    }
}

For each MITRE technique below, generate one PIR (a general question tagged with the parent tactic's T-code, in the format "Has the adversary gained initial access to the network? (TA0001 - Initial Access)") for each tactic. For example, if there is one parent tactic, generate one PIR; if there are two parent tactics, generate two PIRs. For each PIR, generate two indicators (the MITRE technique provided). For each indicator, generate one form of evidence. Each form of evidence should have Data, NAI, and Actions. I have also provided a helpful description of the technique labeled "Description:", and a potential idea for detecting it, labeled "Detection:"."""


# In[ ]:


prompt_library = {}

for technique in technique_dict:
    prompt_library[technique] = base_prompt + f"\n\nTechnique: {technique}\n\nTactic(s): {technique_dict[technique]['tactic']}\n\nDescription: {technique_dict[technique]['description']}\n\nDetection: {technique_dict[technique]['detection']}"


# In[ ]:


# print(prompt_library["T1190 - Exploit Public-Facing Application"])


# ## Generate the Initial Analytic Plans
# 
# This section generates one PIR and two indicators (with evidence, data, data platform, NAI, and action) for each technique in the MITRE ATT&CK matrix for Enterprise. Note that if a technique may be used at multiple points in an operation, as part of more than one tactic, a unique PIR is generated for each instance of that technique.
# 
# First, define a function with embeded context for generating the analytic plans. This function is unique to the initial generation of analytic plans.

# In[ ]:


def generate_initial_plan(prompt, target_model):
    """Generates an analytic plan in JSON format using a generative AI model.

    This function constructs a detailed, multi-turn prompt including definitions, examples,
    and constraints to guide the target generative model in creating an analytic plan tailored
    to a specific cyber threat scenario and MITRE ATT&CK technique. It leverages
    few-shot prompting by providing a complete example (for T1133) within the prompt context.

    Args:
        prompt (str): A string containing the specific details of the MITRE ATT&CK
                      technique for which the ASOM should be generated. This typically
                      includes Technique ID, Name, Tactics, Description, and Detection guidance.
        target_model (str): The name or identifier of the generative AI model to use
                            (e.g., "models/gemini-1.5-flash").

    Returns:
        str: A string containing the generated ASOM in JSON format. Returns the raw text
             response from the model. Error handling for API calls or invalid JSON
             responses is not explicitly included here.
    """
    # Initialize the Generative AI client using the API key stored in environment variables.
    # Ensure the 'GEMINI_API_KEY' environment variable is set before running.
    client = genai.Client(
        api_key=os.environ.get("GEMINI_API_KEY"),
    )

    # Set the target model for the generation request.
    model = target_model

    # Construct the conversation history ('contents') for the generative model.
    # This uses a multi-turn approach to provide detailed instructions and examples (few-shot learning).
    contents = [
        # Turn 1: User provides detailed instructions, definitions, constraints, and a full example.
        types.Content(
            role="user",
            parts=[
                # This large text block defines the ASOM structure, components (IR, Indicators, etc.),
                # the desired JSON output format, data source constraints (Zeek, Win Events),
                # NAI placeholder requirements, action guidance (including data science methods),
                # and crucially, provides a complete example scenario based on T1133.
                # --- START OF DETAILED PROMPT TEXT (DO NOT MODIFY CONTENT) ---
                types.Part.from_text(text="""Generate an Analytic Scheme of Maneuver (ASOM) based on the following definitions of its components. For each component, provide specific examples relevant to a scenario where we suspect a sophisticated external threat is attempting to gain unauthorized access to our organization's sensitive data or negatively impact the network.

ASOM Component Definitions:

1.  Information Requirement (IR): These identify the information about the enemy or the terrain that the commander considers most important. For example, \"Has the adversary gained initial access to the network?\" These should be tagged with MITRE ATT&CK tactic numbers; for example, (TA0001 - Initial Access). A complete PIR looks like this: \"Has the adversary gained initial access to the network? (TA0001 - Initial Access)\".

2.  Indicators: These are positive or negative evidence of threat activity pertaining to one or more information requirements. They are observable clues related to a specific information requirement. For the IR \"Has the adversary gained initial access to the network? (TA0001 - Initial Access)\", the indicator should be a technique within that tactic; for example, \"T1190 - Exploit Public Facing Application\".

3.  Evidence: This is the concrete information that supports or refutes an indicator. It provides the \"proof\" and can vary in complexity. For the IR \"Has the adversary gained initial access to the network?\" and the indicator \"T1190 - Exploit Public Facing Application\" beneath it, appropriate evidence could be:
    * \"Anomalous login attempts from unusual geographic locations.\"
    * \"Network traffic involving known malicious command and control (C2) infrastructure.\"

4.  Data: This describes the precise data necessary to identify evidence. Specificity here is key (e.g., Zeek Conn logs, Sysmon event ID 4624, Active Directory security logs). For the evidence, focus your ASOM on the following data sources: network logs, specifically Zeek logs; host logs, specifically Windows Event IDs. Write only the data name. For example, Windows Event ID 4688; Zeek conn.log

5. Data Platform: This describes the type of system from which the data can be collected. It should be one of the following options: \"Endpoints\", \"Servers\", \"Network devices\".

6.  Named Areas of Interest (NAIs): These are areas where data that will satisfy a specific information requirement can be collected. For all NAIs, use a dummy value of \"Insert site-specific NAI here\".

7.  Actions: These are high-level instructions that guide the analysts' search for evidence. Focus mostly on simple detections, but also look for opportunities to incorporate basic statistical methods data science techniques here, such as descriptive statistics, Inter‑quartile range & box‑plots, entropy measures, correlation analysis, linear regression, time series analysis, and other, similar methods. For the evidence above, appropriate actions could be:
    * \"Identify all source IP addresses for failed and successful login events. Geolocate logain sources, then identify rare source countries by low frequency of occurence or percentiles to flag anomalous login events from unusual geographic locations.\"
    * \"Inner join a list of source and destination IP addresses from public-facing devices with a list of known malicious command and control (C2) servers from a cyber threat intelligence provider updated within at least 30 days. Investigate all sessions where connections involved C2 servers.\"

Based on these definitions, please generate a detailed ASOM in the JSON format. The keys in the JSON object should correspond to the following ASOM components in this order: IR, Indicators, Evidence, Data Platform, Data, NAIs, Actions. Note that the key for IR should be replaced by the actual IR based on the description above. Also note that the key for evidence should be replaced with the actual form of evidence based on the description above. Here is an example template:

{
    \"(Insert IR here)\": {
        \"Indicators\": {
            \"(Insert form of evidence here)\": {
                \"Data\": \"\",
                \"Data Platform\": \"\",
                \"NAI\": \"\",
                \"Action\": \"\"
            },
            \"(Insert form of evidence here)\": {
                \"Data\": \"\",
                \"Data Platform\": \"\",
                \"NAI\": \"\",
                \"Action\": \"\"
            }
        }
    }
}

For each MITRE technique below, generate one PIR (a general question tagged with the parent tactic's T-code, in the format \"Has the adversary gained initial access to the network? (TA0001 - Initial Access)\") for each tactic. For example, if there is one parent tactic, generate one PIR; if there are two parent tactics, generate two PIRs. For each PIR, generate two indicators (the MITRE technique provided). For each indicator, generate one form of evidence. Each form of evidence should have Data, NAI, and Actions. I have also provided a helpful description of the technique labeled \"Description:\", and a potential idea for detecting it, labeled \"Detection:\".

Technique: T1133 - External Remote Services

Tactic: TA0001 - Initial-Access, TA0003 - Persistence

Description: Adversaries may leverage external-facing remote services to initially access and/or persist within a network. Remote services such as VPNs, Citrix, and other access mechanisms allow users to connect to internal enterprise network resources from external locations. There are often remote service gateways that manage connections and credential authentication for these services. Services such as [Windows Remote Management](https://attack.mitre.org/techniques/T1021/006) and [VNC](https://attack.mitre.org/techniques/T1021/005) can also be used externally.(Citation: MacOS VNC software for Remote Desktop)

Access to [Valid Accounts](https://attack.mitre.org/techniques/T1078) to use the service is often a requirement, which could be obtained through credential pharming or by obtaining the credentials from users after compromising the enterprise network.(Citation: Volexity Virtual Private Keylogging) Access to remote services may be used as a redundant or persistent access mechanism during an operation.

Access may also be gained through an exposed service that doesn’t require authentication. In containerized environments, this may include an exposed Docker API, Kubernetes API server, kubelet, or web application such as the Kubernetes dashboard.(Citation: Trend Micro Exposed Docker Server)(Citation: Unit 42 Hildegard Malware)

Detection: Follow best practices for detecting adversary use of [Valid Accounts](https://attack.mitre.org/techniques/T1078) for authenticating to remote services. Collect authentication logs and analyze for unusual access patterns, windows of activity, and access outside of normal business hours.

When authentication is not required to access an exposed remote service, monitor for follow-on activities such as anomalous external use of the exposed API or application."""),
                # --- END OF DETAILED PROMPT TEXT ---
            ],
        ),
        # Turn 2: Model provides its internal plan for generating the example (T1133).
        # This helps guide the model's reasoning process for the actual prompt later.
        types.Content(
            role="model",
            parts=[
                # This text simulates the model outlining its step-by-step plan to generate the ASOM for T1133.
                # --- START OF MODEL PLAN TEXT (DO NOT MODIFY CONTENT) ---
                types.Part.from_text(text="""The user wants an Analytic Scheme of Maneuver (ASOM) in JSON format.
The ASOM should follow a specific structure: IR -> Indicators -> Evidence -> Data, Data Platform, NAI, Action.
The IR key should be the actual Information Requirement string.
The Evidence key should be the actual evidence string.
The scenario involves detecting a sophisticated external threat attempting unauthorized access or network impact.
I need to generate PIRs, Indicators, Evidence, Data, Data Platform, NAI, and Actions based on the provided technique T1133 - External Remote Services.

**Plan:**

1.  **Identify Tactics:** T1133 belongs to TA0001 (Initial Access) and TA0003 (Persistence).
2.  **Formulate PIRs:** Create one PIR for each tactic, using the general question format provided and the tactic T-code.
    * PIR 1 (TA0001): \"Has the adversary gained initial access to the network via external remote services? (TA0001 - Initial Access)\"
    * PIR 2 (TA0003): \"Is the adversary maintaining persistence using external remote services? (TA0003 - Persistence)\"
3.  **Define Indicators:** For each PIR, use the provided technique T1133 as the indicator. Since the user asked for *two* indicators per PIR, but only provided *one* technique (T1133), I will use T1133 twice for each PIR, but focus on different aspects or detection methods derived from the description and detection notes.
    * PIR 1 -> Indicator 1: T1133 - External Remote Services (focus on login anomalies)
    * PIR 1 -> Indicator 2: T1133 - External Remote Services (focus on exposed services)
    * PIR 2 -> Indicator 1: T1133 - External Remote Services (focus on persistent unusual access)
    * PIR 2 -> Indicator 2: T1133 - External Remote Services (focus on redundant access mechanisms)
4.  **Develop Evidence:** For each indicator, create a specific form of evidence based on the detection notes.
    * PIR1/Indicator1 (T1133 - Login Anomalies): \"Anomalous login attempts to external remote services (VPN, Citrix, RDP) from unusual geographic locations or outside normal business hours.\"
    * PIR1/Indicator2 (T1133 - Exposed Services): \"Anomalous external usage patterns targeting potentially exposed remote services (e.g., Docker API, Kubernetes API) that do not require authentication.\"
    * PIR2/Indicator1 (T1133 - Persistent Unusual Access): \"Successful logins to external remote services exhibiting unusual access patterns (e.g., multiple failed logins followed by success, logins at odd hours consistently over time) matching known Valid Accounts.\"
    * PIR2/Indicator2 (T1133 - Redundant Access): \"Connections via external remote services originating from IP addresses associated with other known suspicious activities or previously compromised accounts.\"
5.  **Specify Data Sources:** Identify the specific logs needed for each piece of evidence, focusing on Windows Event IDs and Zeek logs as requested.
    * Evidence 1 (Login Anomalies): Windows Event ID 4624, Windows Event ID 4625, VPN Logs, Citrix Logs, Zeek conn.log
    * Evidence 2 (Exposed Services): Zeek conn.log, Web Server Access Logs (if applicable for K8s dashboard etc.)
    * Evidence 3 (Persistent Unusual Access): Windows Event ID 4624, Windows Event ID 4625, VPN Logs, Citrix Logs
    * Evidence 4 (Redundant Access): Zeek conn.log, VPN Logs, Citrix Logs, Threat Intel Feeds
6.  **Define Data Platform:** Assign the appropriate platform (\"Endpoints\", \"Servers\", \"Network devices\").
    * Evidence 1: Servers (for AD/Event Logs), Network devices (VPN/Citrix/Zeek)
    * Evidence 2: Network devices (Zeek), Servers (Web server logs)
    * Evidence 3: Servers (for AD/Event Logs), Network devices (VPN/Citrix)
    * Evidence 4: Network devices (Zeek/VPN/Citrix), Servers (Threat Intel platform)
7.  **Set NAI:** Use the placeholder \"Insert site-specific NAI here\".
8.  **Create Actions:** Define actions to find the evidence, incorporating basic analysis and statistical methods where applicable.
    * Evidence 1 Action: \"Collect remote service authentication logs (Windows Event ID 4624, 4625, VPN/Citrix). Identify source IP addresses for successful and failed logins. Geolocate source IPs. Calculate frequency distribution of source countries/regions and login times. Flag logins from low-frequency locations or occurring outside typical business hours (e.g., using IQR or percentile analysis on login times).\"
    * Evidence 2 Action: \"Monitor Zeek conn.log for connections to known ports associated with exposed services (e.g., Docker API 2375/2376, K8s API 6443/443, kubelet 10250). Analyze connection volume, source IPs, and request patterns. Use time series analysis to detect sudden spikes or changes in traffic patterns from external IPs to these services.\"
    * Evidence 3 Action: \"Correlate successful logins (Event ID 4624, VPN/Citrix logs) with preceding failed attempts (Event ID 4625). Analyze login timestamps for patterns inconsistent with normal user behavior (e.g., logins consistently occurring late at night or on weekends). Use descriptive statistics to establish baseline login behavior per user/group and identify deviations.\"
    * Evidence 4 Action: \"Cross-reference source IP addresses from remote service logs (Zeek conn.log, VPN/Citrix logs) against a list of IP addresses identified in other security alerts or known compromised host lists. Inner join connection logs with Threat Intelligence feeds for known malicious IPs connecting via remote services.\"
9.  **Assemble JSON:** Construct the final JSON object following the specified structure and order. Use the PIRs as the top-level keys. Use the evidence strings as the keys under \"Indicators\"."""),
                # --- END OF MODEL PLAN TEXT ---
                # This provides the expected JSON output for the T1133 example, acting as a strong few-shot example.
                # --- START OF MODEL EXAMPLE JSON OUTPUT (DO NOT MODIFY CONTENT) ---
                types.Part.from_text(text="""[
  {
    \"Has the adversary gained initial access to the network via external remote services? (TA0001 - Initial Access)\": {
      \"Indicators\": {
        \"T1133 - External Remote Services\": {
          \"Anomalous login attempts to external remote services (VPN, Citrix, RDP) from unusual geographic locations or outside normal business hours.\": {
            \"Data\": \"Windows Event ID 4624, Windows Event ID 4625, Zeek conn.log\",
            \"Data Platform\": \"Servers, Network devices\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Collect remote service authentication logs (Windows Event ID 4624, 4625, VPN/Citrix). Identify source IP addresses for successful and failed logins. Geolocate source IPs. Calculate frequency distribution of source countries/regions and login times. Flag logins from low-frequency locations or occurring outside typical business hours (e.g., using IQR or percentile analysis on login times).\"
          },
          \"Anomalous external usage patterns targeting potentially exposed remote services (e.g., Docker API, Kubernetes API) that do not require authentication.\": {
            \"Data\": \"Zeek conn.log\",
            \"Data Platform\": \"Network devices\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Monitor Zeek conn.log for connections to known ports associated with exposed services (e.g., Docker API 2375/2376, K8s API 6443/443, kubelet 10250). Analyze connection volume, source IPs, and request patterns. Use time series analysis to detect sudden spikes or changes in traffic patterns from external IPs to these services.\"
          }
        }
      }
    }
  },
  {
    \"Is the adversary maintaining persistence using external remote services? (TA0003 - Persistence)\": {
      \"Indicators\": {
        \"T1133 - External Remote Services\": {
          \"Successful logins to external remote services exhibiting unusual access patterns (e.g., multiple failed logins followed by success, logins at odd hours consistently over time) matching known Valid Accounts.\": {
            \"Data\": \"Windows Event ID 4624, Windows Event ID 4625\",
            \"Data Platform\": \"Servers\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Correlate successful logins (Event ID 4624, VPN/Citrix logs) with preceding failed attempts (Event ID 4625). Analyze login timestamps for patterns inconsistent with normal user behavior (e.g., logins consistently occurring late at night or on weekends). Use descriptive statistics to establish baseline login behavior per user/group and identify deviations.\"
          },
          \"Connections via external remote services originating from IP addresses associated with other known suspicious activities or previously compromised accounts.\": {
            \"Data\": \"Zeek conn.log\",
            \"Data Platform\": \"Network devices\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Cross-reference source IP addresses from remote service logs (Zeek conn.log, VPN/Citrix logs) against a list of IP addresses identified in other security alerts or known compromised host lists. Inner join connection logs with Threat Intelligence feeds for known malicious IPs connecting via remote services.\"
          }
        }
      }
    }
  }
]"""),
                # --- END OF MODEL EXAMPLE JSON OUTPUT ---
            ],
        ),
        # Turn 3: User provides the actual prompt containing the target technique details.
        types.Content(
            role="user",
            parts=[
                # The 'prompt' variable passed into the function contains the technique details
                # for which the ASOM should be generated this time.
                types.Part.from_text(text=prompt),
            ],
        ),
    ]

    # Configure the generation request.
    generate_content_config = types.GenerateContentConfig(
        # Specify that the model's response should be formatted as JSON.
        response_mime_type="application/json",
        # Provide system instructions, reinforcing the desired JSON structure.
        # This repeats the template definition from the initial user message for emphasis.
        system_instruction=[
            # --- START OF SYSTEM INSTRUCTION JSON TEMPLATE (DO NOT MODIFY CONTENT) ---
            types.Part.from_text(text="""Based on these definitions, please generate a detailed ASOM in the JSON format. The keys in the JSON object should correspond to the following ASOM components in this order: IR, Indicators, Evidence, Data Platform, Data, NAIs, Actions. Note that the key for IR should be replaced by the actual IR based on the description above. Also note that the key for evidence should be replaced with the actual form of evidence based on the description above. Here is an example template:

{
    \"(Insert IR here)\": {
        \"Indicators\": {
            \"(Insert form of evidence here)\": {
                \"Data\": \"\",
                \"Data Platform\": \"\",
                \"NAI\": \"\",
                \"Action\": \"\"
            },
            \"(Insert form of evidence here)\": {
                \"Data\": \"\",
                \"Data Platform\": \"\",
                \"NAI\": \"\",
                \"Action\": \"\"
            }
        }
    }
}"""),
            # --- END OF SYSTEM INSTRUCTION JSON TEMPLATE ---
        ],
    )

    # Send the request to the generative model.
    response = client.models.generate_content(
        model=model,              # The specified target model
        contents=contents,        # The constructed multi-turn conversation history
        config=generate_content_config, # Configuration including response format and system instructions
    )

    # Return the text content of the model's response, which should be the generated ASOM JSON.
    return response.text


# Since we will go through multiple iterations of generating and refining these analytic plans, we'll create a wrapper function `prompt_model` that makes it easy to map a collection of prompts to a specific generator function.

# In[ ]:


def prompt_model(requests_per_minute: int,
                 max_iterations: int,
                 prompts_collection: dict,
                 generation_function,
                 output_directory: str = "./",
                 model_name: str = "gemini-2.5-flash"):
    """
    Processes a collection of prompts using a specified generation function,
    saving outputs to files while adhering to rate limits and iteration caps.

    This function iterates through `prompts_collection`. For each item, it first
    checks if an output file (named based on the item's key) already exists in
    the `output_directory`. If it does, the item is skipped. Otherwise, the
    function applies rate limiting (if enabled) before calling the
    `generation_function`. The returned content is then saved to a JSON file
    (e.g., `item_key.json`). Processing stops if `max_iterations` of successful
    generations is reached or if an error occurs during any generation step.

    Args:
        requests_per_minute: The target maximum number of generation requests
                             allowed per minute. If 0 or negative, rate limiting
                             is effectively disabled (no waiting).
        max_iterations: The maximum number of *successful* generation requests
                        to perform in this run. Once this count is reached,
                        processing stops.
        prompts_collection: A dictionary where keys are unique identifiers (e.g.,
                            strings representing technique names or slugs) and
                            values are the textual prompts. Keys are used to name
                            output files (e.g., `key.json`).
        generation_function: A callable function responsible for performing the
                             actual content generation. It must accept two arguments:
                             1. `prompt_content` (str): The text prompt.
                             2. `target_model` (str): The model name/identifier.
                             It is expected to return the generated content as a string.
                             Example: `def my_generator(prompt: str, target_model: str) -> str:`
        output_directory: The path to the directory where output JSON files will be
                          saved. Defaults to the current working directory ("./").
                          The directory will be created if it does not exist.
        model_name: The name or identifier of the model to be passed to the
                    `generation_function`.
    """
    if requests_per_minute <= 0:
        min_seconds_between_requests = 0 # Disables rate limiting delays
    else:
        min_seconds_between_requests = 60 / requests_per_minute

    last_request_timestamp = 0  # Timestamp of when the last request was initiated
    successful_requests_count = 0 # Counter for successful generation operations

    # Ensure the output directory exists.
    # Consider using os.makedirs(output_directory, exist_ok=True) for a more concise way.
    if not os.path.exists(output_directory):
        os.makedirs(output_directory)
        log(f"Created output directory: {output_directory}")

    log(f"Starting prompt processing. Max iterations: {max_iterations}, Target RPM: {requests_per_minute if requests_per_minute > 0 else 'Unlimited'}.")

    for item_key, prompt_content in prompts_collection.items():
        # Construct the full path for the potential output file.
        # Note: item_key should be filesystem-safe (see docstring).
        output_filename = f"{item_key}.json"
        output_file_path = os.path.join(output_directory, output_filename)

        # Check if the output file already exists to avoid re-processing.
        if os.path.exists(output_file_path):
            log(f"Skipping '{item_key}': Output file {output_file_path} already exists.")
            continue  # Proceed to the next item in prompts_collection

        # Rate Limiting Check
        if min_seconds_between_requests > 0: # Only apply rate limiting if enabled
            current_time = time.time()
            elapsed_time_since_last_request = current_time - last_request_timestamp

            if elapsed_time_since_last_request < min_seconds_between_requests:
                wait_time = min_seconds_between_requests - elapsed_time_since_last_request
                log(f"Rate limit check: Waiting for {wait_time:.2f} seconds...")
                time.sleep(wait_time)

        log(f"Processing item #{successful_requests_count + 1}/{max_iterations}). Generating content for '{item_key}' using model '{model_name}'...")

        # Record the time *before* making the potentially time-consuming API call.
        # This ensures rate limiting is based on the start of requests.
        last_request_timestamp = time.time()
        try:
            # Call the provided generation function.
            response_content = generation_function(prompt_content, target_model=model_name)

            # Increment the successful request counter *only after* the generation function
            # completes without raising an exception.
            successful_requests_count += 1

            # Save the generated content to the file.
            # Assumes response_content is a string. For complex objects, use json.dump.
            with open(output_file_path, "w", encoding="utf-8") as f: # Added encoding for robustness
                f.write(response_content)

            log(f"Successfully generated content for '{item_key}' and saved as {output_file_path}")

        except Exception as e:
            log(f"ERROR: Failed to generate content for '{item_key}'. Details: {e}")
            # Current strategy: stop all further processing on error.
            # Alternatives: `continue` to skip to next item, or implement retries.
            log("Stopping processing due to an error.")
            return e # Exit the loop by returning the error

        # Check if the maximum number of successful requests for this run has been reached.
        if successful_requests_count >= max_iterations:
            log(f"Maximum generation limit of {max_iterations} successful requests reached. Exiting loop.")
            break # Exit the loop

    log(f"Prompt processing finished. Total successful generations in this run: {successful_requests_count}.")
    return True


# In[ ]:


def default_skip_condition(item_key: str, output_file_path: str, prompt_content: str) -> bool:
    """
    Default condition: Skip if the output file already exists.
    """
    if os.path.exists(output_file_path):
        log(f"Skipping '{item_key}' (default condition): Output file {output_file_path} already exists.")
        return True
    return False


# In[ ]:


def prompt_model(requests_per_minute: int,
                 max_iterations: int,
                 prompts_collection: dict,
                 generation_function,
                 output_directory: str = "./",
                 model_name: str = "gemini-2.5-flash", # Updated to a hypothetical future model
                 skip_condition_func: callable = default_skip_condition):
    """
    Processes a collection of prompts using a specified generation function,
    saving outputs to files while adhering to rate limits and iteration caps.

    This function iterates through `prompts_collection`. For each item, it first
    evaluates the `skip_condition_func`. If the condition is met (function
    returns True), the item is skipped. Otherwise, the function applies rate
    limiting (if enabled) before calling the `generation_function`. The
    returned content is then saved to a JSON file (e.g., `item_key.json`).
    Processing stops if `max_iterations` of successful generations is reached
    or if an error occurs during any generation step.

    Args:
        requests_per_minute: The target maximum number of generation requests
                             allowed per minute. If 0 or negative, rate limiting
                             is effectively disabled (no waiting).
        max_iterations: The maximum number of *successful* generation requests
                        to perform in this run. Once this count is reached,
                        processing stops.
        prompts_collection: A dictionary where keys are unique identifiers (e.g.,
                            strings representing technique names or slugs) and
                            values are the textual prompts. Keys are used to name
                            output files (e.g., `key.json`).
        generation_function: A callable function responsible for performing the
                             actual content generation. It must accept two arguments:
                             1. `prompt_content` (str): The text prompt.
                             2. `target_model` (str): The model name/identifier.
                             It is expected to return the generated content as a string.
                             Example: `def my_generator(prompt: str, target_model: str) -> str:`
        output_directory: The path to the directory where output JSON files will be
                          saved. Defaults to the current working directory ("./").
                          The directory will be created if it does not exist.
        model_name: The name or identifier of the model to be passed to the
                    `generation_function`.
        skip_condition_func: A callable function that takes `item_key` (str),
                             `output_file_path` (str), and `prompt_content` (str)
                             as arguments and returns True if the item should be
                             skipped, False otherwise. Defaults to `default_skip_condition`
                             which skips if the output file already exists.
    """
    if requests_per_minute <= 0:
        min_seconds_between_requests = 0  # Disables rate limiting delays
    else:
        min_seconds_between_requests = 60 / requests_per_minute

    last_request_timestamp = 0  # Timestamp of when the last request was initiated
    successful_requests_count = 0  # Counter for successful generation operations

    # Ensure the output directory exists.
    os.makedirs(output_directory, exist_ok=True)
    log(f"Ensured output directory exists: {output_directory}")

    log(f"Starting prompt processing. Max iterations: {max_iterations}, Target RPM: {requests_per_minute if requests_per_minute > 0 else 'Unlimited'}.")

    for item_key, prompt_content in prompts_collection.items():
        output_filename = f"{item_key}.json" # Ensure item_key is filesystem-safe
        output_file_path = os.path.join(output_directory, output_filename)

        # Use the dynamic skip condition
        if skip_condition_func(item_key, output_file_path, prompt_content):
            continue  # Proceed to the next item

        # Rate Limiting Check
        if min_seconds_between_requests > 0:
            current_time = time.time()
            elapsed_time_since_last_request = current_time - last_request_timestamp

            if elapsed_time_since_last_request < min_seconds_between_requests:
                wait_time = min_seconds_between_requests - elapsed_time_since_last_request
                log(f"Rate limit check: Waiting for {wait_time:.2f} seconds...")
                time.sleep(wait_time)

        log(f"Processing item #{successful_requests_count + 1}/{max_iterations if max_iterations > 0 else 'unlimited'}). Generating content for '{item_key}' using model '{model_name}'...")

        last_request_timestamp = time.time()
        try:
            response_content = generation_function(prompt_content, target_model=model_name)
            successful_requests_count += 1

            # Consider if response_content is already a JSON string or needs `json.dumps`
            # If generation_function returns a Python dict/list, you'd use json.dump(response_content, f)
            with open(output_file_path, "w", encoding="utf-8") as f:
                f.write(str(response_content)) # Ensure it's a string; if it's JSON, it's fine

            log(f"Successfully generated content for '{item_key}' and saved as {output_file_path}")

        except Exception as e:
            log(f"ERROR: Failed to generate content for '{item_key}'. Details: {e}")
            log("Stopping processing due to an error.")
            raise e

        if max_iterations > 0 and successful_requests_count >= max_iterations:
            log(f"Maximum generation limit of {max_iterations} successful requests reached. Exiting loop.")
            break

    log(f"Prompt processing finished. Total successful generations in this run: {successful_requests_count}.")
    return True


# Next, generate the analytic plans.

# In[ ]:


prompt_model(
    requests_per_minute = 10,
    max_iterations = 1,
    prompts_collection = prompt_library,
    generation_function = generate_initial_plan,
    output_directory = "./",
    model_name = "gemini-2.5-pro"
)


# In[ ]:


# # Halt execution before the unfinished portion of the notebook is run
# sys.exit(0)


# ## Build Prompts to Improve the Analytic Plans
# 
# This section generates a prompt to improve the initial analytic plan for each technique in the MITRE ATT&CK matrix. It first defines a base prompt that provides context to the model, and then generates a specific prompt crafted to improve each individual technique. The complete prompts are stored in `refine_prompt_library`, where the key is the technique ID and name in the form "T1190 - Exploit Public-Facing Application".

# In[ ]:


base_prompt = """\
I’m going to show you an Analytic Plan as a JSON object with the following components.

1.  Information Requirement (IR): These identify the information about the enemy or the terrain that the commander considers most important. For example, "Has the adversary gained initial access to the network?" These should be tagged with MITRE ATT&CK tactic numbers; for example, (TA0001 - Initial Access). A complete IR looks like this: "Has the adversary gained initial access to the network? (TA0001 - Initial Access)".

2.  Indicators: These are positive or negative evidence of threat activity pertaining to one or more information requirements. They are observable clues related to a specific information requirement. For the IR "Has the adversary gained initial access to the network? (TA0001 - Initial Access)", the indicator should be a technique within that tactic; for example, "T1190 - Exploit Public Facing Application".

3.  Evidence: This is the concrete information that supports or refutes an indicator. It provides the "proof" and can vary in complexity. For the IR "Has the adversary gained initial access to the network?" and the indicator "T1190 - Exploit Public Facing Application" beneath it, appropriate evidence could be:
    * "Anomalous login attempts from unusual geographic locations."
    * "Network traffic involving known malicious command and control (C2) infrastructure."

4.  Data: This describes the precise data necessary to identify evidence. Specificity here is key (e.g., Zeek Conn logs, Sysmon event ID 4624, Active Directory security logs). For the evidence, focus your ASOM on the following data sources: network logs, specifically Zeek logs; host logs, specifically Windows Event IDs. Write only the data name. For example, Windows Event ID 4688; Zeek conn.log

5. Data Platform: This describes the type of system from which the data can be collected. It should be one of the following options: "Endpoints", "Servers", "Network devices".

6.  Named Areas of Interest (NAIs): These are areas where data that will satisfy a specific information requirement can be collected. For all NAIs, use a dummy value of "Insert site-specific NAI here".

7.  Actions: These are high-level instructions that guide the analysts' search for evidence. Focus mostly on simple detections, but also look for opportunities to incorporate basic statistical methods data science techniques here, such as descriptive statistics, Inter‑quartile range & box‑plots, entropy measures, correlation analysis, linear regression, time series analysis, and other, similar methods. For the evidence above, appropriate actions could be:
    * "Identify all source IP addresses for failed and successful login events. Geolocate logain sources, then identify rare source countries by low frequency of occurence or percentiles to flag anomalous login events from unusual geographic locations."
    * "Inner join a list of source and destination IP addresses from public-facing devices with a list of known malicious command and control (C2) servers from a cyber threat intelligence provider updated within at least 30 days. Investigate all sessions where connections involved C2 servers."

The keys in the JSON object correspond to the following ASOM components in this order: IR, Indicators, Evidence, Data Platform, Data, NAIs, Actions. Note that the key for IR should be replaced by the actual IR based on the description above. Also note that the key for evidence should be replaced with the actual form of evidence based on the description above. "version" and "last_updated" help track when these plans have been updated. Change "version" to "{version}", and change "last_updated" to "{last updated}". Here is an example template:

[
    {
        "(Insert IR here)": {
            "Indicators": {
                "(Insert form of evidence here)": {
                    "Data": "",
                    "Data Platform": "",
                    "NAI": "",
                    "Action": ""
                },
                "(Insert form of evidence here)": {
                    "Data": "",
                    "Data Platform": "",
                    "NAI": "",
                    "Action": ""
                }
            },
          "version": "",
          "last_updated": ""
        }
    }
]

Note that there is one IR (a general question tagged with the parent tactic's T-code, in the format "Has the adversary gained initial access to the network? (TA0001 - Initial Access)") for each tactic. Also note that for each IR, there are two indicators (the MITRE technique provided). Also note that for each indicator, there is one form of evidence. Each form of evidence has Data, NAI, and Actions.

For each indicator, I want you to add two more forms of evidence (refer to the description above) to really improve our ability to identify the specific malicious activity referred to by the parent IR and the parent indicator. Note that each new evidence should have Data, NAI, and Actions. Do not make any other changes to the analytic plan.

I have provided a helpful description of the technique labeled "Description:", and a potential idea for detecting it, labeled "Detection:".

Technique: {technique}

Tactic(s): {tactics}

Description: {description}

Detection: {detection}

Here is the current analytic plan for you to modify:

```
{original plan}
```
"""


# In[ ]:


# Store the new version number and last updated date
new_version = "1.1"
last_updated = "2025-05-07" # Using the current date as per guidelines

def process_technique(technique_item):
    """
    Processes a single technique to generate its refined prompt.
    Expects technique_item to be a tuple: (technique_name, technique_data)
    or just technique_name if technique_dict is globally accessible or passed differently.
    """
    technique_name, technique_data = technique_item

    json_filename = f"{technique_name}.json"
    try:
        with open(json_filename, "r") as fd:
            original_plan_content = fd.read()

        # Perform placeholder replacements
        refined_prompt = base_prompt.replace("{version}", new_version) \
                                    .replace("{last updated}", last_updated) \
                                    .replace("{technique}", technique_name) \
                                    .replace("{tactics}", technique_data['tactic']) \
                                    .replace("{description}", technique_data['description']) \
                                    .replace("{detection}", technique_data['detection']) \
                                    .replace("{original plan}", original_plan_content)
        return technique_name, refined_prompt
    except FileNotFoundError:
        log(f"Warning: File {json_filename} not found for technique {technique_name}.")
        sys.exit(1)
    except Exception as e:
        log(f"Error processing technique {technique_name}: {e}")
        sys.exit(1)

# Initialize an empty dictionary to store the refined prompts.
refine_prompt_library = {}

# Get the number of available CPU cores
# None for os.cpu_count() means it will use all available cores
num_cores = os.cpu_count()
log(f"Using {num_cores} cores for processing.")

# Create a pool of worker processes
# We pass technique_dict.items() to have both key and value in the worker function
# If your technique_dict is very large, consider if technique_dict itself needs to be passed
# or if its content can be accessed differently to avoid large data transfers to child processes.
# For this case, passing items should be fine.

# Create a list of items to process. Each item is a tuple (technique_name, technique_data)
# This assumes technique_dict is defined and accessible here.
tasks = list(technique_dict.items())

with multiprocessing.Pool(processes=num_cores) as pool:
    # map_async is non-blocking and returns an AsyncResult object.
    # The results will be a list of tuples: [(technique_name, refined_prompt), ...]
    results = pool.map(process_technique, tasks)

# Populate the refine_prompt_library from the results
for technique_name, refined_prompt in results:
    if refined_prompt is not None: # Only add if processing was successful
        refine_prompt_library[technique_name] = refined_prompt

print("Processing complete.")


# In[ ]:


# print(refine_prompt_library["T1133 - External Remote Services"])


# ## Improve the Analytic Plans
# 
# This section iterates over the existing analytic plans, then uses generative artificial intelligence to improve them.
# 
# First, define a function with embeded context for improving the analytic plans. This function is unique to the improvement of analytic plans.

# In[ ]:


def generate_improved_plan(prompt, target_model):
    """
    Generates an improved analytic plan in JSON format using a generative AI model.

    Generates an improved analytic plan in JSON format. This function leverages a
    target generative AI model (e.g., Gemini) by constructing a sophisticated,
    multi-turn conversational prompt. The core of this function is its advanced
    prompt engineering, designed to guide the AI model in understanding complex
    requirements and producing a structured JSON output.

    The function aims to take an existing ASOM for a given MITRE ATT&CK technique,
    and instruct the AI to "improve" it by adding two new "Evidence" components
    to each "Indicator" within the plan, while also updating version and
    timestamp information.

    Args:
        prompt (str): A string containing the specific details of the MITRE ATT&CK
                      technique for which the plan should be generated. This includes
                      PIR, Indicator, Evidence, Data, and NAI, as well as technique
                      description, and detection guidance from MITRE.
        target_model (str): The name or identifier of the generative AI model to use
                            (e.g., "models/gemini-1.5-flash").

    Returns:
        str: A string containing the generated analytic plan in JSON format. Returns
             the raw text response from the model.

    Generates an improved analytic plan in JSON format. This function leverages a
    """
    # Initialize the Google Generative AI client.
    # It retrieves the API key from the environment variable 'GEMINI_API_KEY'.
    client = genai.Client(
        api_key=os.environ.get("GEMINI_API_KEY"),
    )

    # Set the model to be used for generation from the function argument.
    model = target_model

    # Construct the multi-turn conversation history ('contents') to guide the model.
    # This includes detailed instructions, definitions, a JSON template,
    # and a comprehensive few-shot example (T1133) demonstrating the
    # task and the desired reasoning process.
    contents = [
        # Turn 1: User provides extensive context, definitions, and a full example
        # of the task to be performed on the T1133 technique.
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text="""I’m going to show you an Analytic Plan as a JSON object with the following components.

1.  Information Requirement (IR): These identify the information about the enemy or the terrain that the commander considers most important. For example, \"Has the adversary gained initial access to the network?\" These should be tagged with MITRE ATT&CK tactic numbers; for example, (TA0001 - Initial Access). A complete IR looks like this: \"Has the adversary gained initial access to the network? (TA0001 - Initial Access)\".

2.  Indicators: These are positive or negative evidence of threat activity pertaining to one or more information requirements. They are observable clues related to a specific information requirement. For the IR \"Has the adversary gained initial access to the network? (TA0001 - Initial Access)\", the indicator should be a technique within that tactic; for example, \"T1190 - Exploit Public Facing Application\".

3.  Evidence: This is the concrete information that supports or refutes an indicator. It provides the \"proof\" and can vary in complexity. For the IR \"Has the adversary gained initial access to the network?\" and the indicator \"T1190 - Exploit Public Facing Application\" beneath it, appropriate evidence could be:
    * \"Anomalous login attempts from unusual geographic locations.\"
    * \"Network traffic involving known malicious command and control (C2) infrastructure.\"

4.  Data: This describes the precise data necessary to identify evidence. Specificity here is key (e.g., Zeek Conn logs, Sysmon event ID 4624, Active Directory security logs). For the evidence, focus your ASOM on the following data sources: network logs, specifically Zeek logs; host logs, specifically Windows Event IDs. Write only the data name. For example, Windows Event ID 4688; Zeek conn.log

5. Data Platform: This describes the type of system from which the data can be collected. It should be one of the following options: \"Endpoints\", \"Servers\", \"Network devices\".

6.  Named Areas of Interest (NAIs): These are areas where data that will satisfy a specific information requirement can be collected. For all NAIs, use a dummy value of \"Insert site-specific NAI here\".

7.  Actions: These are high-level instructions that guide the analysts' search for evidence. Focus mostly on simple detections, but also look for opportunities to incorporate basic statistical methods data science techniques here, such as descriptive statistics, Inter‑quartile range & box‑plots, entropy measures, correlation analysis, linear regression, time series analysis, and other, similar methods. For the evidence above, appropriate actions could be:
    * \"Identify all source IP addresses for failed and successful login events. Geolocate logain sources, then identify rare source countries by low frequency of occurence or percentiles to flag anomalous login events from unusual geographic locations.\"
    * \"Inner join a list of source and destination IP addresses from public-facing devices with a list of known malicious command and control (C2) servers from a cyber threat intelligence provider updated within at least 30 days. Investigate all sessions where connections involved C2 servers.\"

The keys in the JSON object correspond to the following ASOM components in this order: IR, Indicators, Evidence, Data Platform, Data, NAIs, Actions. Note that the key for IR should be replaced by the actual IR based on the description above. Also note that the key for evidence should be replaced with the actual form of evidence based on the description above. \"version\" and \"last_updated\" help track when these plans have been updated. Change \"version\" to \"1.1\", and change \"last_updated\" to \"2025-05-06\". Here is an example template:

[
    {
        \"(Insert IR here)\": {
            \"Indicators\": {
                \"(Insert form of evidence here)\": {
                    \"Data\": \"\",
                    \"Data Platform\": \"\",
                    \"NAI\": \"\",
                    \"Action\": \"\"
                },
                \"(Insert form of evidence here)\": {
                    \"Data\": \"\",
                    \"Data Platform\": \"\",
                    \"NAI\": \"\",
                    \"Action\": \"\"
                }
            },
          \"version\": \"\",
          \"last_updated\": \"\"
        }
    }
]

Note that there is one IR (a general question tagged with the parent tactic's T-code, in the format \"Has the adversary gained initial access to the network? (TA0001 - Initial Access)\") for each tactic. Also note that for each IR, there are two indicators (the MITRE technique provided). Also note that for each indicator, there is one form of evidence. Each form of evidence has Data, NAI, and Actions.

For each indicator, I want you to add two more forms of evidence (refer to the description above) to really improve our ability to identify the specific malicious activity referred to by the parent IR and the parent indicator. Note that each new evidence should have Data, NAI, and Actions. Do not make any other changes to the analytic plan.

I have provided a helpful description of the technique labeled \"Description:\", and a potential idea for detecting it, labeled \"Detection:\".

Technique: T1133 - External Remote Services

Tactic: TA0001 - Initial-Access, TA0003 - Persistence

Description: Adversaries may leverage external-facing remote services to initially access and/or persist within a network. Remote services such as VPNs, Citrix, and other access mechanisms allow users to connect to internal enterprise network resources from external locations. There are often remote service gateways that manage connections and credential authentication for these services. Services such as [Windows Remote Management](https://attack.mitre.org/techniques/T1021/006) and [VNC](https://attack.mitre.org/techniques/T1021/005) can also be used externally.(Citation: MacOS VNC software for Remote Desktop)

Access to [Valid Accounts](https://attack.mitre.org/techniques/T1078) to use the service is often a requirement, which could be obtained through credential pharming or by obtaining the credentials from users after compromising the enterprise network.(Citation: Volexity Virtual Private Keylogging) Access to remote services may be used as a redundant or persistent access mechanism during an operation.

Access may also be gained through an exposed service that doesn’t require authentication. In containerized environments, this may include an exposed Docker API, Kubernetes API server, kubelet, or web application such as the Kubernetes dashboard.(Citation: Trend Micro Exposed Docker Server)(Citation: Unit 42 Hildegard Malware)

Detection: Follow best practices for detecting adversary use of [Valid Accounts](https://attack.mitre.org/techniques/T1078) for authenticating to remote services. Collect authentication logs and analyze for unusual access patterns, windows of activity, and access outside of normal business hours.

When authentication is not required to access an exposed remote service, monitor for follow-on activities such as anomalous external use of the exposed API or application.

Here is the current ASOM for you to modify:

```
[
  {
    \"Has the adversary gained initial access to the network via external remote services? (TA0001 - Initial Access)\": {
      \"Indicators\": {
        \"T1133 - External Remote Services\": {
          \"Anomalous login attempts to external remote services (VPN, Citrix, RDP) from unusual geographic locations or outside normal business hours.\": {
            \"Data\": \"Windows Event ID 4624, Windows Event ID 4625, Zeek conn.log\",
            \"Data Platform\": \"Servers, Network devices\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Collect remote service authentication logs (Windows Event ID 4624, 4625, VPN/Citrix). Identify source IP addresses for successful and failed logins. Geolocate source IPs. Calculate frequency distribution of source countries/regions and login times. Flag logins from low-frequency locations or occurring outside typical business hours (e.g., using IQR or percentile analysis on login times).\"
          },
          \"Anomalous external usage patterns targeting potentially exposed remote services (e.g., Docker API, Kubernetes API) that do not require authentication.\": {
            \"Data\": \"Zeek conn.log\",
            \"Data Platform\": \"Network devices\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Monitor Zeek conn.log for connections to known ports associated with exposed services (e.g., Docker API 2375/2376, K8s API 6443/443, kubelet 10250). Analyze connection volume, source IPs, and request patterns. Use time series analysis to detect sudden spikes or changes in traffic patterns from external IPs to these services.\"
          }
        }
      },
      \"version\": \"1.0\",
      \"last_updated\": \"2025-05-04\"
    }
  },
  {
    \"Is the adversary maintaining persistence using external remote services? (TA0003 - Persistence)\": {
      \"Indicators\": {
        \"T1133 - External Remote Services\": {
          \"Successful logins to external remote services exhibiting unusual access patterns (e.g., multiple failed logins followed by success, logins at odd hours consistently over time) matching known Valid Accounts.\": {
            \"Data\": \"Windows Event ID 4624, Windows Event ID 4625\",
            \"Data Platform\": \"Servers\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Correlate successful logins (Event ID 4624, VPN/Citrix logs) with preceding failed attempts (Event ID 4625). Analyze login timestamps for patterns inconsistent with normal user behavior (e.g., logins consistently occurring late at night or on weekends). Use descriptive statistics to establish baseline login behavior per user/group and identify deviations.\"
          },
          \"Connections via external remote services originating from IP addresses associated with other known suspicious activities or previously compromised accounts.\": {
            \"Data\": \"Zeek conn.log\",
            \"Data Platform\": \"Network devices\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Cross-reference source IP addresses from remote service logs (Zeek conn.log, VPN/Citrix logs) against a list of IP addresses identified in other security alerts or known compromised host lists. Inner join connection logs with Threat Intelligence feeds for known malicious IPs connecting via remote services.\"
          }
        }
      },
      \"version\": \"1.0\",
      \"last_updated\": \"2025-05-04\"
    }
  }
]
```"""), # This is the very long initial instruction string.
            ],
        ),
        # Turn 2: An example of how the model should "think" and respond,
        # showing the modification process for the T1133 example and outputting
        # the improved T1133 ASOM.
        types.Content(
            role="model",
            parts=[
                types.Part.from_text(text="""The user wants to update an existing JSON object representing an Analytic Plan.
I need to:
1.  Parse the provided JSON.
2.  For each IR, and for each Indicator (\"T1133 - External Remote Services\"), I need to add two new \"Evidence\" entries.
3.  Each new \"Evidence\" entry must have \"Data\", \"Data Platform\", \"NAI\", and \"Action\" fields.
4.  The \"Data\" should focus on Zeek logs and Windows Event IDs.
5.  The \"Data Platform\" should be one of \"Endpoints\", \"Servers\", \"Network devices\".
6.  \"NAI\" should be \"Insert site-specific NAI here\".
7.  \"Actions\" should guide analysts, potentially using basic statistical methods.
8.  Update \"version\" to \"1.1\" and \"last_updated\" to \"2025-05-06\" for both top-level IR objects.

Let's break down the task for each IR:

**IR 1: \"Has the adversary gained initial access to the network via external remote services? (TA0001 - Initial Access)\"**
Indicator: \"T1133 - External Remote Services\"
Existing Evidence 1: \"Anomalous login attempts to external remote services (VPN, Citrix, RDP) from unusual geographic locations or outside normal business hours.\"
Existing Evidence 2: \"Anomalous external usage patterns targeting potentially exposed remote services (e.g., Docker API, Kubernetes API) that do not require authentication.\"

*   **New Evidence 1.1 (for TA0001 - Initial Access, T1133):** Based on \"Detecting Valid Accounts\" and \"unusual access patterns\".
    *   Evidence Idea: \"Unusual number of failed login attempts followed by a successful login to an external remote service from a single source IP.\" (This points to brute-forcing or credential stuffing attempts)
    *   Data: \"Windows Event ID 4625 (failed login), Windows Event ID 4624 (successful login)\"
    *   Data Platform: \"Servers\"
    *   NAI: \"Insert site-specific NAI here\"
    *   Action: \"Aggregate Windows Event IDs 4625 and 4624 by source IP address and timestamp. For each source IP, count failed login attempts (4625) within a short time window (e.g., 5 minutes) immediately preceding a successful login (4624) to an external service. Flag IPs with a high ratio of failed to successful logins or exceeding a predefined threshold of failures before success (e.g., >5 failures).\"

*   **New Evidence 1.2 (for TA0001 - Initial Access, T1133):** Based on \"exposed service that doesn’t require authentication\" and \"monitor for follow-on activities\".
    *   Evidence Idea: \"Anomalous command execution or data transfer patterns immediately following connection to an exposed, non-authenticated remote service (e.g., Kubelet API, Docker API).\"
    *   Data: \"Zeek conn.log, Windows Event ID 4688 (Process Creation), Windows Event ID 5156 (Network Connection)\"
    *   Data Platform: \"Servers, Network devices\" (Host logs from server, network logs from network devices)
    *   NAI: \"Insert site-specific NAI here\"
    *   Action: \"Correlate Zeek conn.log entries showing connections to exposed services (e.g., Docker API ports 2375/2376, Kubelet port 10250) with subsequent process creation events (Windows Event ID 4688) or network connection events (Windows Event ID 5156) on the target server originating from the established external connection. Analyze command line arguments in 4688 for suspicious commands (e.g., `docker run`, `kubectl exec`). Use descriptive statistics to identify unusual data transfer volumes in Zeek conn.log associated with these sessions.\"

**IR 2: \"Is the adversary maintaining persistence using external remote services? (TA0003 - Persistence)\"**
Indicator: \"T1133 - External Remote Services\"
Existing Evidence 1: \"Successful logins to external remote services exhibiting unusual access patterns (e.g., multiple failed logins followed by success, logins at odd hours consistently over time) matching known Valid Accounts.\"
Existing Evidence 2: \"Connections via external remote services originating from IP addresses associated with other known suspicious activities or previously compromised accounts.\"

*   **New Evidence 2.1 (for TA0003 - Persistence, T1133):** Based on \"redundant or persistent access mechanism\" and \"Valid Accounts\".
    *   Evidence Idea: \"Use of a non-standard or newly registered external remote service application/client by a valid account.\"
    *   Data: \"Windows Event ID 4688 (Process Creation), Zeek conn.log\"
    *   Data Platform: \"Endpoints, Network devices\" (Process creation on endpoint, connection logs from network)
    *   NAI: \"Insert site-specific NAI here\"
    *   Action: \"Monitor process creation logs (Windows Event ID 4688) on endpoints for execution of uncommon VPN clients or remote access tools not part of standard corporate software. Correlate with Zeek conn.log to identify external connections made by these processes. Maintain a baseline of commonly used remote service clients per user/department and flag deviations. Analyze process parent-child relationships to detect if standard remote tools are launched by unusual parent processes.\"

*   **New Evidence 2.2 (for TA0003 - Persistence, T1133):** Based on \"services such as Windows Remote Management and VNC can also be used externally\" and \"Valid Accounts\".
    *   Evidence Idea: \"Persistent external connections to internal systems using services like WinRM or VNC from a limited set of external IPs over an extended period, especially outside business hours.\"
    *   Data: \"Windows Event ID 4624 (Logon Type 3 for RDP/network, or specific WinRM event logs if available), Zeek conn.log (for VNC ports 5900+, WinRM ports 5985/5986)\"
    *   Data Platform: \"Servers, Network devices\"
    *   NAI: \"Insert site-specific NAI here\"
    *   Action: \"Filter Zeek conn.log for traffic to WinRM (TCP 5985, 5986) or VNC (TCP 5900-590X) ports originating from external IPs. Correlate with Windows Event ID 4624 on target servers to confirm successful network logons. Use time series analysis to identify long-duration connections or recurring connections from the same external IPs, especially those active outside of normal business hours. Calculate the entropy of source IPs connecting to these services; a low entropy might indicate a persistent C2 channel.\"

Now, I will integrate these new evidences into the existing JSON structure and update the version and last_updated fields.
The JSON structure is an array of IR objects. Each IR object has a key which is the IR string itself. Inside this, there's an \"Indicators\" object. The key for the indicator is the technique ID. Inside the indicator, there's an object where keys are the \"Evidence\" strings.

Let's make sure the structure for adding evidence is correct. The indicator (e.g., \"T1133 - External Remote Services\") is a key, and its value is an object. This object contains key-value pairs where the key is the evidence string, and the value is another object with \"Data\", \"Data Platform\", \"NAI\", and \"Action\". So, I'll be adding more key-value pairs at this level.

Final check on constraints:
-   \"version\": \"1.1\"
-   \"last_updated\": \"2025-05-06\"
-   Data sources: network logs (Zeek), host logs (Windows Event IDs).
-   NAI: \"Insert site-specific NAI here\".
-   Actions: simple detections, basic statistical methods.

Let's draft the new evidence parts.

For IR1 (\"Has the adversary gained initial access to the network via external remote services? (TA0001 - Initial Access)\"), Indicator \"T1133 - External Remote Services\":
Add:
1.  Evidence: \"Unusual number of failed login attempts followed by a successful login to an external remote service from a single source IP.\"
    Data: \"Windows Event ID 4625, Windows Event ID 4624\"
    Data Platform: \"Servers\"
    NAI: \"Insert site-specific NAI here\"
    Action: \"Aggregate Windows Event IDs 4625 and 4624 by source IP address and timestamp. For each source IP, count failed login attempts (4625) within a short time window (e.g., 5 minutes) immediately preceding a successful login (4624) to an external service. Flag IPs with a high ratio of failed to successful logins or exceeding a predefined threshold of failures before success (e.g., >5 failures).\"

2.  Evidence: \"Anomalous command execution or data transfer patterns immediately following connection to an exposed, non-authenticated remote service (e.g., Kubelet API, Docker API).\"
    Data: \"Zeek conn.log, Windows Event ID 4688, Windows Event ID 5156\"
    Data Platform: \"Servers, Network devices\"
    NAI: \"Insert site-specific NAI here\"
    Action: \"Correlate Zeek conn.log entries showing connections to exposed services (e.g., Docker API ports 2375/2376, Kubelet port 10250) with subsequent process creation events (Windows Event ID 4688) or network connection events (Windows Event ID 5156) on the target server originating from the established external connection. Analyze command line arguments in 4688 for suspicious commands (e.g., `docker run`, `kubectl exec`). Use descriptive statistics to identify unusual data transfer volumes in Zeek conn.log associated with these sessions.\"

For IR2 (\"Is the adversary maintaining persistence using external remote services? (TA0003 - Persistence)\"), Indicator \"T1133 - External Remote Services\":
Add:
1.  Evidence: \"Use of a non-standard or newly registered external remote service application/client by a valid account for persistent access.\"
    Data: \"Windows Event ID 4688, Zeek conn.log\"
    Data Platform: \"Endpoints, Network devices\"
    NAI: \"Insert site-specific NAI here\"
    Action: \"Monitor process creation logs (Windows Event ID 4688) on endpoints for execution of uncommon VPN clients or remote access tools not part of standard corporate software. Correlate with Zeek conn.log to identify external connections made by these processes. Maintain a baseline of commonly used remote service clients per user/department and flag deviations using frequency analysis. Analyze process parent-child relationships to detect if standard remote tools are launched by unusual parent processes.\"

2.  Evidence: \"Persistent external connections to internal systems using services like WinRM or VNC from a consistent set of external IPs over an extended period, especially outside business hours.\"
    Data: \"Windows Event ID 4624, Zeek conn.log\"
    Data Platform: \"Servers, Network devices\"
    NAI: \"Insert site-specific NAI here\"
    Action: \"Filter Zeek conn.log for traffic to WinRM (TCP 5985, 5986) or VNC (TCP 5900-590X) ports originating from external IPs. Correlate with Windows Event ID 4624 (Logon Type 3) on target servers to confirm successful network logons. Use time series analysis to identify long-duration connections or recurring connections from the same external IPs, especially those active outside of normal business hours. Calculate the entropy of source IPs connecting to these services for specific internal hosts; a persistently low entropy for a given internal host might indicate a targeted persistent C2 channel.\"

Looks good. I will now construct the full JSON.
The original JSON is an array with two objects. Each object has the IR as the key. I need to modify the \"Indicators\"->\"T1133 - External Remote Services\" object within each of these two IR objects.
And update version and last_updated at the IR object level.
The original structure for evidence under an indicator is a dictionary where keys are the evidence strings. I just need to add new keys (new evidence strings) and their corresponding value objects.
I will make sure to carefully merge these into the existing structure.
The problem asks to \"add two more forms of evidence\", so for each indicator, there will be the original 2 + 2 new = 4 evidence entries.
Double check: \"Data Platform\" options are \"Endpoints\", \"Servers\", \"Network devices\". My choices are consistent.
\"Data\" should be specific names like \"Windows Event ID 4688; Zeek conn.log\". I used commas in my thought process, but the example uses semicolons. I'll stick to commas as it's more common in lists within a string unless the example format is strict. The user's example \"Windows Event ID 4688; Zeek conn.log\" implies semicolon might be preferred. I'll use comma separated as it's more standard, if the user has a strong preference for semicolon, they can specify. Given the example for data source \"Windows Event ID 4688; Zeek conn.log\", I will use semicolons for multiple data sources in a single string.

Revisiting the data source format: The example provided by the user says \"Write only the data name. For example, Windows Event ID 4688; Zeek conn.log\". I will adhere to this, using semicolons if multiple data sources are listed for a single evidence.

My planned data entries are:
1.1: \"Windows Event ID 4625; Windows Event ID 4624\"
1.2: \"Zeek conn.log; Windows Event ID 4688; Windows Event ID 5156\"
2.1: \"Windows Event ID 4688; Zeek conn.log\"
2.2: \"Windows Event ID 4624; Zeek conn.log\"
This looks consistent with the request.

The version and last_updated are properties of the IR object, at the same level as \"Indicators\".
Example:
```json
[
    {
        \"(Insert IR here)\": {  // IR object
            \"Indicators\": { ... },
            \"version\": \"\",      // property of IR object
            \"last_updated\": \"\"  // property of IR object
        }
    }
]
```
My plan to update `version` to \"1.1\" and `last_updated` to \"2025-05-06\" for each of the two IR objects is correct.
The phrasing of the new evidence strings is important. They should be descriptive.
For 2.1, \"Use of a non-standard or newly registered external remote service application/client by a valid account for persistent access.\" - Adding \"for persistent access\" clarifies the link to TA0003.
For 2.2, \"Persistent external connections to internal systems using services like WinRM or VNC from a consistent set of external IPs over an extended period, especially outside business hours.\" - This is also good for persistence.

One final check: The prompt states \"For each indicator, I want you to add two more forms of evidence\". Since there's one indicator (\"T1133 - External Remote Services\") under each IR, this means 2 new evidences for the TA0001 IR's T1133, and 2 new evidences for the TA0003 IR's T1133. This is what I've planned."""), # This is the model's example reasoning and output for T1133.
                 # The actual improved JSON for T1133 would follow here in the full prompt.
                 # The example shows the model producing the correctly modified JSON for T1133.
                types.Part.from_text(text="""[
  {
    \"Has the adversary gained initial access to the network via external remote services? (TA0001 - Initial Access)\": {
      \"Indicators\": {
        \"T1133 - External Remote Services\": {
          \"Anomalous login attempts to external remote services (VPN, Citrix, RDP) from unusual geographic locations or outside normal business hours.\": {
            \"Data\": \"Windows Event ID 4624; Windows Event ID 4625; Zeek conn.log\",
            \"Data Platform\": \"Servers, Network devices\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Collect remote service authentication logs (Windows Event ID 4624, 4625, VPN/Citrix logs, Zeek conn.log for VPN traffic). Identify source IP addresses for successful and failed logins. Geolocate source IPs. Calculate frequency distribution of source countries/regions and login times. Flag logins from low-frequency locations or occurring outside typical business hours (e.g., using IQR or percentile analysis on login times).\"
          },
          \"Anomalous external usage patterns targeting potentially exposed remote services (e.g., Docker API, Kubernetes API) that do not require authentication.\": {
            \"Data\": \"Zeek conn.log\",
            \"Data Platform\": \"Network devices\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Monitor Zeek conn.log for connections to known ports associated with exposed services (e.g., Docker API 2375/2376, K8s API 6443/443, kubelet 10250). Analyze connection volume, source IPs, and request patterns. Use time series analysis to detect sudden spikes or changes in traffic patterns from external IPs to these services.\"
          },
          \"Unusual number of failed login attempts followed by a successful login to an external remote service from a single source IP.\": {
            \"Data\": \"Windows Event ID 4625; Windows Event ID 4624\",
            \"Data Platform\": \"Servers\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Aggregate Windows Event IDs 4625 (failed login) and 4624 (successful login) by source IP address and timestamp for external remote services. For each source IP, count failed login attempts (4625) within a short time window (e.g., 5 minutes) immediately preceding a successful login (4624). Flag IPs with a high ratio of failed to successful logins or exceeding a predefined threshold of failures before success (e.g., >5 failures).\"
          },
          \"Anomalous command execution or data transfer patterns immediately following connection to an exposed, non-authenticated remote service (e.g., Kubelet API, Docker API).\": {
            \"Data\": \"Zeek conn.log; Windows Event ID 4688; Windows Event ID 5156\",
            \"Data Platform\": \"Servers, Network devices\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Correlate Zeek conn.log entries showing connections to exposed services (e.g., Docker API ports 2375/2376, Kubelet port 10250) with subsequent process creation events (Windows Event ID 4688) or network connection events (Windows Event ID 5156) on the target server originating from the established external connection. Analyze command line arguments in 4688 for suspicious commands (e.g., `docker run`, `kubectl exec`). Use descriptive statistics (e.g., sum, mean of bytes transferred) to identify unusual data transfer volumes in Zeek conn.log associated with these sessions compared to baseline.\"
          }
        }
      },
      \"version\": \"1.1\",
      \"last_updated\": \"2025-05-06\"
    }
  },
  {
    \"Is the adversary maintaining persistence using external remote services? (TA0003 - Persistence)\": {
      \"Indicators\": {
        \"T1133 - External Remote Services\": {
          \"Successful logins to external remote services exhibiting unusual access patterns (e.g., multiple failed logins followed by success, logins at odd hours consistently over time) matching known Valid Accounts.\": {
            \"Data\": \"Windows Event ID 4624; Windows Event ID 4625\",
            \"Data Platform\": \"Servers\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Correlate successful logins (Event ID 4624, VPN/Citrix logs) with preceding failed attempts (Event ID 4625) for valid accounts. Analyze login timestamps for patterns inconsistent with normal user behavior (e.g., logins consistently occurring late at night or on weekends). Use descriptive statistics (mean, median, mode of login times) to establish baseline login behavior per user/group and identify deviations using percentile analysis.\"
          },
          \"Connections via external remote services originating from IP addresses associated with other known suspicious activities or previously compromised accounts.\": {
            \"Data\": \"Zeek conn.log\",
            \"Data Platform\": \"Network devices\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Cross-reference source IP addresses from remote service logs (Zeek conn.log, VPN/Citrix logs) against a list of IP addresses identified in other security alerts or known compromised host lists. Inner join connection logs with Threat Intelligence feeds for known malicious IPs connecting via remote services. Investigate all sessions involving such IPs.\"
          },
          \"Use of a non-standard or newly registered external remote service application/client by a valid account for persistent access.\": {
            \"Data\": \"Windows Event ID 4688; Zeek conn.log\",
            \"Data Platform\": \"Endpoints, Network devices\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Monitor process creation logs (Windows Event ID 4688) on endpoints for execution of uncommon VPN clients or remote access tools not part of standard corporate software, especially if associated with valid user accounts. Correlate with Zeek conn.log to identify external connections made by these processes. Maintain a baseline of commonly used remote service clients per user/department and flag deviations using frequency analysis. Analyze process parent-child relationships to detect if standard remote tools are launched by unusual parent processes.\"
          },
          \"Persistent external connections to internal systems using services like WinRM or VNC from a consistent set of external IPs over an extended period, especially outside business hours.\": {
            \"Data\": \"Windows Event ID 4624; Zeek conn.log\",
            \"Data Platform\": \"Servers, Network devices\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Filter Zeek conn.log for traffic to WinRM (TCP 5985, 5986) or VNC (TCP 5900-590X) ports originating from external IPs. Correlate with Windows Event ID 4624 (Logon Type 3 for network logon) on target servers to confirm successful logons. Use time series analysis to identify long-duration connections or recurring connections from the same external IPs, especially those active outside of normal business hours. Calculate the entropy of source IPs connecting to these services for specific internal hosts; a persistently low entropy for a given internal host might indicate a targeted persistent C2 channel.\"
          }
        }
      },
      \"version\": \"1.1\",
      \"last_updated\": \"2025-05-06\"
    }
  }
]"""), # Actual improved JSON for T1133 example
            ],
        ),
        # Turn 3: The actual user prompt for the new technique, passed as an argument
        # to this Python function. This is where the specific technique details
        # and its current ASOM to be improved are provided.
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text=prompt),
            ],
        ),
    ]

    # Configuration for the content generation request.
    # Specifies the desired response MIME type as JSON and includes system instructions.
    generate_content_config = types.GenerateContentConfig(
        response_mime_type="application/json", # Instructs the model to output JSON.
        # The system instruction reiterates the JSON structure, acting as a reinforcement.
        system_instruction=[
            types.Part.from_text(text="""The keys in the JSON object correspond to the following ASOM components in this order: IR, Indicators, Evidence, Data Platform, Data, NAIs, Actions. Note that the key for IR should be replaced by the actual IR based on the description above. Also note that the key for evidence should be replaced with the actual form of evidence based on the description above. \"version\" and \"last_updated\" help track when these plans have been updated. Here is an example template:

[
    {
        \"(Insert IR here)\": {
            \"Indicators\": {
                \"(Insert form of evidence here)\": {
                    \"Data\": \"\",
                    \"Data Platform\": \"\",
                    \"NAI\": \"\",
                    \"Action\": \"\"
                },
                \"(Insert form of evidence here)\": {
                    \"Data\": \"\",
                    \"Data Platform\": \"\",
                    \"NAI\": \"\",
                    \"Action\": \"\"
                }
            },
          \"version\": \"\",
          \"last_updated\": \"\"
        }
    }
]"""),
        ],
    )

    # Send the request to the generative AI model.
    response = client.models.generate_content(
        model=model,                  # The specified target model
        contents=contents,            # The constructed multi-turn conversation history
        config=generate_content_config, # Configuration including response format and system instructions
    )

    # Return the text content of the model's response.
    # This is expected to be the generated ASOM in JSON string format.
    return response.text


# 

# In[ ]:


def skip_by_version(item_key: str, output_file_path: str, prompt_content: str) -> bool:
    """
    Custom skip logic
    """
    with open(output_file_path, 'r', encoding='utf-8') as fd:
        try:
            content = fd.read()
            if ("```json" in content):
                content = content.replace("```json", "").replace("```", "")
            data = json.loads(content.replace(r"\"", r"\'").replace("\\", "\\\\"))
        except Exception as e:
            log(f"Error reading {output_file_path}: {e}")
            sys.exit(1)
        all_versions_are_target = all(
            (
                isinstance(item, dict) and  # Ensure the item itself is a dictionary
                len(item) == 1 and          # Ensure the item_dict has exactly one key (the question)
                # Safely get the inner dictionary (value of the single key) and assign to inner_dict
                # next(iter(item.values())) is safe because len(item) == 1 ensures item is not empty
                isinstance(inner_dict := next(iter(item.values())), dict) and # Ensure inner_dict is a dictionary
                inner_dict.get("version") != "1.0" # Check the version safely
            )
            for item in data
        )
        if (all_versions_are_target):
            log(f"Skipping '{item_key}' (custom logic): Version 1.1 already exists.")
            return True
    return False


# Next, generate the improved analytic plans.

# In[ ]:


prompt_model(
    requests_per_minute = 10,
    max_iterations = 30,
    prompts_collection = refine_prompt_library,
    generation_function = generate_improved_plan,
    output_directory = "./",
    skip_condition_func = skip_by_version,
    model_name =  "gemini-2.5-pro"
)


# ## Build Prompts to Refine the Analytic Plans
# 
# This section generates a prompt to refine the initial analytic plans to meet the following criteria:
# 
# 1. Each information requirement must at least one evidence and actions in each of the following categories:
# 
# * Selectors. Detection rules using atomic indicators or threat-specific artifacts. Fast to deploy but easy to evade.
# * Patterns. Rules targeting known tools, command-line usage, and mid-level artifacts. More resilient than atomic IOCs.
# * Behaviors. Rules focused on TTPs, sequences, or hunting hypotheses. Independent of tool or actor. Hardest to evade.
# 
# 2. Each information requirement must detail an exceptionally thorough, rigorous, and reliable method of identifying its target malicious activity.
# 3. Each JSON object must be properly structures, properly formatted, and syntactically correct.
# 
# The code below first defines a base prompt that provides context to the model, and then generates a specific prompt crafted to refine each individual technique. The complete prompts are stored in `final_prompt_library`, where the key is the technique ID and name in the form "T1190 - Exploit Public-Facing Application".

# In[ ]:


# base_prompt = """\
# I’m going to show you an Analytic Plan as a JSON object with the following components.

# 1. Information Requirement (IR): These identify the information about the enemy or the terrain that the commander considers most important. For example, "Has the adversary gained initial access to the network?" These should be tagged with MITRE ATT&CK tactic numbers; for example, (TA0001 - Initial Access). A complete IR looks like this: "Has the adversary gained initial access to the network? (TA0001 - Initial Access)".

# 2. Indicators: These are positive or negative evidence of threat activity pertaining to one or more information requirements. They are observable clues related to a specific information requirement. For the IR "Has the adversary gained initial access to the network? (TA0001 - Initial Access)", the indicator should be a technique within that tactic; for example, "T1190 - Exploit Public Facing Application".

# 3. Evidence: This is the concrete information that supports or refutes an indicator. It provides the "proof" and can vary in complexity. For the IR "Has the adversary gained initial access to the network?" and the indicator "T1190 - Exploit Public Facing Application" beneath it, appropriate evidence could be:
#     * "Anomalous login attempts from unusual geographic locations."
#     * "Network traffic involving known malicious command and control (C2) infrastructure."

# 4. Data: This describes the precise data necessary to identify evidence. Specificity here is key (e.g., Zeek Conn logs, Sysmon event ID 4624, Active Directory security logs). For the evidence, focus your ASOM on the following data sources: network logs, specifically Zeek logs; host logs, specifically Windows Event IDs. Write only the data name. For example, Windows Event ID 4688; Zeek conn.log

# 5. Data Platform: This describes the type of system from which the data can be collected. It should be one of the following options: "Endpoints", "Servers", "Network devices".

# 6. Named Areas of Interest (NAIs): These are areas where data that will satisfy a specific information requirement can be collected. For all NAIs, use a dummy value of "Insert site-specific NAI here".

# 7. Actions: These are high-level instructions that guide the analysts' search for evidence. Focus mostly on simple detections, but also look for opportunities to incorporate basic statistical methods data science techniques here, such as descriptive statistics, Inter‑quartile range & box‑plots, entropy measures, correlation analysis, linear regression, time series analysis, and other, similar methods. For the evidence above, appropriate actions could be:
#     * "Identify all source IP addresses for failed and successful login events. Geolocate logain sources, then identify rare source countries by low frequency of occurence or percentiles to flag anomalous login events from unusual geographic locations."
#     * "Inner join a list of source and destination IP addresses from public-facing devices with a list of known malicious command and control (C2) servers from a cyber threat intelligence provider updated within at least 30 days. Investigate all sessions where connections involved C2 servers."

# The keys in the JSON object correspond to the following analytic plan components in this order: IR, Indicators, Evidence, Data Platform, Data, NAIs, Actions. Note that the key for IR should be replaced by the actual IR based on the description above. Also note that the key for evidence should be replaced with the actual form of evidence based on the description above. "version" and "last_updated" help track when these plans have been updated. Change "version" to "{version}", and change "last_updated" to "{last updated}". Here is an example template:

# [
#     {
#         "(Insert IR here)": {
#             "Indicator": {
#                 "(Insert form of evidence here)": {
#                     "Data": "",
#                     "Data Platform": "",
#                     "NAI": "",
#                     "Action": ""
#                 },
#                 "(Insert form of evidence here)": {
#                     "Data": "",
#                     "Data Platform": "",
#                     "NAI": "",
#                     "Action": ""
#                 }
#             },
#           "version": "",
#           "last_updated": ""
#         }
#     }
# ]

# Note that there is one IR (a general question tagged with the parent tactic's T-code, in the format "Has the adversary gained initial access to the network? (TA0001 - Initial Access)") for each tactic, so you may see multiple IRs. Also note that for each IR, there is one indicator (the MITRE technique provided). Also note that for each indicator, there may be multiple forms of evidence. Each form of evidence has Data, NAI, and Actions.

# I want you to review these analytic plans very carefully to ensure that each IR meets the following criteria:

# 1. Ensure each IR effectively draws detections, as explained in evidence and actions, from every single one of the following categories. If each IR does not, modify them to cover the full spectrum from selectors to patterns to behaviors:

# - Selectors. Detection rules using atomic indicators or threat-specific artifacts. Fast to deploy but easy to evade.
# - Patterns. Rules targeting known tools, command-line usage, and mid-level artifacts. More resilient than atomic IOCs.
# - Behaviors. Rules focused on TTPs, sequences, or hunting hypotheses. Independent of tool or actor. Hardest to evade.

# Every IR must have at least one form of evidence and an associated action based on selectors. Every IR must have at least one form of evidence and an associated action based on Patterns. Every IR must have at least one form of evidence and an associated action based on Behaviors.

# 2. Ensure each IR, in total, details an exceptionally thorough, rigorous, and reliable method of identifying the particular malicious activity identified with the MITRE IDs. Improve each plan to meet this standard if it does not.
# 3. Ensure the plans are properly structured, properly formatted, and syntactically correct JSON objects in the form of the template I already showed you. Make alterations where necessary.

# I have provided a helpful description of the technique labeled "Description:", and a potential idea for detecting it, labeled "Detection:".

# Technique: {technique}

# Tactic: {tactics}

# Description: {description}

# Detection: {detection}

# Here is the current analytic plan for you to modify:

# ```
# {original plan}
# ```
# """
base_prompt = """\
I’m going to show you an Analytic Plan as a JSON object with the following components.

1. Information Requirement (IR): These identify the information about the enemy or the terrain that the commander considers most important. For example, "Has the adversary gained initial access to the network?" These should be tagged with MITRE ATT&CK tactic numbers; for example, (TA0001 - Initial Access). A complete IR looks like this: "Has the adversary gained initial access to the network? (TA0001 - Initial Access)".

2. Indicators: These are positive or negative evidence of threat activity pertaining to one or more information requirements. They are observable clues related to a specific information requirement. For the IR "Has the adversary gained initial access to the network? (TA0001 - Initial Access)", the indicator should be a technique within that tactic; for example, "T1190 - Exploit Public Facing Application".

3. Evidence: This is the concrete information that supports or refutes an indicator. It provides the "proof" and can vary in complexity. For the IR "Has the adversary gained initial access to the network?" and the indicator "T1190 - Exploit Public Facing Application" beneath it, appropriate evidence could be:
    * "Anomalous login attempts from unusual geographic locations."
    * "Network traffic involving known malicious command and control (C2) infrastructure."

4. Data: This describes the precise data necessary to identify evidence. Specificity here is key (e.g., Zeek Conn logs, Sysmon event ID 4624, Active Directory security logs). For the evidence, focus your ASOM on the following data sources: network logs, specifically Zeek logs; host logs, specifically Windows Event IDs. Write only the data name. For example, Windows Event ID 4688; Zeek conn.log

5. Data Platform: This describes the type of system from which the data can be collected. It should be one of the following options: "Endpoints", "Servers", "Network devices".

6. Named Areas of Interest (NAIs): These are areas where data that will satisfy a specific information requirement can be collected. For all NAIs, use a dummy value of "Insert site-specific NAI here".

7. Actions: These are high-level instructions that guide the analysts' search for evidence. Focus mostly on simple detections, but also look for opportunities to incorporate basic statistical methods data science techniques here, such as descriptive statistics, Inter‑quartile range & box‑plots, entropy measures, correlation analysis, linear regression, time series analysis, and other, similar methods. For the evidence above, appropriate actions could be:
    * "Identify all source IP addresses for failed and successful login events. Geolocate logain sources, then identify rare source countries by low frequency of occurence or percentiles to flag anomalous login events from unusual geographic locations."
    * "Inner join a list of source and destination IP addresses from public-facing devices with a list of known malicious command and control (C2) servers from a cyber threat intelligence provider updated within at least 30 days. Investigate all sessions where connections involved C2 servers."

The keys in the JSON object correspond to the following analytic plan components in this order: IR, Indicators, Evidence, Data Platform, Data, NAIs, Actions. Note that the key for IR should be replaced by the actual IR based on the description above. Also note that the key for evidence should be replaced with the actual form of evidence based on the description above. "version" and "last_updated" help track when these plans have been updated. Change "version" to "{version}", and change "last_updated" to "{last updated}". Here is an example template:

[
    {
        "(Insert IR here)": {
            "Indicator": {
                "(Insert form of evidence here)": {
                    "Data": "",
                    "Data Platform": "",
                    "NAI": "",
                    "Action": ""
                },
                "(Insert form of evidence here)": {
                    "Data": "",
                    "Data Platform": "",
                    "NAI": "",
                    "Action": ""
                }
            },
          "version": "",
          "last_updated": ""
        }
    }
]

Note that there is one IR (a general question tagged with the parent tactic's T-code, in the format "Has the adversary gained initial access to the network? (TA0001 - Initial Access)") for each tactic, so you may see multiple IRs. Also note that for each IR, there is one indicator (the MITRE technique provided). Also note that for each indicator, there may be multiple forms of evidence. Each form of evidence has Data, NAI, and Actions.

I want you to review these analytic plans very carefully to ensure that each IR meets the following criteria:

1. Ensure each IR effectively draws detections, as explained in evidence and actions, from every single one of the following categories. If each IR does not, modify them to cover the full spectrum from selectors to patterns to behaviors:

- Selectors. Detection rules using atomic indicators or threat-specific artifacts. Fast to deploy but easy to evade.
- Patterns. Rules targeting known tools, command-line usage, and mid-level artifacts. More resilient than atomic IOCs.
- Behaviors. Rules focused on TTPs, sequences, or hunting hypotheses. Independent of tool or actor. Hardest to evade.

Every IR must have at least one form of evidence and an associated action based on selectors. Every IR must have at least one form of evidence and an associated action based on Patterns. Every IR must have at least one form of evidence and an associated action based on Behaviors.

2. Ensure each IR, in total, details an exceptionally thorough, rigorous, and reliable method of identifying the particular malicious activity identified with the MITRE IDs. Improve each plan to meet this standard if it does not.
3. Ensure the plans are properly structured, properly formatted, and syntactically correct JSON objects in the form of the template I already showed you. Make alterations where necessary.

Technique: {technique}

Tactic: {tactics}

Here is the current analytic plan for you to modify:

```
{original plan}
```
"""


# In[ ]:


# Store the new version number and last updated date
new_version = "2.0"
last_updated = "2025-05-11"

def process_technique(technique_item):
    """
    Processes a single technique to generate its refined prompt.
    Expects technique_item to be a tuple: (technique_name, technique_data)
    or just technique_name if technique_dict is globally accessible or passed differently.
    """
    technique_name, technique_data = technique_item

    json_filename = f"{technique_name}.json"
    try:
        with open(json_filename, "r") as fd:
            original_plan_content = fd.read()

        # # Perform placeholder replacements
        # refined_prompt = base_prompt.replace("{version}", new_version) \
        #                             .replace("{last updated}", last_updated) \
        #                             .replace("{technique}", technique_name) \
        #                             .replace("{tactics}", technique_data['tactic']) \
        #                             .replace("{description}", technique_data['description']) \
        #                             .replace("{detection}", technique_data['detection']) \
        #                             .replace("{original plan}", original_plan_content)
        # Perform placeholder replacements
        refined_prompt = base_prompt.replace("{version}", new_version) \
                                    .replace("{last updated}", last_updated) \
                                    .replace("{technique}", technique_name) \
                                    .replace("{tactics}", technique_data['tactic']) \
                                    .replace("{original plan}", original_plan_content)
        return technique_name, refined_prompt
    except FileNotFoundError:
        log(f"Warning: File {json_filename} not found for technique {technique_name}.")
        sys.exit(1)
    except Exception as e:
        log(f"Error processing technique {technique_name}: {e}")
        sys.exit(1)

# Initialize an empty dictionary to store the refined prompts.
final_prompt_library = {}

# Get the number of available CPU cores
# None for os.cpu_count() means it will use all available cores
num_cores = os.cpu_count()
log(f"Using {num_cores} cores for processing.")

# Create a pool of worker processes
# We pass technique_dict.items() to have both key and value in the worker function
# If your technique_dict is very large, consider if technique_dict itself needs to be passed
# or if its content can be accessed differently to avoid large data transfers to child processes.
# For this case, passing items should be fine.

# Create a list of items to process. Each item is a tuple (technique_name, technique_data)
# This assumes technique_dict is defined and accessible here.
tasks = list(technique_dict.items())

with multiprocessing.Pool(processes=num_cores) as pool:
    # map_async is non-blocking and returns an AsyncResult object.
    # The results will be a list of tuples: [(technique_name, refined_prompt), ...]
    results = pool.map(process_technique, tasks)

# Populate the refine_prompt_library from the results
for technique_name, refined_prompt in results:
    if refined_prompt is not None: # Only add if processing was successful
        final_prompt_library[technique_name] = refined_prompt

print("Processing complete.")


# In[ ]:


# print(final_prompt_library["T1133 - External Remote Services"])


# ## Refine the Analytic Plans
# 
# 

# In[ ]:


def generate_refined_plan(prompt, target_model):
    """
    Generates a refined analytic plan in JSON format using a generative AI model.

    Generates a refined analytic plan in JSON format. This function leverages a
    target generative AI model (e.g., Gemini) by constructing a sophisticated,
    multi-turn conversational prompt. The core of this function is its advanced
    prompt engineering, designed to guide the AI model in understanding complex
    requirements and producing a structured JSON output.

    The function aims to take an existing analytic plan for a given MITRE ATT&CK
    technique, and instruct the AI to "refine" it by ensuring the forms of
    evidence and associated actions meet a series of criteria designed to
    guarantee their ability to identify malicious activity, while also updating
    version and timestamp information.

    Args:
        prompt (str): A string containing the specific details of the MITRE ATT&CK
                      technique for which the plan should be generated. This includes
                      PIR, Indicator, Evidence, Data, and NAI, as well as technique
                      description, and detection guidance from MITRE.
        target_model (str): The name or identifier of the generative AI model to use
                            (e.g., "models/gemini-1.5-flash").

    Returns:
        str: A string containing the generated analytic plan in JSON format. Returns
             the raw text response from the model.

    Generates an improved analytic plan in JSON format. This function leverages a
    """
    client = genai.Client(
        api_key=os.environ.get("GEMINI_API_KEY"),
    )

    model = target_model
    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text="""I’m going to show you an Analytic Plan as a JSON object with the following components.

1. Information Requirement (IR): These identify the information about the enemy or the terrain that the commander considers most important. For example, \"Has the adversary gained initial access to the network?\" These should be tagged with MITRE ATT&CK tactic numbers; for example, (TA0001 - Initial Access). A complete IR looks like this: \"Has the adversary gained initial access to the network? (TA0001 - Initial Access)\".

2. Indicators: These are positive or negative evidence of threat activity pertaining to one or more information requirements. They are observable clues related to a specific information requirement. For the IR \"Has the adversary gained initial access to the network? (TA0001 - Initial Access)\", the indicator should be a technique within that tactic; for example, \"T1190 - Exploit Public Facing Application\".

3. Evidence: This is the concrete information that supports or refutes an indicator. It provides the \"proof\" and can vary in complexity. For the IR \"Has the adversary gained initial access to the network?\" and the indicator \"T1190 - Exploit Public Facing Application\" beneath it, appropriate evidence could be:
    * \"Anomalous login attempts from unusual geographic locations.\"
    * \"Network traffic involving known malicious command and control (C2) infrastructure.\"

4. Data: This describes the precise data necessary to identify evidence. Specificity here is key (e.g., Zeek Conn logs, Sysmon event ID 4624, Active Directory security logs). For the evidence, focus your ASOM on the following data sources: network logs, specifically Zeek logs; host logs, specifically Windows Event IDs. Write only the data name. For example, Windows Event ID 4688; Zeek conn.log

5. Data Platform: This describes the type of system from which the data can be collected. It should be one of the following options: \"Endpoints\", \"Servers\", \"Network devices\".

6. Named Areas of Interest (NAIs): These are areas where data that will satisfy a specific information requirement can be collected. For all NAIs, use a dummy value of \"Insert site-specific NAI here\".

7. Actions: These are high-level instructions that guide the analysts' search for evidence. Focus mostly on simple detections, but also look for opportunities to incorporate basic statistical methods data science techniques here, such as descriptive statistics, Inter‑quartile range & box‑plots, entropy measures, correlation analysis, linear regression, time series analysis, and other, similar methods. For the evidence above, appropriate actions could be:
    * \"Identify all source IP addresses for failed and successful login events. Geolocate logain sources, then identify rare source countries by low frequency of occurence or percentiles to flag anomalous login events from unusual geographic locations.\"
    * \"Inner join a list of source and destination IP addresses from public-facing devices with a list of known malicious command and control (C2) servers from a cyber threat intelligence provider updated within at least 30 days. Investigate all sessions where connections involved C2 servers.\"

The keys in the JSON object correspond to the following analytic plan components in this order: IR, Indicators, Evidence, Data Platform, Data, NAIs, Actions. Note that the key for IR should be replaced by the actual IR based on the description above. Also note that the key for evidence should be replaced with the actual form of evidence based on the description above. \"version\" and \"last_updated\" help track when these plans have been updated. Change \"version\" to \"2.0\", and change \"last_updated\" to \"2025-05-10\". Here is an example template:

[
    {
        \"(Insert IR here)\": {
            \"Indicator\": {
                \"(Insert form of evidence here)\": {
                    \"Data\": \"\",
                    \"Data Platform\": \"\",
                    \"NAI\": \"\",
                    \"Action\": \"\"
                },
                \"(Insert form of evidence here)\": {
                    \"Data\": \"\",
                    \"Data Platform\": \"\",
                    \"NAI\": \"\",
                    \"Action\": \"\"
                }
            },
          \"version\": \"\",
          \"last_updated\": \"\"
        }
    }
]

Note that there is one IR (a general question tagged with the parent tactic's T-code, in the format \"Has the adversary gained initial access to the network? (TA0001 - Initial Access)\") for each tactic, so you may see multiple IRs. Also note that for each IR, there is one indicator (the MITRE technique provided). Also note that for each indicator, there may be multiple forms of evidence. Each form of evidence has Data, NAI, and Actions.

I want you to review these analytic plans very carefully to ensure that each IR meets the following criteria:

1. Ensure each IR effectively draws detections, as explained in evidence and actions, from every single one of the following categories. If each IR does not, modify them to cover the full spectrum from selectors to patterns to behaviors:

- Selectors. Detection rules using atomic indicators or threat-specific artifacts. Fast to deploy but easy to evade.
- Patterns. Rules targeting known tools, command-line usage, and mid-level artifacts. More resilient than atomic IOCs.
- Behaviors. Rules focused on TTPs, sequences, or hunting hypotheses. Independent of tool or actor. Hardest to evade.

Every IR must have at least one form of evidence and an associated action based on selectors. Every IR must have at least one form of evidence and an associated action based on Patterns. Every IR must have at least one form of evidence and an associated action based on Behaviors.

2. Ensure each IR, in total, details an exceptionally thorough, rigorous, and reliable method of identifying the particular malicious activity identified with the MITRE IDs. Improve each plan to meet this standard if it does not.
3. Ensure the plans are properly structured, properly formatted, and syntactically correct JSON objects in the form of the template I already showed you. Make alterations where necessary.

Output JSON only and no other prepended or appended text.

I have provided a helpful description of the technique labeled \"Description:\", and a potential idea for detecting it, labeled \"Detection:\".

Technique: T1133 - External Remote Services

Tactic: TA0001 - Initial-Access, TA0003 - Persistence

Description: Adversaries may leverage external-facing remote services to initially access and/or persist within a network. Remote services such as VPNs, Citrix, and other access mechanisms allow users to connect to internal enterprise network resources from external locations. There are often remote service gateways that manage connections and credential authentication for these services. Services such as [Windows Remote Management](https://attack.mitre.org/techniques/T1021/006) and [VNC](https://attack.mitre.org/techniques/T1021/005) can also be used externally.(Citation: MacOS VNC software for Remote Desktop)

Access to [Valid Accounts](https://attack.mitre.org/techniques/T1078) to use the service is often a requirement, which could be obtained through credential pharming or by obtaining the credentials from users after compromising the enterprise network.(Citation: Volexity Virtual Private Keylogging) Access to remote services may be used as a redundant or persistent access mechanism during an operation.

Access may also be gained through an exposed service that doesn’t require authentication. In containerized environments, this may include an exposed Docker API, Kubernetes API server, kubelet, or web application such as the Kubernetes dashboard.(Citation: Trend Micro Exposed Docker Server)(Citation: Unit 42 Hildegard Malware)

Detection: Follow best practices for detecting adversary use of [Valid Accounts](https://attack.mitre.org/techniques/T1078) for authenticating to remote services. Collect authentication logs and analyze for unusual access patterns, windows of activity, and access outside of normal business hours.

When authentication is not required to access an exposed remote service, monitor for follow-on activities such as anomalous external use of the exposed API or application.

Here is the current analytic plan for you to modify:

```
[
  {
    \"Has the adversary gained initial access to the network via external remote services? (TA0001 - Initial Access)\": {
      \"Indicators\": {
        \"T1133 - External Remote Services\": {
          \"Anomalous login attempts to external remote services (VPN, Citrix, RDP) from unusual geographic locations or outside normal business hours.\": {
            \"Data\": \"Windows Event ID 4624; Windows Event ID 4625; Zeek conn.log\",
            \"Data Platform\": \"Servers, Network devices\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Collect remote service authentication logs (Windows Event ID 4624, 4625, VPN/Citrix logs, Zeek conn.log for VPN traffic). Identify source IP addresses for successful and failed logins. Geolocate source IPs. Calculate frequency distribution of source countries/regions and login times. Flag logins from low-frequency locations or occurring outside typical business hours (e.g., using IQR or percentile analysis on login times).\"
          },
          \"Anomalous external usage patterns targeting potentially exposed remote services (e.g., Docker API, Kubernetes API) that do not require authentication.\": {
            \"Data\": \"Zeek conn.log\",
            \"Data Platform\": \"Network devices\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Monitor Zeek conn.log for connections to known ports associated with exposed services (e.g., Docker API 2375/2376, K8s API 6443/443, kubelet 10250). Analyze connection volume, source IPs, and request patterns. Use time series analysis to detect sudden spikes or changes in traffic patterns from external IPs to these services.\"
          },
          \"Multiple failed login attempts from a single external IP or geographic location followed by a successful login to an external remote service.\": {
            \"Data\": \"Windows Event ID 4625; Windows Event ID 4624\",
            \"Data Platform\": \"Servers\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Aggregate Windows Event ID 4625 (failed login) and 4624 (successful login) logs for external remote services by source IP address and destination user account within a short time window (e.g., 1-10 minutes). Count failed login attempts from each source IP before a successful login for the same user. Flag source IPs exceeding a threshold of failed attempts before success (e.g., >10 failures within 5 minutes). Use descriptive statistics on attempt counts and time deltas.\"
          },
          \"Discovery or exploitation attempts observed via connection patterns to external-facing services that do not require authentication (e.g., unusual scan activity, malformed requests, unexpected ports).\": {
            \"Data\": \"Zeek conn.log; Zeek http.log\",
            \"Data Platform\": \"Network devices\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Monitor Zeek conn.log for connections to known or unusual ports exposed externally (e.g., Docker API 2375/2376, K8s API 6443/443, kubelet 10250, or other ports with low-frequency connection counts from external IPs). Identify patterns indicative of scanning (e.g., rapid connections across multiple ports). Analyze Zeek http.log for malformed requests or requests targeting known vulnerabilities against exposed web services. Use frequency analysis and correlation to identify potential discovery or exploitation attempts.\"
          }
        }
      },
      \"version\": \"1.1\",
      \"last_updated\": \"2025-05-07\"
    }
  },
  {
    \"Is the adversary maintaining persistence using external remote services? (TA0003 - Persistence)\": {
      \"Indicators\": {
        \"T1133 - External Remote Services\": {
          \"Successful logins to external remote services exhibiting unusual access patterns (e.g., multiple failed logins followed by success, logins at odd hours consistently over time) matching known Valid Accounts.\": {
            \"Data\": \"Windows Event ID 4624; Windows Event ID 4625\",
            \"Data Platform\": \"Servers\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Correlate successful logins (Event ID 4624, VPN/Citrix logs) with preceding failed attempts (Event ID 4625) for valid accounts. Analyze login timestamps for patterns inconsistent with normal user behavior (e.g., logins consistently occurring late at night or on weekends). Use descriptive statistics (mean, median, mode of login times) to establish baseline login behavior per user/group and identify deviations using percentile analysis.\"
          },
          \"Connections via external remote services originating from IP addresses associated with other known suspicious activities or previously compromised accounts.\": {
            \"Data\": \"Zeek conn.log\",
            \"Data Platform\": \"Network devices\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Cross-reference source IP addresses from remote service logs (Zeek conn.log, VPN/Citrix logs) against a list of IP addresses identified in other security alerts or known compromised host lists. Inner join connection logs with Threat Intelligence feeds for known malicious IPs connecting via remote services. Investigate all sessions involving such IPs.\"
          },
          \"Repeated successful logins by a valid account to external remote services from changing or unusual geographic locations or at consistent, off-hour times over an extended period.\": {
            \"Data\": \"Windows Event ID 4624; Zeek conn.log\",
            \"Data Platform\": \"Servers, Network devices\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Analyze successful external remote service logins (Event ID 4624, Zeek conn.log for relevant connections) for valid accounts. Geolocate source IPs. Track source location diversity and login times for specific accounts over weeks or months. Identify accounts logging in from a wide range of locations or consistently outside their normal work hours or at the same unusual off-hour times daily/weekly. Use statistical profiling (descriptive statistics, time series analysis) for individual account access patterns to detect anomalies.\"
          },
          \"Successful external connections to internal systems using typically internal-only remote administration tools (e.g., WinRM, RDP to internal IPs, VNC) via unexpected network paths or without standard VPN usage.\": {
            \"Data\": \"Zeek conn.log; Windows Event ID 4624\",
            \"Data Platform\": \"Servers, Network devices\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Filter Zeek conn.log for external source IPs connecting to internal destination IPs on ports associated with internal remote administration (e.g., WinRM 5985/5986, internal RDP 3389, VNC 5900+). Correlate with Windows Event ID 4624 (Logon Type 3 for network logon) on the destination server for successful logons originating from these external IPs. Investigate connections that bypass expected external access methods (e.g., corporate VPN gateway). Use frequency analysis to identify hosts receiving unexpected external remote administration connections.\"
          }
        }
      },
      \"version\": \"1.1\",
      \"last_updated\": \"2025-05-07\"
    }
  }
]
```"""),
            ],
        ),
        types.Content(
            role="model",
            parts=[
                types.Part.from_text(text="""The user wants me to review and modify an existing analytic plan for MITRE ATT&CK technique T1133 - External Remote Services.
The analytic plan is a JSON object.
I need to ensure the following:

1.  **Correct JSON Structure and Formatting**: The output must be a valid JSON object adhering to the provided template.
    *   `version` should be \"2.0\".
    *   `last_updated` should be \"2025-05-10\".
    *   The key for IR should be the IR string.
    *   The key for \"Indicators\" is \"Indicators\".
    *   The key for the technique (e.g., \"T1133 - External Remote Services\") is the Indicator.
    *   The keys under the technique are the \"Evidence\" strings.
    *   Each Evidence object contains \"Data\", \"Data Platform\", \"NAI\", and \"Action\".

2.  **Thorough, Rigorous, and Reliable Detection**: The plan should provide a comprehensive way to identify the malicious activity (T1133).
    *   I need to review the existing evidence and actions and enhance them if necessary.

3.  **Coverage of Selectors, Patterns, and Behaviors**: Each IR must have at least one piece of evidence and action corresponding to each of these detection categories.
    *   **Selectors**: Atomic indicators (IPs, domains, hashes, specific vulnerability signatures).
    *   **Patterns**: Known tool signatures, command-line usage, mid-level artifacts (e.g., specific sequence of events for a tool, known user-agent strings).
    *   **Behaviors**: TTPs, sequences of actions, hunting hypotheses (e.g., unusual login patterns, data staging followed by exfiltration).

Let's break down the task for each IR:

**IR 1: \"Has the adversary gained initial access to the network via external remote services? (TA0001 - Initial Access)\"**
*   **Indicator**: \"T1133 - External Remote Services\"

    Existing Evidence/Actions:
    1.  **Evidence 1**: \"Anomalous login attempts to external remote services (VPN, Citrix, RDP) from unusual geographic locations or outside normal business hours.\"
        *   **Data**: \"Windows Event ID 4624; Windows Event ID 4625; Zeek conn.log\"
        *   **Data Platform**: \"Servers, Network devices\"
        *   **Action**: \"Collect remote service authentication logs (Windows Event ID 4624, 4625, VPN/Citrix logs, Zeek conn.log for VPN traffic). Identify source IP addresses for successful and failed logins. Geolocate source IPs. Calculate frequency distribution of source countries/regions and login times. Flag logins from low-frequency locations or occurring outside typical business hours (e.g., using IQR or percentile analysis on login times).\"
        *   **Category**: This seems to be **Behavioral** (unusual locations, outside normal hours).

    2.  **Evidence 2**: \"Anomalous external usage patterns targeting potentially exposed remote services (e.g., Docker API, Kubernetes API) that do not require authentication.\"
        *   **Data**: \"Zeek conn.log\"
        *   **Data Platform**: \"Network devices\"
        *   **Action**: \"Monitor Zeek conn.log for connections to known ports associated with exposed services (e.g., Docker API 2375/2376, K8s API 6443/443, kubelet 10250). Analyze connection volume, source IPs, and request patterns. Use time series analysis to detect sudden spikes or changes in traffic patterns from external IPs to these services.\"
        *   **Category**: This is also **Behavioral** (anomalous usage, spikes in traffic). Could have a **Pattern** element if looking for specific known malicious request patterns against these APIs.

    3.  **Evidence 3**: \"Multiple failed login attempts from a single external IP or geographic location followed by a successful login to an external remote service.\"
        *   **Data**: \"Windows Event ID 4625; Windows Event ID 4624\"
        *   **Data Platform**: \"Servers\"
        *   **Action**: \"Aggregate Windows Event ID 4625 (failed login) and 4624 (successful login) logs for external remote services by source IP address and destination user account within a short time window (e.g., 1-10 minutes). Count failed login attempts from each source IP before a successful login for the same user. Flag source IPs exceeding a threshold of failed attempts before success (e.g., >10 failures within 5 minutes). Use descriptive statistics on attempt counts and time deltas.\"
        *   **Category**: This is a strong **Behavioral** indicator (brute-force pattern).

    4.  **Evidence 4**: \"Discovery or exploitation attempts observed via connection patterns to external-facing services that do not require authentication (e.g., unusual scan activity, malformed requests, unexpected ports).\"
        *   **Data**: \"Zeek conn.log; Zeek http.log\"
        *   **Data Platform**: \"Network devices\"
        *   **Action**: \"Monitor Zeek conn.log for connections to known or unusual ports exposed externally (e.g., Docker API 2375/2376, K8s API 6443/443, kubelet 10250, or other ports with low-frequency connection counts from external IPs). Identify patterns indicative of scanning (e.g., rapid connections across multiple ports). Analyze Zeek http.log for malformed requests or requests targeting known vulnerabilities against exposed web services. Use frequency analysis and correlation to identify potential discovery or exploitation attempts.\"
        *   **Category**: This is a mix. \"Requests targeting known vulnerabilities\" is a **Selector** (if specific CVE signatures are used) or **Pattern** (if generic exploit patterns). \"Unusual scan activity\" and \"malformed requests\" are **Behavioral**. \"Unexpected ports\" could be **Pattern** or **Behavioral**.

    **Missing for IR1:**
    *   Clear **Selector-based** evidence/action.
    *   Clear **Pattern-based** evidence/action.

    **Improvements for IR1:**
    *   Add a selector-based detection: e.g., connections from known malicious IPs to remote services.
    *   Add a pattern-based detection: e.g., specific user-agent strings associated with malicious tools attempting to access remote services, or known exploit strings for specific remote service vulnerabilities.
    *   Refine existing ones to clearly map to categories.

**IR 2: \"Is the adversary maintaining persistence using external remote services? (TA0003 - Persistence)\"**
*   **Indicator**: \"T1133 - External Remote Services\"

    Existing Evidence/Actions:
    1.  **Evidence 1**: \"Successful logins to external remote services exhibiting unusual access patterns (e.g., multiple failed logins followed by success, logins at odd hours consistently over time) matching known Valid Accounts.\"
        *   **Data**: \"Windows Event ID 4624; Windows Event ID 4625\"
        *   **Data Platform**: \"Servers\"
        *   **Action**: \"Correlate successful logins (Event ID 4624, VPN/Citrix logs) with preceding failed attempts (Event ID 4625) for valid accounts. Analyze login timestamps for patterns inconsistent with normal user behavior (e.g., logins consistently occurring late at night or on weekends). Use descriptive statistics (mean, median, mode of login times) to establish baseline login behavior per user/group and identify deviations using percentile analysis.\"
        *   **Category**: **Behavioral** (unusual access patterns over time, odd hours consistently).

    2.  **Evidence 2**: \"Connections via external remote services originating from IP addresses associated with other known suspicious activities or previously compromised accounts.\"
        *   **Data**: \"Zeek conn.log\"
        *   **Data Platform**: \"Network devices\"
        *   **Action**: \"Cross-reference source IP addresses from remote service logs (Zeek conn.log, VPN/Citrix logs) against a list of IP addresses identified in other security alerts or known compromised host lists. Inner join connection logs with Threat Intelligence feeds for known malicious IPs connecting via remote services. Investigate all sessions involving such IPs.\"
        *   **Category**: This is **Selector-based** (known malicious IPs, IPs from other alerts).

    3.  **Evidence 3**: \"Repeated successful logins by a valid account to external remote services from changing or unusual geographic locations or at consistent, off-hour times over an extended period.\"
        *   **Data**: \"Windows Event ID 4624; Zeek conn.log\"
        *   **Data Platform**: \"Servers, Network devices\"
        *   **Action**: \"Analyze successful external remote service logins (Event ID 4624, Zeek conn.log for relevant connections) for valid accounts. Geolocate source IPs. Track source location diversity and login times for specific accounts over weeks or months. Identify accounts logging in from a wide range of locations or consistently outside their normal work hours or at the same unusual off-hour times daily/weekly. Use statistical profiling (descriptive statistics, time series analysis) for individual account access patterns to detect anomalies.\"
        *   **Category**: **Behavioral** (changing geo-locations, consistent off-hour times over extended period).

    4.  **Evidence 4**: \"Successful external connections to internal systems using typically internal-only remote administration tools (e.g., WinRM, RDP to internal IPs, VNC) via unexpected network paths or without standard VPN usage.\"
        *   **Data**: \"Zeek conn.log; Windows Event ID 4624\"
        *   **Data Platform**: \"Servers, Network devices\"
        *   **Action**: \"Filter Zeek conn.log for external source IPs connecting to internal destination IPs on ports associated with internal remote administration (e.g., WinRM 5985/5986, internal RDP 3389, VNC 5900+). Correlate with Windows Event ID 4624 (Logon Type 3 for network logon) on the destination server for successful logons originating from these external IPs. Investigate connections that bypass expected external access methods (e.g., corporate VPN gateway). Use frequency analysis to identify hosts receiving unexpected external remote administration connections.\"
        *   **Category**: This is **Behavioral** (unexpected network paths, bypassing VPN). It also has a **Pattern** element (use of internal tools externally).

    **Missing for IR2:**
    *   Needs a clearer **Pattern-based** detection. Current one is mixed.

    **Improvements for IR2:**
    *   Refine existing to clearly map. Add a dedicated pattern-based detection. For persistence, this could involve checking for specific configurations or scheduled tasks that re-establish remote connections, or specific heartbeat traffic patterns from implants using remote services.

Let's start constructing the new JSON.

**General Plan:**
1.  Update `version` to \"2.0\" and `last_updated` to \"2025-05-10\" for both IRs.
2.  Ensure `NAI` is \"Insert site-specific NAI here\" for all.
3.  Data sources: Stick to Zeek logs and Windows Event IDs as requested.

**IR1: \"Has the adversary gained initial access to the network via external remote services? (TA0001 - Initial Access)\"**

*   **Selector-based detection:**
    *   **Evidence**: \"Connections to external remote services from IP addresses or involving domains known to be malicious or associated with exploit attempts against remote services.\"
    *   **Data**: \"Zeek conn.log\"
    *   **Data Platform**: \"Network devices\"
    *   **Action**: \"Inner join source IP addresses from remote service connection logs (Zeek conn.log, specifically to VPN, RDP, Citrix, Docker API, K8s API ports) with a high-confidence threat intelligence feed of known malicious IPs or IPs associated with remote service exploitation (updated within at least 24 hours). Alert on any matches.\"

*   **Pattern-based detection:**
    *   **Evidence**: \"Use of known exploit strings or specific User-Agent strings associated with remote service exploitation tools targeting external-facing services.\"
    *   **Data**: \"Zeek http.log (for web-based remote services or exposed APIs like Docker/K8s); Zeek conn.log (for payload data if available/decryptable)\"
    *   **Data Platform**: \"Network devices\"
    *   **Action**: \"Scan Zeek http.log request bodies and URIs for signatures of known exploits against common external remote services (e.g., Citrix, VPN vulnerabilities, Docker API, K8s API). Monitor for User-Agent strings commonly used by vulnerability scanners or exploitation tools targeting these services. Use regular expression matching for specific patterns. Maintain and update a list of such signatures and User-Agents.\"
    *   Consider another Pattern: \"Specific sequence of network events or log entries matching known T1133 exploit kits or attack chains for initial access.\"
        *   **Evidence**: \"Observed sequence of network connection attempts or specific log event patterns matching known exploit kits or multi-stage attacks targeting external remote services.\"
        *   **Data**: \"Zeek conn.log; Windows Event ID 4625, 4624\"
        *   **Data Platform**: \"Network devices, Servers\"
        *   **Action**: \"Define and monitor for sequences of events, such as: 1. Multiple failed login attempts (Event ID 4625) from an external IP to a remote service. 2. Followed by a successful login (Event ID 4624) from the same IP. 3. Followed by unusual network traffic patterns (Zeek conn.log) from the compromised host. Correlate events across data sources based on IP and timestamp using a defined time window (e.g., 30 minutes).\" This one is a bit behavioral too. Let's stick to the User-Agent one for clarity as a pattern.

*   **Behavioral-based detection (can refine existing ones or add new):**
    *   Existing 1: \"Anomalous login attempts...\" -> Good behavioral.
    *   Existing 2: \"Anomalous external usage patterns targeting potentially exposed remote services...\" -> Good behavioral.
    *   Existing 3: \"Multiple failed login attempts...followed by a successful login...\" -> Good behavioral (brute-force pattern).
    *   Existing 4: \"Discovery or exploitation attempts observed via connection patterns...\" -> Good behavioral (scanning, malformed requests).

    Let's re-evaluate the given Evidence 4 for IR1: \"Discovery or exploitation attempts observed via connection patterns to external-facing services that do not require authentication (e.g., unusual scan activity, malformed requests, unexpected ports).\"
    *   This is good. The action already mentions \"requests targeting known vulnerabilities\" which can be a selector part if specific signatures. \"Unusual scan activity\" is behavioral. \"Malformed requests\" is behavioral. \"Unexpected ports\" is behavioral.
    *   We can make this more explicitly cover a broader behavioral aspect.

Let's try to make sure we have 3 distinct categories well represented.

**IR1 - New Plan:**

1.  **Selector**:
    *   **Evidence**: \"Connections to external remote services from IP addresses known to be malicious or associated with credential stuffing campaigns.\"
    *   **Data**: \"Zeek conn.log; Windows Event ID 4624, 4625\"
    *   **Data Platform**: \"Network devices, Servers\"
    *   **Action**: \"Correlate source IP addresses from remote service authentication logs (Windows Event IDs 4624, 4625) and network connection logs (Zeek conn.log to VPN, RDP, Citrix, Docker API, K8s API ports) against a CTI feed of known malicious IPs, TOR exit nodes, or IPs associated with recent credential stuffing campaigns (updated daily). Alert on matches.\"

2.  **Pattern**:
    *   **Evidence**: \"Use of specific tool-related artifacts (e.g., User-Agent strings, known exploit patterns, default credentials) in connection attempts to external remote services.\"
    *   **Data**: \"Zeek http.log; Zeek conn.log\"
    *   **Data Platform**: \"Network devices\"
    *   **Action**: \"Monitor Zeek http.log for requests to external remote services (including exposed APIs like Docker/K8s) containing known malicious User-Agent strings, specific exploit patterns for known vulnerabilities (e.g., CVEs in Citrix, VPNs), or attempts to use default credentials. Use regular expression matching and a maintained list of such patterns. For non-HTTP services, inspect Zeek conn.log for unusual payload patterns if deep packet inspection is feasible or for connection attempts to services known to be targeted by specific tools.\"

3.  **Behavioral**:
    *   **Evidence**: \"A cluster of multiple failed login attempts from a single external IP or disparate IPs within the same geolocation to an external remote service, possibly followed by a successful login, indicating brute-force or password spraying.\"
    *   **Data**: \"Windows Event ID 4625; Windows Event ID 4624\"
    *   **Data Platform**: \"Servers\"
    *   **Action**: \"Aggregate Windows Event ID 4625 (failed login) and 4624 (successful login) for external remote services. Identify high rates of failed logins from single IPs or multiple IPs from similar geolocations targeting one or more accounts. Correlate failed attempts (e.g., >N failures in M minutes from one IP, or >X failures across Y IPs from a /24 targeting a specific account) with subsequent successful logins. Use descriptive statistics (counts, frequency) and set thresholds. Box-plots of failed login counts per IP can identify outliers.\" (This refines existing Evidence 3 for IR1).

4.  **Behavioral (additional, good one from original)**:
    *   **Evidence**: \"Anomalous login activity to external remote services (VPN, Citrix, RDP) based on deviations from established user or organizational baselines, such as logins from unusual geographic locations, at atypical times, or with impossible travel scenarios.\"
    *   **Data**: \"Windows Event ID 4624; Zeek conn.log\"
    *   **Data Platform**: \"Servers, Network devices\"
    *   **Action**: \"Establish baseline login patterns for users/groups accessing external remote services (source IP geolocation, time-of-day/day-of-week, login frequency). Monitor Windows Event ID 4624 and Zeek conn.log. Geolocate source IPs. Use statistical methods (e.g., IQR, Z-score, percentile analysis on login times and locations, linear regression for trend anomalies) to detect deviations from these baselines. Implement 'impossible travel' detection by correlating login timestamps and geolocations for individual accounts.\" (This refines existing Evidence 1 for IR1).

5.  **Behavioral (for exposed non-auth services)**:
    *   **Evidence**: \"Unusual connection patterns or anomalous API interaction with exposed external remote services (e.g., Docker API, Kubernetes API) that do not require authentication.\"
    *   **Data**: \"Zeek conn.log; Zeek http.log\"
    *   **Data Platform**: \"Network devices\"
    *   **Action**: \"Monitor Zeek conn.log for connections to ports of exposed services (Docker API 2375/2376, K8s API 6443/443, kubelet 10250). Profile normal traffic volume, source IP reputation, and request types/paths (from Zeek http.log if applicable). Use time series analysis (e.g., ARIMA, Exponential Smoothing) to detect significant spikes or changes in traffic patterns. Analyze entropy of requested API endpoints or commands to detect unusual breadth of interaction from a single external source.\" (This refines existing Evidence 2 & 4 for IR1).

This gives IR1 one Selector, one Pattern, and three Behavioral pieces of evidence, fulfilling the requirement. The detections are also more rigorous.

**IR 2: \"Is the adversary maintaining persistence using external remote services? (TA0003 - Persistence)\"**

*   **Selector-based detection (already good):**
    *   Existing Evidence 2: \"Connections via external remote services originating from IP addresses associated with other known suspicious activities or previously compromised accounts.\" This is a good selector.
        *   **Action**: \"Cross-reference source IP addresses from remote service logs (Zeek conn.log for VPN/Citrix/RDP, Windows Event ID 4624 for logins) against a CTI feed of known malicious IPs (updated daily) and internal lists of IPs associated with prior incidents or compromised accounts. Inner join connection logs and investigate all sessions involving such IPs.\" (Slightly rephrased for clarity).

*   **Pattern-based detection:**
    *   This is where IR2 was a bit weak. Persistence might involve specific tools or configurations.
    *   **Evidence**: \"Identification of unauthorized remote access tools or specific heartbeat traffic patterns to external C2 servers tunneled through legitimate remote services.\"
    *   **Data**: \"Zeek conn.log; Zeek ssl.log (for SNI/JA3); Windows Event ID 4688 (Process Creation)\"
    *   **Data Platform**: \"Network devices, Endpoints, Servers\"
    *   **Action**: \"Monitor Zeek conn.log for long-lived connections or periodic 'beaconing' traffic from internal hosts to external IPs over ports associated with remote services, especially if these deviate from typical user session lengths or data transfer patterns. Analyze Zeek ssl.log for unusual SNI values or JA3 hashes associated with known C2 frameworks tunneled over SSL/TLS via remote services. On hosts accessed via remote services, monitor for process creation events (Windows Event ID 4688) indicative of unauthorized remote access tools being launched post-login.\"
    *   *Alternative Pattern for T1133 itself*: \"Detection of new, unauthorized accounts configured for external remote service access, or modification of existing accounts to enable remote access where it wasn't previously allowed, especially service accounts.\" (This is more about account manipulation T1078, but for persistence via T1133).
    *   Let's stick with the tunneling/heartbeat one, as it's more directly T1133 persistence.

*   **Behavioral-based detection:**
    *   Existing Evidence 1: \"Successful logins to external remote services exhibiting unusual access patterns (e.g., multiple failed logins followed by success, logins at odd hours consistently over time) matching known Valid Accounts.\" -> Good behavioral.
    *   Existing Evidence 3: \"Repeated successful logins by a valid account to external remote services from changing or unusual geographic locations or at consistent, off-hour times over an extended period.\" -> Good behavioral.
    *   Existing Evidence 4: \"Successful external connections to internal systems using typically internal-only remote administration tools (e.g., WinRM, RDP to internal IPs, VNC) via unexpected network paths or without standard VPN usage.\" -> Good behavioral.

Let's refine these and ensure they are distinct.

**IR2 - New Plan:**

1.  **Selector**:
    *   **Evidence**: \"Connections via external remote services originating from or destined to IP addresses/domains on threat intelligence lists or associated with previously compromised internal accounts.\"
    *   **Data**: \"Zeek conn.log; Windows Event ID 4624\"
    *   **Data Platform**: \"Network devices, Servers\"
    *   **Action**: \"Continuously cross-reference source IP addresses from external remote service logs (Zeek conn.log capturing VPN, Citrix, RDP sessions; Windows Event ID 4624 for logins) against up-to-date threat intelligence feeds for known malicious IPs/domains. Also, correlate with internal lists of IP addresses associated with prior security incidents or known compromised user accounts. Alert on any confirmed matches.\"

2.  **Pattern**:
    *   **Evidence**: \"Traffic patterns characteristic of C2 communication (e.g., regular beaconing, specific data exfiltration patterns, known malicious JA3/SNI) tunneled through established external remote service connections.\"
    *   **Data**: \"Zeek conn.log; Zeek ssl.log; Zeek files.log\"
    *   **Data Platform**: \"Network devices\"
    *   **Action**: \"After a remote session is established (identified via Zeek conn.log or VPN logs), monitor the subsequent traffic from the internal endpoint. Analyze Zeek conn.log for periodic, fixed-interval connections (beaconing) or unusually large/sustained data transfers to external destinations not typical for the user/service. Inspect Zeek ssl.log for JA3/JA3S hashes or SNI values known to be associated with C2 frameworks. Monitor Zeek files.log for suspicious file transfers over the remote session. Use correlation analysis between session start time and these suspicious patterns.\"

3.  **Behavioral**:
    *   **Evidence**: \"A valid account exhibiting persistent, anomalous login behavior to external remote services, such as consistent off-hours access, logins from multiple, geographically disparate locations over a short period, or session durations significantly different from baseline.\"
    *   **Data**: \"Windows Event ID 4624; Zeek conn.log\"
    *   **Data Platform**: \"Servers, Network devices\"
    *   **Action**: \"For each valid account using external remote services, establish a baseline of normal login behavior (times, geolocations, session durations, frequency) using historical Windows Event ID 4624 and Zeek conn.log data. Continuously monitor for deviations. Employ time series analysis to detect changes in login frequency or timing. Use geolocation tracking to identify logins from new or improbable locations or 'impossible travel' scenarios. Flag accounts with consistently high entropy in login source IPs or login times outside of 2-3 standard deviations from their norm.\" (This refines existing Evidence 1 & 3 for IR2).

4.  **Behavioral (additional)**:
    *   **Evidence**: \"Use of external remote services to establish connections to high-value internal assets or perform sensitive actions not typical for the user account or originating IP, especially if bypassing normal multi-factor authentication controls.\"
    *   **Data**: \"Zeek conn.log; Windows Event ID 4624; Active Directory security logs (for internal actions)\"
    *   **Data Platform**: \"Network devices, Servers\"
    *   **Action**: \"Correlate external remote service access (Zeek conn.log, Windows Event ID 4624) with subsequent internal network activity from the source host of the remote session. Monitor for access to critical servers, databases, or administrative interfaces that are unusual for the user or the context of remote access. If MFA logs are available, investigate remote sessions that bypassed or satisfied MFA under suspicious circumstances (e.g., MFA fatigue success after multiple prompts). Use regression analysis to identify if certain remote access events correlate with increased internal risk indicators.\"

5.  **Behavioral (for persistence via exposed services)**:
    *   **Evidence**: \"Sustained or regularly repeated anomalous external connections to non-authenticating remote services (e.g., exposed Docker API, K8s API) from the same external source, indicating persistent unauthorized access.\"
    *   **Data**: \"Zeek conn.log; Zeek http.log\"
    *   **Data Platform**: \"Network devices\"
    *   **Action**: \"Track external IP addresses interacting with exposed, non-authenticating remote services (e.g., Docker API, K8s API) using Zeek conn.log and http.log. Profile interaction frequency, duration, and commands/API calls over time. Use time series analysis to detect sources that maintain unusually long connections, reconnect frequently from the same IP/subnet, or issue commands indicative of ongoing malicious activity (e.g., deploying containers, exfiltrating data) rather than one-off exploitation. Calculate entropy of commands per source IP over time; a stable, non-zero entropy might indicate persistent interactive use.\"

This gives IR2 one Selector, one Pattern, and three Behavioral pieces of evidence.

Final check on JSON structure and keys:
-   Top level is a list `[]`.
-   Each item in the list is an object `{}`.
-   The first key in this object is the IR string.
-   Inside that, \"Indicators\" key maps to an object.
-   Inside \"Indicators\", the Technique ID string is the key, mapping to an object.
-   Inside the Technique ID object, each \"Evidence\" string is a key, mapping to an object with \"Data\", \"Data Platform\", \"NAI\", \"Action\".
-   \"version\" and \"last_updated\" are at the same level as \"Indicators\".

The original template uses `\"Indicator\"` (singular) but the user's current plan uses `\"Indicators\"` (plural). The user's example template says `\"(Insert IR here)\": { \"Indicator\": { ... } }` but the current plan shows `\"Indicators\": { \"T1133 - External Remote Services\": { ... } }`. The problem description states \"The keys in the JSON object correspond to the following analytic plan components in this order: IR, Indicators, Evidence...\". I will stick to \"Indicators\" as the key, under which the actual indicator (Technique T1133) is nested, which seems more logical and aligns with the *current plan provided for modification*.

The problem states: \"Also note that for each IR, there is one indicator (the MITRE technique provided)\". The structure ` \"Indicators\": { \"T1133 - External Remote Services\": { ... } } ` reflects this. The Technique ID *is* the indicator.

The NAI dummy value: \"Insert site-specific NAI here\". This is used correctly.
Data sources: Windows Event IDs and Zeek logs are primary. I've included some specific event IDs (4624, 4625, 4688) and Zeek log types (conn, http, ssl, files).
Data Platforms: \"Endpoints\", \"Servers\", \"Network devices\". Correctly used.

The actions incorporate statistical methods as requested (frequency distribution, IQR, percentile analysis, time series analysis, entropy measures, correlation analysis, regression).

The language should be precise and actionable.

Let's review the definition of selector, pattern, behavior again:
-   **Selectors**: Atomic indicators (IPs, domains, hashes, specific vulnerability signatures).
    *   IR1.1: IPs from CTI. Yes.
    *   IR2.1: IPs/domains from CTI. Yes.
-   **Patterns**: Known tools, command-line usage, mid-level artifacts (e.g., specific sequence of events for a tool, known user-agent strings).
    *   IR1.2: User-Agents, exploit patterns, default creds. Yes.
    *   IR2.2: C2 beaconing, specific JA3/SNI, file transfer patterns. Yes.
-   **Behaviors**: TTPs, sequences, or hunting hypotheses. Independent of tool or actor. Hardest to evade.
    *   IR1.3: Brute-force/password spraying. Yes.
    *   IR1.4: Anomalous login activity (geo, time, impossible travel). Yes.
    *   IR1.5: Anomalous interaction with non-auth services. Yes.
    *   IR2.3: Persistent anomalous login behavior (geo, time, duration). Yes.
    *   IR2.4: Remote access to high-value assets / sensitive actions. Yes.
    *   IR2.5: Sustained/repeated anomalous connections to non-auth services. Yes.

Looks good. The coverage is there and the detections are more thorough.
The prompt mentions \"For the IR 'Has the adversary gained initial access to the network? (TA0001 - Initial Access)', the indicator should be a technique within that tactic; for example, 'T1190 - Exploit Public Facing Application'.\" My indicator is T1133. This is correct as T1133 is the technique being analyzed.

The structure:
```json
[
  { // First IR object
    \"Has the adversary gained initial access to the network via external remote services? (TA0001 - Initial Access)\": {
      \"Indicators\": {
        \"T1133 - External Remote Services\": { // This is the single indicator for this IR
          \"Evidence string 1 for Selector\": { /* Data, Platform, NAI, Action */ },
          \"Evidence string 2 for Pattern\": { /* Data, Platform, NAI, Action */ },
          \"Evidence string 3 for Behavior\": { /* Data, Platform, NAI, Action */ },
          // Potentially more evidence items
        }
      },
      \"version\": \"2.0\",
      \"last_updated\": \"2025-05-10\"
    }
  },
  { // Second IR object
    \"Is the adversary maintaining persistence using external remote services? (TA0003 - Persistence)\": {
      \"Indicators\": {
        \"T1133 - External Remote Services\": { // This is the single indicator for this IR
          \"Evidence string 1 for Selector\": { /* Data, Platform, NAI, Action */ },
          \"Evidence string 2 for Pattern\": { /* Data, Platform, NAI, Action */ },
          \"Evidence string 3 for Behavior\": { /* Data, Platform, NAI, Action */ },
          // Potentially more evidence items
        }
      },
      \"version\": \"2.0\",
      \"last_updated\": \"2025-05-10\"
    }
  }
]
```
This matches the requested structure and the user's provided \"current analytic plan\" structure.
The \"Indicators\" key seems to be a container for the actual indicator, which is the technique ID.
The evidence strings must be descriptive and unique.
I'll make sure the actions are detailed and guide the analyst properly.
The prompt mentions using \"MITRE ATT&CK tactic numbers; for example, (TA0001 - Initial Access)\" in the IR. This is already present in the IRs given.
The indicator is a technique within that tactic, \"T1190 - Exploit Public Facing Application\". Here, it's \"T1133 - External Remote Services\". This is correct.
The Data fields should just be data names: \"Windows Event ID 4688; Zeek conn.log\". This is how I've formatted them.
The Data Platform options are \"Endpoints\", \"Servers\", \"Network devices\". Used correctly.
NAI is \"Insert site-specific NAI here\". Used correctly.
Actions include simple detections and statistical methods. Yes.
\"version\" to \"2.0\", \"last_updated\" to \"2025-05-10\". Yes.
Double-check the exact phrasing for evidence to ensure it reflects the detection type clearly.
For instance, for IR1, the selector: \"Connections to external remote services from IP addresses known to be malicious or associated with credential stuffing campaigns.\" This is clear.
The pattern: \"Use of specific tool-related artifacts (e.g., User-Agent strings, known exploit patterns for common vulnerabilities, default credentials) in connection attempts to external remote services.\" This is clear.
The behavioral: \"A cluster of multiple failed login attempts from a single external IP or disparate IPs within the same geolocation to an external remote service, possibly followed by a successful login, indicating brute-force or password spraying activity.\" This is clear.
The language in actions seems appropriate, offering guidance and mentioning specific statistical techniques.
Looks ready."""),
                types.Part.from_text(text="""
[
  {
    \"Has the adversary gained initial access to the network via external remote services? (TA0001 - Initial Access)\": {
      \"Indicators\": {
        \"T1133 - External Remote Services\": {
          \"Connections to external remote services from IP addresses known to be malicious or associated with credential stuffing campaigns targeting remote service credentials.\": {
            \"Data\": \"Zeek conn.log; Windows Event ID 4624; Windows Event ID 4625\",
            \"Data Platform\": \"Network devices, Servers\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Correlate source IP addresses from remote service authentication logs (Windows Event IDs 4624 for success, 4625 for failure) and network connection logs (Zeek conn.log monitoring traffic to VPN, RDP, Citrix, Docker API, K8s API ports) against a continuously updated cyber threat intelligence (CTI) feed of known malicious IPs, TOR exit nodes, or IPs associated with recent credential stuffing campaigns. Alert on any confirmed matches and investigate associated activity.\"
          },
          \"Use of specific tool-related artifacts (e.g., distinctive User-Agent strings, known exploit patterns for common vulnerabilities, default credentials) in connection attempts to external remote services.\": {
            \"Data\": \"Zeek http.log; Zeek conn.log\",
            \"Data Platform\": \"Network devices\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Monitor Zeek http.log for requests to external remote services (including exposed APIs like Docker/K8s) containing known malicious User-Agent strings, specific exploit patterns targeting vulnerabilities in remote services (e.g., CVEs in Citrix, VPNs), or attempts to use default credentials. Use regular expression matching against a maintained list of such patterns. For non-HTTP services, inspect Zeek conn.log for unusual payload patterns if deep packet inspection is feasible or for connection attempts to services known to be targeted by specific exploitation tools.\"
          },
          \"A cluster of multiple failed login attempts from a single external IP or disparate IPs within the same geolocation to an external remote service, possibly followed by a successful login, indicating brute-force or password spraying activity.\": {
            \"Data\": \"Windows Event ID 4625; Windows Event ID 4624\",
            \"Data Platform\": \"Servers\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Aggregate Windows Event ID 4625 (failed login) and 4624 (successful login) for external remote services. Identify high rates of failed logins from single IPs or multiple IPs from similar geolocations targeting one or multiple accounts. Correlate high-volume failed attempts (e.g., >10 failures in 5 minutes from one IP, or >50 failures across >5 IPs from a /24 CIDR targeting a specific account or service) with any subsequent successful logins from those sources. Use descriptive statistics (counts, frequency distributions) and establish dynamic thresholds. Box-plots of failed login counts per source IP can help identify outliers indicative of brute-force attempts.\"
          },
          \"Anomalous login activity to external remote services (VPN, Citrix, RDP) based on deviations from established user or organizational baselines, such as logins from unusual geographic locations, at atypical times, or exhibiting impossible travel scenarios.\": {
            \"Data\": \"Windows Event ID 4624; Zeek conn.log\",
            \"Data Platform\": \"Servers, Network devices\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Establish baseline login patterns for users and groups accessing external remote services, considering source IP geolocation, time-of-day/day-of-week, login frequency, and typical session duration using historical Windows Event ID 4624 and Zeek conn.log data. Continuously monitor for deviations. Geolocate source IPs for all remote logins. Employ statistical methods (e.g., Inter-Quartile Range (IQR) for outlier detection, Z-score for deviations, percentile analysis on login times and locations, linear regression for trend anomalies) to flag anomalous logins. Implement 'impossible travel' detection by correlating login timestamps and geolocations for individual accounts, alerting on physically unfeasible sequential logins.\"
          },
          \"Unusual connection patterns or anomalous API interaction with exposed external remote services (e.g., Docker API, Kubernetes API) that do not require authentication, indicative of scanning, discovery, or initial exploitation.\": {
            \"Data\": \"Zeek conn.log; Zeek http.log\",
            \"Data Platform\": \"Network devices\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Monitor Zeek conn.log for connections to known ports of exposed, non-authenticating services (e.g., Docker API 2375/2376, K8s API 6443/443, kubelet 10250). Profile normal traffic volume, source IP diversity, and request types/paths (from Zeek http.log if applicable). Use time series analysis (e.g., ARIMA, Exponential Smoothing) to detect significant spikes or changes in traffic patterns from external IPs. Analyze entropy of requested API endpoints or commands from a single external source; unusually high or low entropy compared to baseline may indicate scanning or targeted interaction. Correlate findings with network scans or malformed requests.\"
          }
        }
      },
      \"version\": \"2.0\",
      \"last_updated\": \"2025-05-10\"
    }
  },
  {
    \"Is the adversary maintaining persistence using external remote services? (TA0003 - Persistence)\": {
      \"Indicators\": {
        \"T1133 - External Remote Services\": {
          \"Connections via external remote services originating from or destined to IP addresses/domains on threat intelligence lists, or involving user accounts previously associated with suspicious activity or compromise.\": {
            \"Data\": \"Zeek conn.log; Windows Event ID 4624\",
            \"Data Platform\": \"Network devices, Servers\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Continuously cross-reference source IP addresses from external remote service logs (Zeek conn.log capturing VPN, Citrix, RDP sessions; Windows Event ID 4624 for logins) against up-to-date threat intelligence feeds for known malicious IPs/domains. Additionally, correlate login events with internal lists of user accounts or IP addresses associated with prior security incidents or confirmed compromises. Alert on any confirmed matches and prioritize investigation for persistent threats.\"
          },
          \"Traffic patterns characteristic of command and control (C2) communication (e.g., regular beaconing, specific data exfiltration signatures, known malicious JA3/SNI hashes) tunneled through established external remote service connections.\": {
            \"Data\": \"Zeek conn.log; Zeek ssl.log; Zeek files.log; Windows Event ID 4688\",
            \"Data Platform\": \"Network devices, Endpoints, Servers\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"After an external remote session is established (identified via Zeek conn.log, VPN logs, or Windows Event ID 4624), monitor the subsequent network traffic from the internal endpoint involved in the session. Analyze Zeek conn.log for periodic, fixed-interval connections (beaconing) or unusually large/sustained data transfers to external destinations inconsistent with typical user behavior. Inspect Zeek ssl.log for JA3/JA3S hashes or SNI values known to be associated with C2 frameworks. Monitor Zeek files.log for suspicious file transfers (type, size, frequency) over the remote session. Correlate with Windows Event ID 4688 on the accessed host for processes launched during the remote session that might initiate C2 traffic. Use correlation analysis between session start time and these patterns.\"
          },
          \"A valid user account exhibiting persistent, anomalous login behavior to external remote services over an extended period, such as consistent off-hours access from unusual geolocations, or session characteristics markedly different from established individual baselines.\": {
            \"Data\": \"Windows Event ID 4624; Zeek conn.log\",
            \"Data Platform\": \"Servers, Network devices\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"For each valid account using external remote services, maintain and update a dynamic baseline of normal login behavior (typical login times, source geolocations, session durations, data transfer volumes, frequency of access) using historical Windows Event ID 4624 and Zeek conn.log data. Continuously monitor for significant long-term deviations. Employ time series analysis to detect gradual or sudden shifts in login frequency or timing patterns. Use geolocation tracking to identify accounts consistently logging in from new or high-risk locations, or those exhibiting 'impossible travel' scenarios repeatedly. Flag accounts with persistently high entropy in login source IPs or login times consistently falling outside of 2-3 standard deviations from their established norm, which could indicate account takeover for persistent access.\"
          },
          \"Use of external remote services to establish connections to high-value internal assets or perform sensitive administrative actions not typical for the user account's role or the context of remote access, especially if MFA appears to be bypassed or satisfied suspiciously.\": {
            \"Data\": \"Zeek conn.log; Windows Event ID 4624; Active Directory security logs; MFA logs\",
            \"Data Platform\": \"Network devices, Servers\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Correlate external remote service access (identified via Zeek conn.log, Windows Event ID 4624) with subsequent internal network activity originating from the endpoint used for the remote session. Monitor for access to critical servers (e.g., domain controllers, financial systems), databases, or administrative interfaces that are anomalous for the user's role or typical remote access behavior. Analyze Active Directory security logs for privileged group modifications or sensitive commands executed. If MFA logs are available, investigate remote sessions that bypassed MFA or where MFA was satisfied under suspicious circumstances (e.g., multiple MFA push notifications followed by approval, logins from new devices). Use regression analysis to identify if specific remote access events correlate with increased internal risk indicators or lateral movement attempts.\"
          },
          \"Sustained or regularly repeated anomalous external connections to non-authenticating remote services (e.g., exposed Docker API, Kubernetes API) from the same external source or range, indicating persistent unauthorized access rather than one-off exploitation.\": {
            \"Data\": \"Zeek conn.log; Zeek http.log\",
            \"Data Platform\": \"Network devices\",
            \"NAI\": \"Insert site-specific NAI here\",
            \"Action\": \"Track external IP addresses and their interaction patterns with exposed, non-authenticating remote services (e.g., Docker API on ports 2375/2376, Kubernetes API on 6443/443, kubelet on 10250) using Zeek conn.log and http.log. Profile interaction frequency, session duration, and types of commands/API calls over time for each source. Use time series analysis to detect external sources that maintain unusually long connections, reconnect at regular intervals (suggesting scripted persistence), or issue commands indicative of ongoing malicious activity (e.g., repeatedly deploying malicious containers, querying sensitive information, exfiltrating data) rather than initial reconnaissance. Calculate entropy of API commands per source IP over extended periods; stable, non-zero entropy might indicate persistent interactive use or automated tool activity.\"
          }
        }
      },
      \"version\": \"2.0\",
      \"last_updated\": \"2025-05-10\"
    }
  }
]"""),
            ],
        ),
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text=prompt),
            ],
        ),
    ]
    generate_content_config = types.GenerateContentConfig(
        response_mime_type="text/plain",
        system_instruction=[
            types.Part.from_text(text="""Here is an example analytic plans template:

[
    {
        \"(Insert IR here)\": {
            \"Indicators\": {
                \"(Insert form of evidence here)\": {
                    \"Data\": \"\",
                    \"Data Platform\": \"\",
                    \"NAI\": \"\",
                    \"Action\": \"\"
                },
                \"(Insert form of evidence here)\": {
                    \"Data\": \"\",
                    \"Data Platform\": \"\",
                    \"NAI\": \"\",
                    \"Action\": \"\"
                }
            },
          \"version\": \"\",
          \"last_updated\": \"\"
        }
    }
]"""),
        ],
    )

    # Send the request to the generative AI model.
    response = client.models.generate_content(
        model=model,                  # The specified target model
        contents=contents,            # The constructed multi-turn conversation history
        config=generate_content_config, # Configuration including response format and system instructions
    )

    # Return the text content of the model's response.
    # This is expected to be the generated ASOM in JSON string format.
    return response.text


# In[ ]:


def skip_by_version(item_key: str, output_file_path: str, prompt_content: str) -> bool:
    """
    Custom skip logic
    """
    with open(output_file_path, 'r', encoding='utf-8') as fd:
        try:
            content = fd.read()
            if ("```json" in content):
                content = content.replace("```json", "").replace("```", "")
            data = json.loads(content.replace(r"\"", r"\'").replace("\\", "\\\\"))
        except Exception as e:
            log(f"Error reading {output_file_path}: {e}")
            sys.exit(1)
        all_versions_are_target = all(
            (
                isinstance(item, dict) and  # Ensure the item itself is a dictionary
                len(item) == 1 and          # Ensure the item_dict has exactly one key (the question)
                # Safely get the inner dictionary (value of the single key) and assign to inner_dict
                # next(iter(item.values())) is safe because len(item) == 1 ensures item is not empty
                isinstance(inner_dict := next(iter(item.values())), dict) and # Ensure inner_dict is a dictionary
                inner_dict.get("version") == "2.0" # Check the version safely
            )
            for item in data
        )
        if (all_versions_are_target):
            log(f"Skipping '{item_key}' (custom logic): Version 2.0 already exists.")
            return True
    return False


# In[ ]:


# Number of retries
max_retries = 10

for attempt in range(max_retries):
    try:
        prompt_model(
            requests_per_minute = 10,
            max_iterations = 100,
            prompts_collection = final_prompt_library,
            generation_function = generate_refined_plan,
            output_directory = "./",
            skip_condition_func = skip_by_version,
            model_name =  "gemini-2.5-pro" # "gemini-2.5-pro-preview-05-06"
        )
        # If the call is successful, break out of the loop
        log("Function executed successfully!")
        break
    except genai.errors.ServerError as e:
        # Check if the error is a 503 "model overloaded" error
        if "503 UNAVAILABLE" in str(e):
            log(f"Attempt {attempt + 1} failed with a 503 Server Error. Model is overloaded.")

            # If we have not reached the max number of retries, wait and then continue
            if attempt < max_retries - 1:
                log("Waiting for 60 seconds before retrying...")
                time.sleep(60)
            else:
                # If this was the last attempt, print a final error message
                log("Max retries reached. Could not execute the function.")
                # Optionally, you can re-raise the exception if you want the program to halt
                # raise e
        else:
            # If it's a different ServerError, you might want to handle it differently
            # For now, we will just print the error and stop
            log(f"An unexpected server error occurred: {e}")
            break
    except Exception as e:
        # Catch any other potential exceptions
        log(f"An unexpected error occurred: {e}")
        break


# ## Additional Code
# 
# This section contains additional code written during this notebook's development that is not strictly necessary to build, improve, and refine the analytic plans. Execution halts after this cell so that subsequent cells will not be run automatically.

# In[ ]:


# Halt execution before the code below is run
sys.exit(0)


# In[ ]:


def update_json_files_structure(root_dir, dict_to_add_or_update):
    """
    Recursively finds JSON files in root_dir and adds or updates keys
    from dict_to_add_or_update into the dictionary value of the primary
    key within each item of the main list.

    Args:
        root_dir (str): The path to the directory to start searching from.
        dict_to_add_or_update (dict): A dictionary containing key-value pairs
                                      to add or update.
    """
    if not isinstance(dict_to_add_or_update, dict):
        log("Error: 'dict_to_add_or_update' must be a dictionary.")
        return

    if not dict_to_add_or_update:
        log("Warning: 'dict_to_add_or_update' is empty. No changes will be made.")
        # Optionally return here if desired, or proceed to scan files anyway.
        # return

    if not os.path.isdir(root_dir):
        log(f"Error: Directory '{root_dir}' not found.")
        return

    log(f"Starting scan in directory: {root_dir}")
    log(f"Data to add/update: {dict_to_add_or_update}")
    files_processed = 0
    files_modified = 0
    errors_encountered = 0
    error_files = []
    warning_files = []

    for subdir, _, files in os.walk(root_dir):
        for filename in files:
            if filename.lower().endswith('.json'):
                file_path = os.path.join(subdir, filename)
                log(f"Processing file: {file_path}")
                files_processed += 1
                modified_in_this_file = False
                try:
                    # Read the JSON file with UTF-8 encoding
                    # Use a variable for the file handle to ensure it's closed
                    # even if errors occur before json.load() finishes
                    read_f = None
                    try:
                        read_f = open(file_path, 'r', encoding='utf-8')
                        data = json.load(read_f)
                    finally:
                        if read_f:
                            read_f.close()


                    # Ensure the top level is a list
                    if isinstance(data, list):
                        # Iterate through each item (dictionary) in the list
                        for item in data:
                            if isinstance(item, dict):
                                # Iterate through the primary keys in the item dictionary
                                # (Assuming one primary key per item as per example)
                                for primary_key in list(item.keys()): # Use list() for safe iteration
                                    # Check if the value associated with the primary key is a dictionary
                                    if isinstance(item[primary_key], dict):
                                        target_dict = item[primary_key]
                                        # Iterate through the keys and values to add/update
                                        for key_to_update, value_to_update in dict_to_add_or_update.items():
                                            # Check if the key needs to be added or if the value is different
                                            if key_to_update not in target_dict or target_dict[key_to_update] != value_to_update:
                                                target_dict[key_to_update] = value_to_update
                                                modified_in_this_file = True # Mark that a change occurred
                                    else:
                                        log(f"  Warning: Value for key '{primary_key}' in {file_path} is not a dictionary. Skipping update for this key.")
                                        warning_files.append(file_path)
                            else:
                                 log(f"  Warning: Found an item in the list that is not a dictionary in {file_path}. Skipping this item.")
                                 warning_files.append(file_path)

                    else:
                        log(f"  Warning: Root object in {file_path} is not a list. Skipping this file.")
                        warning_files.append(file_path)
                        continue # Skip to the next file

                    # Write the modified data back to the file if changes were made
                    if modified_in_this_file:
                        write_f = None
                        try:
                            write_f = open(file_path, 'w', encoding='utf-8')
                            # Use indent for pretty printing, adjust as needed (e.g., indent=4)
                            json.dump(data, write_f, indent=2, ensure_ascii=False)
                            log(f"  Successfully updated: {file_path}")
                            files_modified += 1
                        finally:
                             if write_f:
                                 write_f.close()
                    else:
                        log(f"  No changes needed for: {file_path}")


                except json.JSONDecodeError:
                    log(f"  Error: Invalid JSON format in {file_path}. Skipping.")
                    error_files.append(file_path)
                    errors_encountered += 1
                except IOError as e:
                    log(f"  Error: Could not read/write file {file_path}. Reason: {e}. Skipping.")
                    error_files.append(file_path)
                    errors_encountered += 1
                except Exception as e:
                    # Log the type of exception for better debugging
                    log(f"  Error: An unexpected {type(e).__name__} occurred processing {file_path}. Reason: {e}. Skipping.")
                    error_files.append(file_path)
                    errors_encountered += 1

    print("\n--- Processing Summary ---")
    print(f"Total files scanned: {files_processed}")
    print(f"Files successfully modified: {files_modified}")
    # Corrected calculation for skipped/unchanged files
    skipped_unchanged = files_processed - files_modified - errors_encountered
    print(f"Files skipped, unchanged, or with warnings: {skipped_unchanged}")
    print(f"Errors encountered during processing: {errors_encountered}")
    print("--------------------------")
    print("Error files:\n",error_files)
    print()
    print("Warning files:\n",warning_files)


# In[ ]:


# target_directory = "./"

# data_to_add = {
#     "version": "1.0",
#     "last_updated": "2025-05-04"
# }

# # Call the generalized function
# update_json_files_structure(target_directory, data_to_add)




================================================
FILE: ASOM Generator.py
================================================
#!/usr/bin/env python
# coding: utf-8

# # Analytic Scheme of Maneuver Generator
# 
# This notebook uses analytic plans to generate an analytic scheme of maneuver.
# 
# ## Background
# 
# As described in TC 3-12.2.4.1, "The Analytic Scheme of Maneuver is the plan to collect and analyze technical data to meet specific information requirements. It identifies what data to analyze, how to analyze it, and why it is being analyzed." The analytic scheme of maneuver, or ASOM, consists of the following components:
# 
# * **Priority information requirement**:
# * **Indicator**:
# * **Evidence**:
# * **Data**:
# * **NAI**:
# * **Analytic**:
# 
# 
# 
# ## Environment Setup
# 
# This section sets up the environment. It installs packages necessary to generate the analytic plans, imports modules, initializes helper functions, and finally defines global variables. This section also mounts Google Drive to the runtime and moves into the project folder.
# 
# ### Install Packages

# In[1]:


get_ipython().system('pip install -U -q "google" 1> /dev/null')


# ### Import Modules

# In[2]:


from google.colab import userdata
from google.colab import drive
import json
import os
import datetime
import pandas as pd
import re
from collections import defaultdict


# ### Initialize Helper Functions
# 
# The first function, `log`, logs a message to the console prepended with the current timestamp in the ISO8601 format.

# In[3]:


def log(message, end="\n", flush = True):
    """
    Logs a message to the console, prepended with the current timestamp
    in ISO 8601 format.

    Args:
    message (str): The string message to log.
    """

    # Access the global flag controlling verbosity
    global verbose

    # Get the current date and time
    current_time = datetime.datetime.now()

    # Format the timestamp in ISO 8601 format
    timestamp = current_time.isoformat()

    # Construct the final log string using an f-string for clean formatting
    log_string = f"[{timestamp}] {message}"

    # Print the log string to the console if logging is turned on (verbose = True)
    if (verbose == True):
        print(log_string, end = end, flush = flush)


# The second function, `build_asom`, accepts a series of MITRE ATT&CK techniques as input and returns a collection of analytic plans that correspond to those techniques.

# In[4]:


import os
import json
import copy
import re
from pathlib import Path
from typing import Dict, List, Set, Any, Tuple

TACTIC_ID_PATTERN = re.compile(r'^(?P<tactic_id>[A-Z0-9]{2,}-?[A-Z0-9]{0,})')
# Technique IDs: MITRE (T#### or T####.###) OR D3-* style identifiers
TECHNIQUE_ID_PATTERN = re.compile(r'^(?P<tech_id>T\d{4}(?:\.\d{3})?|D3-[A-Z]+)')

def _normalize_tactic_key(tactic: str) -> Tuple[str, str]:
    """
    Accepts strings like:
        "TA0001 - Initial Access"
        "D3-D - Detect"
        "TA0001"
    Returns (tactic_id, tactic_name_or_empty).
    """
    if " - " in tactic:
        tid, name = tactic.split(" - ", 1)
        return tid.strip(), name.strip()
    return tactic.strip(), ""

def _normalize_technique_id(tech: str) -> str:
    """
    Extract technique ID from various accepted forms:
      "T1055"
      "T1055.009"
      "T1055.009 - Process Injection"
      "D3-NTA - Network Traffic Analysis"
      "D3-PM"
    Returns the normalized ID or empty string if not found.
    """
    m = TECHNIQUE_ID_PATTERN.match(tech.strip())
    return m.group("tech_id") if m else ""

def _load_json_safely(path: Path) -> Any:
    """
    Loads JSON; strips code fences if present.
    Returns parsed object or raises.
    """
    text = path.read_text(encoding="utf-8").strip()
    if text.startswith("```"):
        # Remove fenced code markers (``` or ```json)
        text = re.sub(r'^```(?:json)?\s*', '', text)
        text = re.sub(r'\s*```$', '', text)
    return json.loads(text)

def _is_new_schema_object(obj: dict) -> bool:
    required = {"information_requirement", "tactic_id", "tactic_name", "indicators"}
    return isinstance(obj, dict) and required.issubset(obj.keys())

def _filter_indicators(ir_obj: dict, allowed_ids: Set[str]) -> dict:
    """
    Return a *new copy* of ir_obj with indicators filtered to those whose technique_id
    is in allowed_ids. If allowed_ids is empty, returns ir_obj unchanged.
    """
    if not allowed_ids:
        return ir_obj  # no filtering requested

    new_obj = copy.deepcopy(ir_obj)
    new_indicators = [
        ind for ind in new_obj.get("indicators", [])
        if _normalize_technique_id(ind.get("technique_id", "")) in allowed_ids
    ]
    new_obj["indicators"] = new_indicators
    return new_obj

def build_asom(
    attack_chain: Dict[str, List[str]],
    directory: str | Path = ".",
    detect_filename: str = "D3-D.json",
    include_detect_first: bool = True,
    filter_indicators: bool = True,
    deduplicate: bool = True
) -> List[dict]:
    """
    Build an ASOM (list of IR objects in the *new* analytic plan schema) filtered
    by the provided attack_chain.

    Parameters
    ----------
    attack_chain : dict
        Mapping of tactic strings -> list of technique strings.
        Tactic strings can be 'TA0001 - Initial Access' or just 'TA0001'.
        Techniques can be:
            'T1078'
            'T1078 - Valid Accounts'
            'T1055.009'
            'T1055.009 - Process Injection'
            'D3-NTA - Network Traffic Analysis'
            'D3-PM'
    directory : str | Path
        Directory containing new-format analytic plan JSON files.
    detect_filename : str
        Special file whose IR objects should be placed first (if include_detect_first = True).
    include_detect_first : bool
        If True, IR objects from detect_filename (if present) are prepended before the rest.
    filter_indicators : bool
        If True, restrict the 'indicators' list in each returned IR object to only those
        techniques explicitly requested for that tactic. If False, include all indicators
        for matching IR objects (as long as tactic matches).
    deduplicate : bool
        If True, remove duplicates (same information_requirement + tactic_id combination).
        Keeps the first occurrence (maintaining order).

    Returns
    -------
    list[dict]
        List of IR objects (each already in the new schema).
    """

    directory = Path(directory)

    # Normalize attack_chain into:
    #   tactic_map: tactic_id -> set(normalized technique IDs)
    tactic_map: Dict[str, Set[str]] = {}
    for tactic_str, technique_list in attack_chain.items():
        tactic_id, _ = _normalize_tactic_key(tactic_str)
        norm_tecs = {_normalize_technique_id(t) for t in technique_list}
        # Remove empties
        norm_tecs = {t for t in norm_tecs if t}
        tactic_map.setdefault(tactic_id, set()).update(norm_tecs)

    results: List[dict] = []

    def process_file(path: Path):
        try:
            data = _load_json_safely(path)
        except Exception as e:  # noqa
            print(f"[WARN] Could not parse {path.name}: {e}")
            return

        if not isinstance(data, list):
            # print(f"[WARN] {path.name} root is not a list. Skipping.")
            return

        for obj in data:
            if not _is_new_schema_object(obj):
                print(f"[WARN] {path.name} does not conform to schema. Skipping.")
                continue  # silently skip non-conforming entries

            tactic_id = obj.get("tactic_id", "").strip()
            if tactic_id not in tactic_map:
                # print(f"[INFO] {tactic_id} not in tactic map. Skipping.")
                continue  # tactic not requested

            # Possibly filter indicators
            if filter_indicators:
                filtered_obj = _filter_indicators(copy.deepcopy(obj), tactic_map[tactic_id])
                # If after filtering there are no indicators (and we requested some), skip.
                if tactic_map[tactic_id] and not filtered_obj.get("indicators"):
                    continue
                results.append(filtered_obj)
            else:
                results.append(copy.deepcopy(obj))

    # 1. Optionally process the special detect_filename first
    if include_detect_first:
        detect_path = directory / detect_filename
        if detect_path.exists():
            process_file(detect_path)
        else:
            print(f"[ERROR] {detect_filename} not found")
            pass

    # 2. Process all other JSON files
    for path in sorted(directory.glob("*.json")):
        if include_detect_first and path.name == detect_filename:
            continue
        process_file(path)

    # 3. Deduplicate (stable) if requested
    if deduplicate:
        seen = set()
        unique: List[dict] = []
        for obj in results:
            key = (obj.get("information_requirement"), obj.get("tactic_id"))
            if key in seen:
                continue
            seen.add(key)
            unique.append(obj)
        results = unique

    return results


# ### Define Global Variables

# In[5]:


# Toggle logging on (verbose = True)/off (verbose = False)
verbose = True
# verbose = False


# In[6]:


# Rate limits: https://ai.google.dev/gemini-api/docs/rate-limits
# Pricing: https://ai.google.dev/gemini-api/docs/pricing
# Usage: https://console.cloud.google.com/apis/api/generativelanguage.googleapis.com/metrics?project=gen-lang-client-0497172401
# Note that this notebook is designed to be run in Google Colab. The line below reads the Gemini API key for AI Studio,
# which is configured in the Secrets tab on the left side of the Colab window.
os.environ["GEMINI_API_KEY"] = userdata.get("GOOGLE_API_KEY")
log("Gemii API key loaded.")


# ### Mount Google Drive

# In[7]:


# Mount Google Drive and move into the Google AI Studio folder
DRIVE_PATH = "/content/drive"
TECHNIQUES_PATH = "/content/drive/MyDrive/Google AI Studio/analytic-plans"

drive.mount(DRIVE_PATH)
log(f"Google Drive mounted to {DRIVE_PATH}")

os.chdir(TECHNIQUES_PATH)
log(f"Changed directory to {TECHNIQUES_PATH}")


# ## Test Analytic Scheme of Maneuver Generation
# 
# This section generates a small analytic scheme of maneuver off of a test attack chain data set.

# In[8]:


# Example attack_chain data structure
attack_chain_data = {
  "D3-D - Detect": [
    "D3-NTA - Network Traffic Analysis",
    "D3-PM - Platform Monitoring"
  ],
  "TA0001 - Initial Access": [
      "T1133 - External Remote Services"
  ],
  "TA0003 - Persistence": [
    "T1078 - Valid Accounts",
    "T1053 - Scheduled Task/Job"
  ],
  "TA0008 - Lateral Movement": [
    "T1021 - Remote Services",
    "T1570 - Lateral Tool Transfer"
  ],
  "TA0011 - Command and Control": [
    "T1071 - Application Layer Protocol"
  ],
  "TA0010 - Exfiltration": [
     "T1041 - Exfiltration Over C2 Channel",
     "T1048 - Exfiltration Over Alternative Protocol",
     "T1567 - Exfiltration Over Web Service"
  ]
}

log("Building ASOM...")
resulting_asom = build_asom(attack_chain_data)
log("Finished.")


# In[ ]:


# print(json.dumps(resulting_asom, indent=2))


# In[9]:


def format_asom(asom_input_list, joiner="; "):
    """
    Format the output from build_asom_new (new analytic plan schema) into a pandas DataFrame.

    Parameters
    ----------
    asom_input_list : list[dict]
        List of IR objects, each with keys:
          - information_requirement (str)
          - tactic_id (str)
          - tactic_name (str)
          - indicators (list of indicator dicts)
              * indicator dict: { "technique_id", "name", "evidence": [ evidence dicts ] }
              * evidence dict: { "description", "data_sources", "data_platforms", "nai", "action" }
          - version
          - date_created
          - last_updated
          - contributors (list)
    joiner : str
        Delimiter to join list fields (data_sources, data_platforms, contributors).

    Returns
    -------
    pandas.DataFrame
        Tabular view with hierarchical-style indices expressed as string columns:
          IR Index -> Indicator Index (IRIndex.TechSubIndex) -> Evidence Index (IRIndex.TechSubIndex.EvidenceSubIndex)
    """
    import pandas as pd

    table_rows = []
    ir_index = 0

    if not isinstance(asom_input_list, list):
        raise TypeError("Expected asom_input_list to be a list produced by build_asom_new.")

    for ir_obj in asom_input_list:
        if not isinstance(ir_obj, dict):
            print(f"[WARN] Skipping non-dict IR object: {ir_obj}")
            continue

        required_ir_keys = {"information_requirement", "tactic_id", "tactic_name", "indicators"}
        if not required_ir_keys.issubset(ir_obj.keys()):
            print(f"[WARN] IR object missing required keys: {ir_obj.keys()}")
            continue

        indicators = ir_obj.get("indicators", [])
        if not indicators:
            # Optionally emit a placeholder row; for now just warn and skip.
            print(f"[INFO] IR '{ir_obj.get('information_requirement')}' has no indicators; skipping.")
            continue

        ir_index += 1
        tactic_id = ir_obj.get("tactic_id", "")
        tactic_name = ir_obj.get("tactic_name", "")
        information_requirement = ir_obj.get("information_requirement", "")
        information_requirement = f"{information_requirement} ({tactic_id} - {tactic_name})"
        version = ir_obj.get("version", "")
        date_created = ir_obj.get("date_created", "")
        last_updated = ir_obj.get("last_updated", "")
        contributors = ir_obj.get("contributors", [])
        contributors_joined = joiner.join(contributors) if isinstance(contributors, list) else str(contributors)

        tech_sub_index = 0
        for indicator in indicators:
            if not isinstance(indicator, dict):
                print(f"[WARN] Skipping non-dict indicator in IR '{information_requirement}': {indicator}")
                continue

            technique_id = indicator.get("technique_id", "")
            technique_name = indicator.get("name", "")
            evidence_list = indicator.get("evidence", [])

            tech_sub_index += 1
            indicator_index_str = f"{ir_index}.{tech_sub_index}"

            if not evidence_list:
                # If no evidence entries, optionally create a placeholder row.
                # (Current behavior: skip but warn.)
                print(f"[INFO] Indicator '{technique_id} - {technique_name}' has no evidence entries.")
                continue

            evidence_sub_index = 0
            for evidence in evidence_list:
                if not isinstance(evidence, dict):
                    print(f"[WARN] Skipping non-dict evidence under technique '{technique_id}': {evidence}")
                    continue

                evidence_sub_index += 1
                evidence_index_str = f"{indicator_index_str}.{evidence_sub_index}"

                description = evidence.get("description", "")
                data_sources = evidence.get("data_sources", [])
                data_platforms = evidence.get("data_platforms", [])
                nai = evidence.get("nai", "")
                action = evidence.get("action", "")

                # Normalize list fields
                if isinstance(data_sources, list):
                    data_sources_joined = joiner.join(data_sources)
                else:
                    data_sources_joined = str(data_sources)

                if isinstance(data_platforms, list):
                    data_platforms_joined = joiner.join(data_platforms)
                else:
                    data_platforms_joined = str(data_platforms)

                row = {
                    "CCIR Index": ir_index,
                    "CCIR": information_requirement,
                    "Tactic ID": tactic_id,
                    "Tactic Name": tactic_name,
                    "Indicator Index": indicator_index_str,
                    "Indicator": f"{technique_id} - {technique_name}",
                    "Technique ID": technique_id,
                    "Technique Name": technique_name,
                    "Evidence Index": evidence_index_str,
                    "Evidence Description": description,
                    "Data Sources": data_sources_joined,
                    "Data Platforms": data_platforms_joined,
                    "NAI": nai,
                    "Action": action
                    # "Version": version,
                    # "Date Created": date_created,
                    # "Last Updated": last_updated,
                    # "Contributors": contributors_joined,
                }
                table_rows.append(row)

    df = pd.DataFrame(table_rows)

    # Ensure all expected columns exist (important if there were zero rows)
    column_order = [
        "CCIR Index",
        "CCIR",
        "Tactic ID",
        "Tactic Name",
        "Indicator Index",
        "Indicator",
        "Technique ID",
        "Technique Name",
        "Evidence Index",
        "Evidence Description",
        "Data Sources",
        "Data Platforms",
        "NAI",
        "Action"
        # "Version",
        # "Date Created",
        # "Last Updated",
        # "Contributors",
    ]
    for col in column_order:
        if col not in df.columns:
            df[col] = pd.NA

    # Optional: sort by hierarchical indices for readability
    if not df.empty:
        df.sort_values(by=["CCIR Index", "Indicator Index", "Evidence Index"], inplace=True, ignore_index=True)

    return df[column_order]


# In[10]:


formatted_df = format_asom(resulting_asom)


# ### Preview Formatted DataFrame

# In[11]:


# To display the full content of cells if they are long
with pd.option_context('display.max_rows', None,
                        'display.max_columns', None,
                        'display.width', 1000,
                        'display.max_colwidth', None):
    display(formatted_df)


# ### Sort and Re-Index DataFrame

# In[14]:


import pandas as pd
import re

def renumber_formatted_df(formatted_df: pd.DataFrame, d3_tactic_id: str = "D3-D") -> pd.DataFrame:
    """
    Reorder and re-index the formatted analytic summary DataFrame produced from the new schema.

    Fixes prior issue where identical CCIR text received different indices by normalizing the CCIR
    string (collapsing internal whitespace, stripping leading/trailing spaces) before assigning indices.

    Steps
    -----
    1. Sort so that rows with Tactic ID == d3_tactic_id appear first, then remaining tactics
       in normal alphanumeric order of Tactic ID.
    2. Rebuild CCIR Index (1, 2, 3, ...) based on first appearance order (post-sort) of each
       normalized CCIR string.
    3. Rebuild Indicator Index inside each CCIR (e.g., 1.1, 1.2, 2.1, ...), preserving the
       first-seen ordering of each unique Indicator within its CCIR.
    4. Rebuild Evidence Index inside each (CCIR, Indicator) (e.g., 1.1.1, 1.1.2, ...), preserving
       first-seen ordering of each unique Evidence Description within that (CCIR, Indicator).
       Duplicate evidence descriptions within the same (CCIR, Indicator) get the same index.
    5. Return the updated DataFrame (sorted) with refreshed indices.

    Parameters
    ----------
    formatted_df : pd.DataFrame
        The original DataFrame (must include columns: 'CCIR', 'Tactic ID', 'Indicator',
        'Evidence Description', 'CCIR Index', 'Indicator Index', 'Evidence Index').
    d3_tactic_id : str
        The tactic ID to force to the top (default "D3-D").

    Returns
    -------
    pd.DataFrame
        Reindexed and sorted DataFrame.
    """

    df = formatted_df.copy()

    # --- 1. Sorting with D3 tactic first ---
    # Create a primary sort key: 0 for D3 tactic, 1 otherwise.
    df["_tactic_primary_key"] = (df["Tactic ID"] != d3_tactic_id).astype(int)

    # Keep original row order to maintain stable ordering within groups when needed
    df["_orig_row"] = range(len(df))

    # Sort: D3 Tactic first, then by Tactic ID (alphanumeric), then CCIR (normalized),
    # then by Indicator, then Evidence Description to cluster logically.
    def _norm_ccir(s: str) -> str:
        # Normalize whitespace in CCIR (collapse internal spaces & strip)
        return re.sub(r"\s+", " ", s).strip()

    df["_CCIR_norm"] = df["CCIR"].map(_norm_ccir)

    df.sort_values(
        by=["_tactic_primary_key", "Tactic ID", "_CCIR_norm", "Indicator", "Evidence Description", "_orig_row"],
        kind="stable",
        inplace=True
    )

    # --- 2. Rebuild CCIR Index based on normalized CCIR text ---
    ccir_index_map = {}
    next_ccir_idx = 0
    new_ccir_indices = []
    for ccir_norm in df["_CCIR_norm"]:
        # print(ccir_norm)
        if ccir_norm not in ccir_index_map:
            next_ccir_idx += 1
            ccir_index_map[ccir_norm] = next_ccir_idx
        new_ccir_indices.append(ccir_index_map[ccir_norm])
    df["CCIR Index"] = new_ccir_indices

    # --- 3. Rebuild Indicator Index within each CCIR (preserve first-seen order) ---
    indicator_index_map = {}  # (ccir_norm, indicator) -> sub index
    ccir_indicator_counters = {}  # ccir_norm -> next sub index
    new_indicator_indices = []

    for ccir_norm, ccir_idx, indicator in zip(df["_CCIR_norm"], df["CCIR Index"], df["Indicator"]):
        key = (ccir_norm, indicator)
        if key not in indicator_index_map:
            ccir_indicator_counters.setdefault(ccir_norm, 0)
            ccir_indicator_counters[ccir_norm] += 1
            indicator_index_map[key] = ccir_indicator_counters[ccir_norm]
        sub_idx = indicator_index_map[key]
        new_indicator_indices.append(f"{ccir_idx}.{sub_idx}")
    df["Indicator Index"] = new_indicator_indices

    # Precompute (ccir_norm, indicator) -> (ccir_idx, indicator_sub_idx) for evidence indexing
    indicator_sub_lookup = {}
    for (ccir_norm, indicator), sub in indicator_index_map.items():
        indicator_sub_lookup[(ccir_norm, indicator)] = sub

    # --- 4. Rebuild Evidence Index inside each (CCIR, Indicator) ---
    evidence_index_map = {}  # (ccir_norm, indicator, evidence_description) -> sub evidence idx
    evidence_counters = {}   # (ccir_norm, indicator) -> next evidence sub idx
    new_evidence_indices = []

    for ccir_norm, indicator, evidence_desc in zip(df["_CCIR_norm"], df["Indicator"], df["Evidence Description"]):
        parent_key = (ccir_norm, indicator)
        evidence_key = (ccir_norm, indicator, evidence_desc)
        if evidence_key not in evidence_index_map:
            evidence_counters.setdefault(parent_key, 0)
            evidence_counters[parent_key] += 1
            evidence_index_map[evidence_key] = evidence_counters[parent_key]
        ccir_idx = ccir_index_map[ccir_norm]
        indicator_sub_idx = indicator_sub_lookup[parent_key]
        evidence_sub_idx = evidence_index_map[evidence_key]
        new_evidence_indices.append(f"{ccir_idx}.{indicator_sub_idx}.{evidence_sub_idx}")
    df["Evidence Index"] = new_evidence_indices

    # --- 5. Final tidy & cleanup ---
    df.sort_values(
        by=["CCIR Index", "Indicator Index", "Evidence Index"],
        kind="stable",
        inplace=True
    )
    df.drop(columns=["_tactic_primary_key", "_orig_row", "_CCIR_norm"], inplace=True)
    df.reset_index(drop=True, inplace=True)

    return df


# ---------- Example usage (uncomment in your notebook) ----------
updated_df = renumber_formatted_df(formatted_df)

# To display the full content of cells if they are long
with pd.option_context('display.max_rows', None,
                        'display.max_columns', None,
                        'display.width', 1000,
                        'display.max_colwidth', None):
    display(updated_df)


# In[13]:


# Export the full ASOM to an Excel file
# updated_df.to_excel("test_asom_full.xlsx")
updated_df.to_excel("asom_2025.08.19.xlsx")


# ### Create Visually Spanned ASOM

# In[ ]:


import pandas as pd
from pathlib import Path

def _compute_hierarchical_spans(df: pd.DataFrame, span_columns):
    """
    Compute hierarchical row spans.

    For each column in span_columns (ordered highest -> lowest), we:
      * Partition the DataFrame into the ranges (row intervals) created by the
        *previous* column's spans.
      * Within each parent range, merge only contiguous identical values.
      * Record span length at the first row of each run; mark subsequent rows with 0.

    Returns
    -------
    spans : dict[col -> list[int]]
        spans[col][i] = 0 means the cell at (i, col) is hidden under a rowspan
        spans[col][i] = k (k>=1) means a rowspan of length k starts at row i.
    """
    spans = {col: [1] * len(df) for col in span_columns}
    # Start with a single parent range covering all rows
    parent_ranges = [(0, len(df))]

    for col in span_columns:
        col_spans = [1] * len(df)
        new_parent_ranges = []
        for (start, end) in parent_ranges:
            i = start
            while i < end:
                val = df.iat[i, df.columns.get_loc(col)]
                j = i + 1
                while j < end and df.iat[j, df.columns.get_loc(col)] == val:
                    j += 1
                run_len = j - i
                if run_len > 1:
                    col_spans[i] = run_len
                    for r in range(i + 1, j):
                        col_spans[r] = 0
                # Even singleton runs become parent ranges for the next column
                new_parent_ranges.append((i, j))
                i = j
        spans[col] = col_spans
        parent_ranges = new_parent_ranges  # constrain next column to these subranges
    return spans


def html_with_rowspan_hier(df: pd.DataFrame, span_columns):
    """
    Hierarchical version: only merges in a lower column if higher-level
    columns are already merged (or identical) across that same block.
    """
    df = df.copy()
    span_columns = [c for c in span_columns if c in df.columns]
    spans = _compute_hierarchical_spans(df, span_columns)

    def esc(x):
        if pd.isna(x):
            return ""
        return (str(x)
                .replace("&", "&amp;")
                .replace("<", "&lt;")
                .replace(">", "&gt;"))

    cols = list(df.columns)
    parts = [
        "<table border='1' cellspacing='0' cellpadding='4' "
        "style='border-collapse:collapse;font-family:Arial, sans-serif;font-size:12px;'>"
    ]
    parts.append("<thead><tr>" + "".join(f"<th>{esc(c)}</th>" for c in cols) + "</tr></thead>")
    parts.append("<tbody>")

    for i in range(len(df)):
        parts.append("<tr>")
        for col in cols:
            if col in span_columns:
                span_val = spans[col][i]
                if span_val == 0:
                    continue  # covered by an earlier row's rowspan
                elif span_val > 1:
                    parts.append(f"<td rowspan='{span_val}' style='vertical-align:top'>{esc(df.at[i, col])}</td>")
                else:
                    parts.append(f"<td>{esc(df.at[i, col])}</td>")
            else:
                parts.append(f"<td>{esc(df.at[i, col])}</td>")
        parts.append("</tr>")
    parts.append("</tbody></table>")
    return "\n".join(parts)


def display_rowspan_hier(updated_df: pd.DataFrame):
    """
    Convenience display using hierarchical rowspan merging.
    """
    from IPython.display import HTML, display
    span_columns = [
        "CCIR Index", "CCIR",
        "Tactic ID", "Tactic Name",
        "Indicator Index", "Indicator",
        "Technique ID", "Technique Name"
    ]
    html = html_with_rowspan_hier(updated_df, span_columns=span_columns)
    display(HTML(html))


def export_with_merged_cells_hier(
    df: pd.DataFrame,
    span_columns,
    path: str | Path,
    sheet_name="Sheet1",
    header_style=None
):
    """
    Excel export with TRUE hierarchical merges (using openpyxl).
    Lower-level merges never cross boundaries defined by higher-level merges.
    """
    from openpyxl import Workbook
    from openpyxl.utils import get_column_letter
    from openpyxl.styles import Alignment, Font

    df = df.copy()
    span_columns = [c for c in span_columns if c in df.columns]
    spans = _compute_hierarchical_spans(df, span_columns)

    wb = Workbook()
    ws = wb.active
    ws.title = sheet_name
    cols = list(df.columns)

    # Header
    for c_idx, col in enumerate(cols, start=1):
        cell = ws.cell(row=1, column=c_idx, value=col)
        if header_style:
            if "font" in header_style: cell.font = header_style["font"]
            if "alignment" in header_style: cell.alignment = header_style["alignment"]
        else:
            cell.font = Font(bold=True)
            cell.alignment = Alignment(horizontal="center", vertical="center", wrap_text=True)

    # Data
    for r_idx, (_, row) in enumerate(df.iterrows(), start=2):
        for c_idx, col in enumerate(cols, start=1):
            ws.cell(row=r_idx, column=c_idx, value="" if pd.isna(row[col]) else row[col])

    # Apply merges hierarchically using precomputed spans
    for col in span_columns:
        c_idx = cols.index(col) + 1
        r = 0
        while r < len(df):
            span_len = spans[col][r]
            excel_row_start = r + 2  # offset for header
            if span_len > 1:
                ws.merge_cells(
                    start_row=excel_row_start,
                    start_column=c_idx,
                    end_row=excel_row_start + span_len - 1,
                    end_column=c_idx
                )
                anchor = ws.cell(row=excel_row_start, column=c_idx)
                anchor.alignment = Alignment(vertical="top", wrap_text=True)
            r += max(span_len, 1)

    # Simple auto-width
    for c_idx, col in enumerate(cols, start=1):
        max_len = max(len(str(ws.cell(row=r, column=c_idx).value or "")) for r in range(1, len(df) + 2))
        ws.column_dimensions[get_column_letter(c_idx)].width = min(max_len + 2, 60)

    path = Path(path)
    wb.save(path)
    return path


# ---------------- Example Usage ----------------
span_cols = [
    "CCIR Index", "CCIR",
    "Tactic ID", "Tactic Name",
    "Indicator Index", "Indicator",
    "Technique ID", "Technique Name"
]

from IPython.display import HTML
HTML(html_with_rowspan_hier(updated_df, span_cols))

display_rowspan_hier(updated_df)

export_with_merged_cells_hier(updated_df, span_cols, "asom_hierarchical.xlsx")


# In[ ]:







================================================
FILE: Edit Plans.py
================================================
#!/usr/bin/env python
# coding: utf-8

# In[16]:


import os
import re
from pathlib import Path

# Configuration
DRY_RUN = False          # Set to False to actually rename
ALLOWED_EXT = ".json"   # Only process JSON files
UNIQUE_SUFFIX = True    # If True, append a counter when a target filename already exists
PRINT_SKIPPED = True    # Verbose reporting of skipped files

# Regex to capture the leading "code" segment before the first ' - '
# Supports:
#   TA0001            (tactic)
#   T1078             (technique)
#   T1055.009         (sub-technique)
#   D3-D              (custom framework code like D3)
#   D3-NTA, D3-PM     (other D3 codes)
CODE_PATTERN = re.compile(r"""
    ^
    (?P<code>                       # Capture group 'code'
        (?:TA\d{4})                 # e.g. TA0001
        | (?:T\d{4}(?:\.\d{3})?)    # e.g. T1078 or T1055.009
        | (?:D3-[A-Z]+)             # e.g. D3-D or D3-NTA
    )
    \s*-\s+                         # Separator: dash with surrounding spaces
    .+                              # Remainder of the name (ignored)
    $
""", re.VERBOSE)

def derive_new_name(filename: str) -> str | None:
    """
    Given a filename (without directory), return the new filename (code.json) or None if not match.
    """
    stem, ext = os.path.splitext(filename)
    if ext.lower() != ALLOWED_EXT:
        return None

    match = CODE_PATTERN.match(stem)
    if not match:
        return None

    code = match.group("code")
    return f"{code}{ALLOWED_EXT}"

def safe_rename(src: Path, dst: Path) -> Path:
    """
    Rename src -> dst, optionally appending a numeric suffix to avoid overwrites.
    Returns the final destination path (even in dry run).
    """
    final_dst = dst
    if UNIQUE_SUFFIX:
        counter = 1
        while final_dst.exists() and final_dst.resolve() != src.resolve():
            final_dst = dst.with_name(f"{dst.stem}_{counter}{dst.suffix}")
            counter += 1

    if DRY_RUN:
        # print(f"[DRY RUN] {src.name} -> {final_dst.name}")
        pass
    else:
        src.rename(final_dst)
        print(f"[RENAMED] {src.name} -> {final_dst.name}")
    return final_dst

def rename_technique_files(directory: str | Path = "."):
    """
    Scan `directory` for JSON files whose names start with a recognized code + ' - '.
    Rename them to just the code (preserving extension). Collisions handled per config.
    """
    directory = Path(directory)
    if not directory.is_dir():
        raise NotADirectoryError(directory)

    processed = 0
    renamed = 0
    skipped = 0

    for path in sorted(directory.iterdir()):
        if not path.is_file():
            continue
        new_name = derive_new_name(path.name)
        if new_name is None:
            if PRINT_SKIPPED:
                print(f"[SKIP] {path.name} (pattern not matched)")
            skipped += 1
            continue

        processed += 1
        if path.name == new_name:
            if PRINT_SKIPPED:
                print(f"[SKIP] {path.name} (already normalized)")
            continue

        target_path = path.with_name(new_name)
        safe_rename(path, target_path)
        renamed += 1

    print("\nSummary:")
    print(f"  Processed (matched pattern): {processed}")
    print(f"  Renamed:                    {renamed}")
    print(f"  Skipped (non-matching):     {skipped}")
    if DRY_RUN:
        print("\nNOTE: DRY_RUN=True (no actual renames performed). Set DRY_RUN=False and re-run to apply.")

# -------- Run (adjust DRY_RUN above first) --------
if __name__ == "__main__":
    rename_technique_files(".")


# In[ ]:






