Directory structure:
└── watson/
    ├── AskSage.ipynb
    ├── generator.py
    ├── watson.py
    ├── config/
    │   ├── asom.yml
    │   └── generator.yml
    └── src/
        ├── attack_retriever.py
        ├── colorlog.py
        ├── formatting.py
        └── processing.py

================================================
FILE: AskSage.ipynb
================================================
Error processing notebook: 'text/plain'


================================================
FILE: generator.py
================================================
#!/usr/bin/env python3
import logging

import sys
from pathlib import Path
# Add project root to path to allow src imports
sys.path.insert(0, str(Path(__file__).resolve().parent))

from src.colorlog import make_console_handler

# Define a module-level logger to be accessible by all functions
logger = logging.getLogger(__name__)

logger.info("Importing built-in modules.")
import json
from datetime import datetime, timezone
import base64
import os
import re
import requests
import urllib3

logger.info("Importing installed modules")
from asksageclient import AskSageClient
from google import genai
from google.genai import types
import yaml

logger.info("Importing project-specific modules.")
from src.attack_retriever import build_technique_dictionary

BASE_PROMPT = """
I need you to generate an analytic plan. The analytic plan consists of the following components:

1.  Information Requirements (IRs): These identify the information that the commander considers most important. For example, 'Has the adversary gained initial access? (TA0001 - Initial Access)' (PIR) or 'What data is available for threat detection and modeling? (D3-D - Detect)' (FFIR). Note that PIRs are tagged with a MITRE ATT&CK tactic, and FFIRs are tagged with a MITRE D3FEND tactic. We call these "CCIR" generally.

2.  Indicators: These are positive or negative evidence of threat activity pertaining to one or more information requirements. They are observable clues related to a specific information requirement. For the IR above, indicators might include:
    * T1078 - Valid Accounts
    For the FFIR above, indicators might include:
    * D3-NTA - Network Traffic Analysis
    Note that indicators for PIRs are tagged with MITRE ATT&CK techniques, and FFIRs are tagged with MITRE D3FEND techniques.

3.  Evidence: This is the concrete information that supports or refutes an indicator. It provides the 'proof' and can vary in complexity. For the indicator 'T1078 - Valid Accounts', evidence could be 'A valid account login exhibits multiple anomalous characteristics simultaneously, such as originating from a rare geographic location, using an unfamiliar device, and occurring outside of normal working hours.' For the indicator 'D3-NTA', evidence could be 'Logs generated from network activity such as network flow metadata and network traffic content'.

4.  Data: This describes the precise data necessary to identify evidence. Specificity here is key (e.g., Zeek Conn logs, Sysmon event ID 4624, Active Directory security logs). For the evidence, focus your plan on the following data sources: network logs, specifically Zeek logs; host logs, specifically Windows Event IDs. Write only the data name. For example, Windows Event ID 4688, Zeek conn.log

5. Data Source (Platform): Use a dummy value here of "TBD".

6. Named Areas of Interest (NAIs): These are areas where data that will satisfy a specific information requirement can be collected. For the IR above, NAIs could include 'Our organization's internet gateway', 'Authentication servers', 'Servers hosting sensitive data', and 'Endpoint devices of high-value targets'.

7.  Actions: These are high-level instructions that guide the analysts' search for evidence. For the evidence associated with the indicator 'T1078 - Valid Accounts' and the associated PIR 'Has the adversary gained initial access? (TA0001 - Initial Access)', an action could be: 'For each successful login (Windows Event ID 4624), enrich with geolocation data from the source IP (Zeek conn.log). Establish a multi-faceted baseline for each user account including typical login times, source countries/ISPs, and devices used. Use a scoring system where deviations from the baseline (e.g., rare country, login at 3 AM, new device) add to a risk score. A high cumulative risk score, identified using statistical models or descriptive statistics (e.g., multiple metrics exceeding 2 standard deviations from the norm), indicates a likely compromised account.' For the evidence associated wit hthe indicator 'D3-NTA' and the associated FFIR 'What data is available for threat detection and modeling? (D3-D - Detect)', an acount could be: 'Inventory available network log sources (e.g., networking appliances, Zeek, PCAP). For each source, perform a time series analysis to visualize data volume over at least the last 30 days to identify collection gaps or anomalies. Use descriptive statistics to summarize key fields like protocol distribution in Zeek conn.log and the frequency of top requested domains in dns.log to establish a cursory understanding of network activity. Compare across data sources to validate collection consistency and identify individual sensor blind spots.' Focus mostly on simple detections, but also look for opportunities to incorporate basic data science techniques here, such as percentiles, entropy scores, statistics, and other, similar methods.

Based on these definitions, please generate a detailed analytic plan in plain, unstyled text in the JSON format below. Provide specific and relevant examples for each component within this format.

[
  {
    "information_requirement": "Insert CCIR here",
    "tactic_id": "Insert MITRE ATT&CK or MITRE D3FEND tactic T-code here",
    "tactic_name": "Insert the tactic name here",
    "indicators": [
      {
        "technique_id": "Insert MITRE technique T-code here",
        "name": "Insert the technique name here",
        "evidence": [
          {
            "description": "Describe the evidence here",
            "data_sources": [
              "First data source",
              "Second data source"
            ],
            "data_platforms": [
              "TBD"
            ],
            "nai": "Insert site-specific NAI here",
            "action": "Describe the action here"
          }
        ]
      }
    ]
   }
]

Based on that format, generate an analytic plan for the following technique. If you are given an offensive technique, a T-code, then only generate PIRs; if you are given a defensive technique, a D-code, then only generate FFIRs. Pay extremely close attention to the type of matrix the technique references (enterprise, ICS, mobile), which will have a significant impact on how you build this plan.
"""

def setup_logging() -> tuple[str, Path]:
    """Initializes console and file logging."""
    run_ts = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")
    logs_dir = Path("logs")
    logs_dir.mkdir(parents=True, exist_ok=True)
    log_path = logs_dir / f"generator_{run_ts}.log"

    fmt = "%(asctime)s %(levelname)-8s %(name)s :: %(message)s"
    datefmt = "%Y-%m-%d %H:%M:%S"

    # Configure the root logger
    root = logging.getLogger()
    root.setLevel(logging.INFO)
    root.handlers.clear()
    root.addHandler(make_console_handler(fmt, datefmt))

    fh = logging.FileHandler(log_path, encoding="utf-8")
    fh.setFormatter(logging.Formatter(fmt=fmt, datefmt=datefmt))
    root.addHandler(fh)

    return run_ts, log_path

def load_config(path: Path) -> dict:
    """Loads the YAML configuration file."""
    if not path.exists():
        logger.critical(f"Configuration file not found at '{path}'. Exiting.")
        raise SystemExit(1)
    
    with open(path, "r", encoding="utf-8") as f:
        try:
            config = yaml.safe_load(f)
            logger.info(f"Successfully loaded configuration from '{path}'.")
            return config
        except yaml.YAMLError as e:
            logger.critical(f"Error parsing YAML configuration: {e}")
            raise SystemExit(1)

def generate_analytic_plan(prompt, model, ask_sage_client, max_retries=3, retry_delay=1):    
    # Validate inputs
    if not prompt or not isinstance(prompt, str):
        raise ValueError("Prompt must be a non-empty string")
    
    if not model or not isinstance(model, str):
        raise ValueError("Model must be a non-empty string")
    
    # Primary attempt with Gemini
    for attempt in range(max_retries):
        try:
            # Initialize Gemini client with API key
            api_key = os.environ.get("GEMINI_API_KEY")
            if not api_key:
                logger.warning("GEMINI_API_KEY not found in environment variables")
                raise ValueError("GEMINI_API_KEY environment variable is not set")
            
            client = genai.Client(api_key=api_key)
            
            # Generate content using Gemini
            response = client.models.generate_content(
                model=model,
                contents=[
                    types.Content(
                        role="user",
                        parts=[
                            types.Part.from_text(text=prompt),
                        ],
                    ),
                ],
                config=types.GenerateContentConfig(
                    temperature=0.7,
                    # max_output_tokens=2048,
                    # top_p=0.95,
                    # top_k=40,
                ),
            )
            
            # Validate response
            if response and hasattr(response, 'text'):
                logger.info(f"Successfully generated response using {model}")
                return response.text
            else:
                logger.warning("Response received but no text content found")
                raise ValueError("Invalid response format from Gemini")
                
        except exceptions.ResourceExhausted as e:
            # Handle 429 Too Many Requests specifically
            logger.warning(f"Rate limit hit (attempt {attempt + 1}/{max_retries}): {str(e)}")
            
            if attempt < max_retries - 1:
                # Exponential backoff
                wait_time = retry_delay * (2 ** attempt)
                logger.info(f"Waiting {wait_time} seconds before retry...")
                time.sleep(wait_time)
                continue
            else:
                # All retries exhausted, fall back to backup model
                logger.info("All Gemini retries exhausted, falling back to AskSage backup model")
                break
                
        except exceptions.GoogleAPIError as e:
            # Handle other Google API errors
            error_message = str(e)
            
            # Check for rate limiting in error message
            if "429" in error_message or "quota" in error_message.lower() or "rate" in error_message.lower():
                logger.warning(f"Rate limit detected in error message: {error_message}")
                
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (2 ** attempt)
                    logger.info(f"Waiting {wait_time} seconds before retry...")
                    time.sleep(wait_time)
                    continue
                else:
                    logger.info("Falling back to AskSage backup model")
                    break
            else:
                # For non-rate-limit errors, log and re-raise
                logger.error(f"Google API error: {error_message}")
                raise
                
        except Exception as e:
            # Handle any other unexpected errors
            error_message = str(e)
            
            # Check if it's a rate limit error in disguise
            if "429" in error_message or "Too Many Requests" in error_message:
                logger.warning(f"Rate limit detected in general exception: {error_message}")
                
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (2 ** attempt)
                    logger.info(f"Waiting {wait_time} seconds before retry...")
                    time.sleep(wait_time)
                    continue
                else:
                    logger.info("Falling back to AskSage backup model")
                    break
            else:
                # For non-rate-limit errors, log the error
                logger.error(f"Unexpected error with Gemini: {error_message}")
                # Attempt fallback instead of failing completely
                break
    
    # Fallback to AskSage backup model
    try:
        logger.info("Using AskSage backup model: google-gemini-2.5-pro")
        
        # Validate ask_sage_client
        if not ask_sage_client:
            raise ValueError("ask_sage_client is not initialized")
        
        response = ask_sage_client.query(
            prompt,
            persona="default",
            dataset="none",
            limit_references=0,
            temperature=0.7,  # Match the temperature from primary model
            live=0,
            model="google-gemini-2.5-pro",
            system_prompt=None
        )
        
        # Validate response structure
        if not response or not isinstance(response, dict):
            raise ValueError("Invalid response from AskSage API")
        
        if 'message' not in response:
            logger.error(f"Response missing 'message' field: {response}")
            raise ValueError("Response from AskSage API does not contain 'message' field")
        
        message = response['message']
        
        if not message:
            raise ValueError("Empty message received from AskSage API")
        
        logger.info("Successfully generated response using AskSage backup model")
        return message
        
    except Exception as e:
        logger.error(f"Backup model also failed: {str(e)}")
        raise Exception(f"Both primary and backup models failed. Last error: {str(e)}")

def main():
    """Main script execution."""
    
    run_ts, log_path = setup_logging()
    logger.info(f"Run initialized at: {run_ts} | Logging to: {log_path}")

    # --- 0. Instantiate models ---

    logger.info("Loading Gemini API key")
    try:
        with open(".GEMINI_API_KEY", "r") as fd:
            os.environ["GEMINI_API_KEY"] = fd.read().strip()
    except:
        logger.info("Failed to import Gemini API key")
    
    logger.info("Loading Sage API key")
    try:
        with open("./credentials.json", "r") as file:
            credentials = json.load(file)
            # Validate required keys
            if 'credentials' not in credentials or 'api_key' not in credentials['credentials']:
                logger.error("Missing required keys in the credentials file.")
                raise
    
        # Extract the API key and email from the credentials
        sage_api_key = credentials['credentials']['api_key']
        sage_email = credentials['credentials']['Ask_sage_user_info']['username']
    except FileNotFoundError:
        raise FileNotFoundError(f"Credentials file not found at ./credentials.json")
    except json.JSONDecodeError:
        raise ValueError(f"Invalid JSON format in the credentials file: ./credentials.json")
    
    # --- 1. Load Configuration ---
    logger.info("Loading configuration")
    config = load_config(Path("config/generator.yml"))
    output_dirs_map = config.get("output_directories", {})
    default_output_dir = Path(output_dirs_map.get("default", "techniques"))
    matrices = config.get("matrices", ["enterprise"])
    filter_techniques = config.get("techniques", [])
    model = config.get("model", "gemini-2.5-flash")

    if not output_dirs_map:
        logger.critical("Configuration key 'output_directories' is missing or empty. Cannot determine where to save files.")
        raise SystemExit(1)

    # --- 2. Build Technique Dictionary ---
    logger.info("Building technique dictionary")
    technique_dict = build_technique_dictionary(matrices)
    
    target_techniques = {}
    if filter_techniques:
        logger.info(f"Filtering for {len(filter_techniques)} specific techniques from config.")
        for full_key, tech_data in technique_dict.items():
            if tech_data['technique_id'] in filter_techniques:
                target_techniques[full_key] = tech_data
    else:
        logger.info("Processing all available techniques from selected matrices.")
        target_techniques = technique_dict
    
    logger.info(f"Will generate analytic plans for {len(target_techniques)} techniques.")

    # --- 3. Instantiate Sage client ---
    logger.info("Instantiating Sage client")
    # Disable SSL warnings
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    # Monkey-patch requests to disable SSL verification globally
    old_request = requests.Session.request
    
    def new_request(self, method, url, **kwargs):
        kwargs['verify'] = False
        return old_request(self, method, url, **kwargs)
    
    requests.Session.request = new_request

    # Now create your client
    ask_sage_client = AskSageClient(
        sage_email, 
        sage_api_key, 
        user_base_url="https://api.genai.army.mil/user/", 
        server_base_url="https://api.genai.army.mil/server/"
    )

    # --- 4. Generate and Save Analytic Plans ---
    logger.info("Generating analytic plans")
    for i, (full_key, tech_data) in enumerate(target_techniques.items()):
        
        # --- Determine the correct output directory for this technique ---
        matrix_type = tech_data.get('matrix')
        if matrix_type and matrix_type in output_dirs_map:
            output_dir = Path(output_dirs_map[matrix_type])
        else:
            logger.warning(f"No output directory specified for matrix '{matrix_type}'. Using default: '{default_output_dir}'")
            output_dir = default_output_dir

        # --- Extract Technique ID and check if file already exists ---
        try:
            technique_id = full_key.split(" - ")[0].strip()
        except IndexError:
            logger.error(f"Could not parse technique ID from key '{full_key}'. Skipping.")
            continue

        file_path = output_dir / f"{technique_id}.json"
        if file_path.exists():
            logger.warning(f"Plan for {technique_id} already exists at '{file_path}'. Skipping generation.")
            continue
            
        logger.info(f"[{i+1}/{len(target_techniques)}] Generating plan for {full_key}...")
        
        # Ensure the target directory exists before writing
        output_dir.mkdir(parents=True, exist_ok=True)
        
        prompt = (
            f"{BASE_PROMPT}\n\n"
            f"Technique: {full_key}\n\n"
            f"Matrix: MITRE ATT&CK for {tech_data['matrix'].upper()}\n\n"
            f"Tactic(s): {tech_data['tactic']}\n\n"
            f"Description: {tech_data['description']}\n\n"
            f"Detection: {tech_data['detection']}"
        )
        
        # This is where you would call your actual AI model
        plan_blob = generate_analytic_plan(prompt, model, ask_sage_client=ask_sage_client)

        # --- Extract JSON from the raw text blob ---
        json_str = None
        match = re.search(r"```(?:json)?\s*([\s\S]*?)\s*```", plan_blob)
        if match:
            json_str = match.group(1).strip()
        else:
            start_index = -1
            first_bracket = plan_blob.find('[')
            first_curly = plan_blob.find('{')
            
            if first_bracket != -1 and first_curly != -1:
                start_index = min(first_bracket, first_curly)
            elif first_bracket != -1:
                start_index = first_bracket
            else:
                start_index = first_curly

            if start_index != -1:
                end_index = max(plan_blob.rfind(']'), plan_blob.rfind('}'))
                if end_index > start_index:
                    json_str = plan_blob[start_index : end_index + 1]

        if not json_str:
            logger.error(f"Could not find a valid JSON object in the response for {full_key}. Skipping.")
            continue

        # --- Parse, add metadata, and save the file ---
        try:
            plan_data = json.loads(json_str)
            if isinstance(plan_data, list):
                current_date_str = datetime.now(timezone.utc).strftime("%Y-%m-%d")
                for ir_object in plan_data:
                    ir_object["version"] = "1.0"
                    ir_object["date_created"] = current_date_str
                    ir_object["last_updated"] = current_date_str
                    ir_object["contributors"] = ["Zachary Szewczyk"]

                try:
                    with open(file_path, "w", encoding="utf-8") as f:
                        json.dump(plan_data, f, indent=2)
                    logger.info(f"Successfully saved plan for {technique_id} to '{file_path}'.")
                except IOError as e:
                    logger.error(f"Failed to write file for {technique_id}: {e}")
            else:
                logger.warning(f"Expected a list from AI for {full_key}, but got {type(plan_data)}. Skipping file write.")

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse extracted JSON for {full_key}. Error: {e}. Skipping file write.")
            
    logger.info("Script finished successfully.")

if __name__ == "__main__":
    main()


================================================
FILE: watson.py
================================================
#!/usr/bin/env python3
import logging
from datetime import datetime, timezone
from pathlib import Path

import pandas as pd
import yaml

# Add project root to path to allow src imports
import sys
sys.path.insert(0, str(Path(__file__).resolve().parent))

# Import local modules
from src.colorlog import make_console_handler
from src.formatting import export_simple_excel, export_with_merged_cells
from src.processing import build_asom, format_asom, renumber_formatted_df

def setup_logging() -> tuple[str, Path]:
    """Initializes console and file logging."""
    run_ts = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")
    logs_dir = Path("logs")
    logs_dir.mkdir(parents=True, exist_ok=True)
    log_path = logs_dir / f"{run_ts}.log"

    fmt = "%(asctime)s %(levelname)-8s %(name)s :: %(message)s"
    datefmt = "%Y-%m-%d %H:%M:%S"

    root = logging.getLogger()
    root.setLevel(logging.INFO)
    root.handlers.clear()

    # Colored console handler
    root.addHandler(make_console_handler(fmt, datefmt))

    # Plain file handler
    fh = logging.FileHandler(log_path, encoding="utf-8")
    fh.setFormatter(logging.Formatter(fmt=fmt, datefmt=datefmt))
    root.addHandler(fh)

    return run_ts, log_path

def load_config(path: Path) -> dict:
    """Loads and validates the YAML configuration file."""
    logger = logging.getLogger("config")
    if not path.exists():
        logger.critical(f"Configuration file not found at '{path}'. Exiting.")
        raise SystemExit(1)
    
    with open(path, "r", encoding="utf-8") as f:
        try:
            config = yaml.safe_load(f)
            logger.info(f"Successfully loaded configuration from '{path}'.")
            return config
        except yaml.YAMLError as e:
            logger.critical(f"Error parsing YAML configuration: {e}")
            raise SystemExit(1)

def main():
    """Main script execution."""
    run_ts, log_path = setup_logging()
    logger = logging.getLogger("main")
    logger.info(f"Run initialized at: {run_ts} | Logging to: {log_path}")

    # --- 1. Load Configuration ---
    config = load_config(Path("config/asom.yml"))
    detect_chain = config.get("detect_chain", {})
    attack_chain = config.get("attack_chain", {})
    plan_dirs_config = config.get("analytic_plan_dirs", {})
    plan_dirs_list = [Path(p) for p in plan_dirs_config.values() if p]
    output_dir = Path(config.get("output_dir", "."))
    output_basename = config.get("output_basename", "asom_report")
    column_widths = config.get("column_widths_pixels", {})
    output_columns = config.get("output_columns", [])
    
    output_dir.mkdir(exist_ok=True, parents=True)

    if not plan_dirs_list:
        logger.critical("Configuration key 'analytic_plan_dirs' is empty or not found. No directories to process.")
        return

    if not detect_chain and not attack_chain:
        logger.warning("Both 'detect_chain' and 'attack_chain' are empty in the config. No data to process.")
        logger.info("Script finished.")
        return

    # --- 2. Build Raw ASOM Data ---
    logger.info(f"Building ASOM from {len(plan_dirs_list)} specified analytic plan directories...")
    raw_asom = build_asom(
        detect_chain=detect_chain,
        attack_chain=attack_chain,
        directories=plan_dirs_list
    )
    if not raw_asom:
        logger.warning("ASOM generation resulted in no data. Check chains and input files.")
        logger.info("Script finished.")
        return
    logger.info(f"Successfully built raw ASOM with {len(raw_asom)} information requirements.")

    # --- 3. Format, Renumber, and Filter DataFrame ---
    logger.info("Formatting ASOM data into a DataFrame...")
    formatted_df = format_asom(raw_asom)
    logger.info("Re-sorting and re-numbering DataFrame indices...")
    final_df = renumber_formatted_df(formatted_df)

    # Filter columns based on config, if specified
    if output_columns:
        # Validate that the requested columns exist to prevent errors
        valid_columns = [col for col in output_columns if col in final_df.columns]
        missing_columns = set(output_columns) - set(valid_columns)
        if missing_columns:
            logger.warning(f"The following columns from 'output_columns' in config were not found and will be ignored: {list(missing_columns)}")
        
        if valid_columns:
            logger.info(f"Filtering output to {len(valid_columns)} columns as specified in config.")
            final_df = final_df[valid_columns]
        else:
            logger.warning("'output_columns' in config resulted in an empty list of valid columns. All columns will be exported.")

    logger.info(f"Processing complete. Final DataFrame has {len(final_df)} rows and {len(final_df.columns)} columns.")

    # --- 4. Export to Excel ---
    ts_suffix = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Simple Excel export (without merged cells, for easier data parsing)
    simple_excel_path = output_dir / f"{output_basename}_{ts_suffix}.xlsx"
    logger.info(f"Exporting standard Excel file to: {simple_excel_path}")
    export_simple_excel(final_df, simple_excel_path, column_widths_pixels=column_widths)

    # Hierarchical/Merged Excel export (for presentation)
    hier_excel_path = output_dir / f"{output_basename}_merged_{ts_suffix}.xlsx"
    logger.info(f"Exporting Excel file with merged cells to: {hier_excel_path}")
    export_with_merged_cells(final_df, hier_excel_path, column_widths_pixels=column_widths)
    
    logger.info("Script finished successfully.")

if __name__ == "__main__":
    main()


================================================
FILE: config/asom.yml
================================================
# config/asom.yml
# Configuration for the Analytic Scheme of Maneuver (ASOM) generator.

# --- Input and Output Paths ---
# Directories containing the analytic plan JSON files, categorized by framework.
# The script will search for .json files in each of these directories.
analytic_plan_dirs:
  enterprise: "techniques/attack/enterprise"
  ics: "techniques/attack/ics"
  mobile: "techniques/attack/mobile"
  d3fend: "techniques/d3fend"

# Directory where the generated Excel reports will be saved.
output_dir: "./"

# --- Filename Settings ---
# Basename for the output files. A timestamp will be appended.
output_basename: "asom_test"

# --- Detect Chain Definition ---
# Defines the "Detect" phase of the operation. These are processed first.
# Tactic format: "TACTIC_ID - Tactic Name"
# Technique format: "TECHNIQUE_ID - Technique Name"
detect_chain:
  "D3-D - Detect":
    - "D3-NTA - Network Traffic Analysis"
    - "D3-PM - Platform Monitoring"
  "D3-AI - Asset Inventory":
    - "D3-HCI - Hardware Component Inventory"
    - "D3-NNI - Network Node Inventory"
    - "D3-SWI - Software Inventory"
    - "D3-AVE - Asset Vulnerability Enumeration"
    - "D3-DI - Data Inventory"

# --- Attack Chain Definition ---
# Defines the MITRE ATT&CK phases of the operation. Processed after the detect chain.
attack_chain:
  "TA0001 - Initial Access":
    - "T1133 - External Remote Services"
  "TA0003 - Persistence":
    - "T1078 - Valid Accounts"
    - "T1053 - Scheduled Task/Job"
  "TA0008 - Lateral Movement":
    - "T1021 - Remote Services"
    - "T1570 - Lateral Tool Transfer"
  "TA0011 - Command and Control":
    - "T1071 - Application Layer Protocol"
  "TA0010 - Exfiltration":
    - "T1041 - Exfiltration Over C2 Channel"
    - "T1048 - Exfiltration Over Alternative Protocol"
    - "T1567 - Exfiltration Over Web Service"

# --- Output Columns ---
# Specify the exact columns to include in the final Excel reports.
# The order of columns in this list will be preserved in the output.
# If this list is empty or commented out, all columns will be included.
output_columns:
  - "CCIR Index"
  - "CCIR"
  # - "Tactic ID"
  # - "Tactic Name"
  - "Indicator Index"
  - "Indicator"
  # - "Technique ID"
  # - "Technique Name"
  - "Evidence Index"
  - "Evidence Description"
  - "Data Sources"
  # - "Data Platforms"
  - "NAI"
  - "Action"

# --- Formatting Settings ---
# Set approximate column widths in pixels for the Excel export.
# This is an approximation, as Excel's units are based on character width, not pixels.
# Columns not listed here will be auto-sized.
column_widths_pixels:
  "CCIR Index": 50
  "CCIR": 200
  "Tactic ID": 50
  "Tactic Name": 100
  "Indicator Index": 60
  "Indicator": 150
  "Technique ID": 75
  "Technique Name": 200
  "Evidence Index": 65
  "Evidence Description": 250
  "Data Sources": 200
  "Data Platforms": 150
  "NAI": 100
  "Action": 700


================================================
FILE: config/generator.yml
================================================
# config/generator.yml

# --- Output Settings ---
# Directories where the generated analytic plan JSON files will be saved.
# The script will use the key that matches the technique's matrix
# (e.g., 'enterprise', 'ics', 'mobile') to determine the save location.
output_directories:
  enterprise: "techniques/attack/enterprise"
  ics: "techniques/attack/ics"
  mobile: "techniques/attack/mobile"
  default: "techniques/misc"


# --- ATT&CK Matrix Selection ---
# List of ATT&CK matrices to download and process.
# Valid options: "enterprise", "mobile", "ics"
matrices:
  # - "enterprise"
  - "mobile"
  # - "ics"

# --- Technique Filtering (Optional) ---
# A list of specific technique IDs (e.g., "T1548", "T1078.001") to generate plans for.
# If the list is empty or commented out, the script will generate plans for ALL techniques
# found in the selected matrices.
techniques:
  # - "T0817"
  # - "T0846"
  # - "T0882"

model: "gemini-2.5-pro"


================================================
FILE: src/attack_retriever.py
================================================
import json
import logging
import os
from functools import lru_cache

import requests
from mitreattack.stix20 import MitreAttackData

logger = logging.getLogger(__name__)

MATRIX_URLS = {
    "enterprise": "https://raw.githubusercontent.com/mitre/cti/master/enterprise-attack/enterprise-attack.json",
    "mobile": "https://raw.githubusercontent.com/mitre/cti/master/mobile-attack/mobile-attack.json",
    "ics": "https://raw.githubusercontent.com/mitre/cti/master/ics-attack/ics-attack.json",
}

def _download_matrix(matrix_name: str, filename: str) -> bool:
    """Downloads and saves a MITRE ATT&CK matrix if it doesn't exist locally."""
    if os.path.exists(filename):
        logger.info(f"Using cached version of '{matrix_name}' matrix from '{filename}'.")
        return True

    url = MATRIX_URLS.get(matrix_name)
    if not url:
        logger.error(f"Invalid matrix name '{matrix_name}'. No URL found.")
        return False

    logger.info(f"Downloading '{matrix_name}' matrix from {url}...")
    try:
        response = requests.get(url, timeout=60)
        response.raise_for_status()
        with open(filename, "w", encoding='utf-8') as f:
            json.dump(response.json(), f)
        logger.info(f"Successfully saved '{matrix_name}' matrix to '{filename}'.")
        return True
    except requests.exceptions.RequestException as e:
        logger.error(f"Error downloading ATT&CK data for '{matrix_name}': {e}")
    except (json.JSONDecodeError, IOError) as e:
        logger.error(f"Error processing or saving data for '{matrix_name}': {e}")
    
    return False

@lru_cache(maxsize=None)
def _get_mitre_attack_data(matrix_name: str) -> MitreAttackData | None:
    """Initializes and caches a MitreAttackData object for a given matrix."""
    filename = f"{matrix_name}-attack.json"
    if _download_matrix(matrix_name, filename):
        try:
            return MitreAttackData(filename)
        except Exception as e:
            logger.error(f"Error initializing MitreAttackData from '{filename}': {e}")
    return None

def _extract_attack_id_from_stix(stix_obj):
    """Extracts the MITRE ATT&CK ID (e.g., T1548) from a STIX object."""
    for ref in stix_obj.get("external_references", []):
        if ref.get("source_name") == "mitre-attack":
            return ref.get("external_id")
    return None

def build_technique_dictionary(matrices: list[str]) -> dict:
    """
    Builds a comprehensive dictionary of techniques from the specified ATT&CK matrices.
    """
    technique_dict = {}
    
    for matrix_name in matrices:
        logger.info(f"Processing matrix: {matrix_name}")
        mad = _get_mitre_attack_data(matrix_name)
        if not mad:
            logger.warning(f"Skipping matrix '{matrix_name}' due to download/initialization failure.")
            continue

        tactic_name_to_id = {t.name.lower(): _extract_attack_id_from_stix(t) for t in mad.get_tactics()}
        all_techniques = mad.get_techniques(remove_revoked_deprecated=True) + \
                         mad.get_subtechniques(remove_revoked_deprecated=True)

        for tech in all_techniques:
            tid = _extract_attack_id_from_stix(tech)
            if not tid:
                continue

            name = tech.get("name", "").strip().replace("/", "-")
            full_key = f"{tid} - {name}"
            
            # Avoid overwriting if a technique (e.g., from enterprise) is already present
            if full_key in technique_dict:
                continue

            tactic_names = []
            for phase in tech.get("kill_chain_phases", []):
                # Ensure the phase belongs to a mitre-attack kill chain for the correct matrix
                if phase.get("kill_chain_name") in (f"mitre-{matrix_name}-attack", "mitre-attack"):
                    phase_name_lookup = phase.get("phase_name", "").lower().replace("-", " ")
                    tactic_id = tactic_name_to_id.get(phase_name_lookup)
                    if tactic_id:
                        tactic_display_name = phase_name_lookup.title()
                        tactic_names.append(f"{tactic_id} - {tactic_display_name}")
            
            technique_dict[full_key] = {
                "technique_id": tid,
                "name": name,
                "matrix": matrix_name,
                "tactic": ", ".join(sorted(set(tactic_names))),
                "description": tech.get("description", "").strip(),
                "detection": tech.get("x_mitre_detection", "").strip(),
            }

    logger.info(f"Built dictionary with {len(technique_dict)} unique techniques across {len(matrices)} matrices.")
    return technique_dict


================================================
FILE: src/colorlog.py
================================================
# src/colorlog.py
from __future__ import annotations
import logging
import os
import sys
from typing import Dict, Any, Optional

# Optional: enable ANSI colors on Windows if colorama is present.
try:
    import colorama  # type: ignore
    colorama.just_fix_windows_console()
except Exception:
    pass

ANSI_CODES = {
    "reset": "\x1b[0m",
    "bold": "\x1b[1m",
    "dim": "\x1b[2m",
    "underline": "\x1b[4m",
    "black": "\x1b[30m",
    "red": "\x1b[31m",
    "green": "\x1b[32m",
    "yellow": "\x1b[33m",
    "blue": "\x1b[34m",
    "magenta": "\x1b[35m",
    "cyan": "\x1b[36m",
    "white": "\x1b[37m",
    "bg_black": "\x1b[40m",
    "bg_red": "\x1b[41m",
    "bg_green": "\x1b[42m",
    "bg_yellow": "\x1b[43m",
    "bg_blue": "\x1b[44m",
    "bg_magenta": "\x1b[45m",
    "bg_cyan": "\x1b[46m",
    "bg_white": "\x1b[47m",
}

LEVEL_COLORS = {
    logging.DEBUG:  ("blue", None, False),
    logging.INFO:   ("green", None, False),
    logging.WARNING:("yellow", None, True),
    logging.ERROR:  ("red", None, True),
    logging.CRITICAL:("white", "bg_red", True),
}

def supports_color(stream) -> bool:
    try:
        # Only color when writing to a real TTY
        return hasattr(stream, "isatty") and stream.isatty()
    except Exception:
        return False

def c(text: str, *, fg: Optional[str]=None, bg: Optional[str]=None, bold: bool=False, underline: bool=False, dim: bool=False) -> str:
    """
    Manually colorize a substring. Use in your message like:
      logger.info("hello %s", c("world", fg="cyan", bold=True))
    """
    parts = []
    if bold: parts.append(ANSI_CODES["bold"])
    if dim: parts.append(ANSI_CODES["dim"])
    if underline: parts.append(ANSI_CODES["underline"])
    if fg and fg in ANSI_CODES: parts.append(ANSI_CODES[fg])
    if bg and bg in ANSI_CODES: parts.append(ANSI_CODES[bg])
    start = "".join(parts)
    end = ANSI_CODES["reset"] if parts else ""
    return f"{start}{text}{end}"

class ConsoleColoredFormatter(logging.Formatter):
    """
    Colors console output. File logs should use a plain formatter.

    How to color specific lines:
      logger.info("Message", extra={"msg_color":"cyan","msg_bold":True,"msg_underline":False,"msg_bg":"bg_black"})
    If not provided, it colors level name based on LEVEL_COLORS and leaves message plain.
    """
    def __init__(self, fmt: str, datefmt: str | None = None, *, colorize: bool = True, color_levelnames: bool = True):
        super().__init__(fmt=fmt, datefmt=datefmt)
        self.colorize = colorize
        self.color_levelnames = color_levelnames

    def format(self, record: logging.LogRecord) -> str:
        # Save originals
        orig_levelname = record.levelname
        orig_msg = record.getMessage()

        # Apply colors only if enabled and stream supports it (checked when handler is created)
        if self.colorize:
            # 1) Colorize LEVELNAME
            if self.color_levelnames:
                fg, bg, bold = LEVEL_COLORS.get(record.levelno, ("white", None, False))
                record.levelname = c(orig_levelname, fg=fg, bg=bg, bold=bold)

            # 2) Colorize message if requested via extra
            fg = getattr(record, "msg_color", None)
            bg = getattr(record, "msg_bg", None)
            bold = bool(getattr(record, "msg_bold", False))
            underline = bool(getattr(record, "msg_underline", False))
            dim = bool(getattr(record, "msg_dim", False))

            # If user supplied any styling flag, colorize whole message
            if any([fg, bg, bold, underline, dim]):
                colored_msg = c(orig_msg, fg=fg, bg=bg, bold=bold, underline=underline, dim=dim)
            else:
                # Otherwise leave message as-is; user can still embed c("...") manually.
                colored_msg = orig_msg

            # Inject a temporary attribute for the format string
            record.__dict__["_colored_message"] = colored_msg
            # Use %(message)s normally (we'll swap below)
            out = super().format(record)
            # Reset modified fields
            record.levelname = orig_levelname
            return out.replace(orig_msg, colored_msg, 1)
        else:
            # No color
            record.levelname = orig_levelname
            return super().format(record)

def make_console_handler(fmt: str, datefmt: str) -> logging.Handler:
    stream = sys.stdout
    colorize = supports_color(stream)
    handler = logging.StreamHandler(stream)
    handler.setLevel(logging.INFO)
    handler.setFormatter(ConsoleColoredFormatter(fmt=fmt, datefmt=datefmt, colorize=colorize))
    return handler



================================================
FILE: src/formatting.py
================================================
from pathlib import Path
from typing import Dict, Optional

import pandas as pd
from openpyxl import Workbook
from openpyxl.styles import Alignment, Font
from openpyxl.utils import get_column_letter

def _pixels_to_width(pixels: int) -> float:
    """
    Approximate conversion from pixels to openpyxl's character width unit.
    """
    return pixels / 7.0

def export_simple_excel(df: pd.DataFrame, path: Path, column_widths_pixels: Optional[Dict[str, int]] = None):
    """
    Exports a DataFrame to a standard Excel file with custom formatting.
    - Applies word wrap to all cells.
    - Sets column widths based on pixel approximation or auto-sizing.
    """
    wb = Workbook()
    ws = wb.active
    ws.title = "ASOM"
    cols = list(df.columns)

    # Styles
    header_font = Font(bold=True)
    header_alignment = Alignment(horizontal="center", vertical="center", wrap_text=True)
    cell_alignment = Alignment(vertical="top", wrap_text=True)

    # Write Header
    for c_idx, col_name in enumerate(cols, start=1):
        cell = ws.cell(row=1, column=c_idx, value=col_name)
        cell.font = header_font
        cell.alignment = header_alignment

    # Write Data
    for r_idx, row in enumerate(df.itertuples(index=False), start=2):
        for c_idx, value in enumerate(row, start=1):
            cell = ws.cell(row=r_idx, column=c_idx, value="" if pd.isna(value) else value)
            cell.alignment = cell_alignment

    # Set column widths
    for c_idx, col_name in enumerate(cols, start=1):
        col_letter = get_column_letter(c_idx)
        if column_widths_pixels and col_name in column_widths_pixels:
            ws.column_dimensions[col_letter].width = _pixels_to_width(column_widths_pixels[col_name])
        else:
            max_len = df[col_name].astype(str).map(len).max()
            max_len = max(len(col_name), max_len if pd.notna(max_len) else 0)
            ws.column_dimensions[col_letter].width = min(max_len + 2, 60)

    wb.save(path)


def _compute_hierarchical_spans(df: pd.DataFrame, span_columns):
    spans = {col: [1] * len(df) for col in span_columns}
    parent_ranges = [(0, len(df))]

    for col in span_columns:
        col_spans = [1] * len(df)
        new_parent_ranges = []
        for (start, end) in parent_ranges:
            i = start
            while i < end:
                val = df.iat[i, df.columns.get_loc(col)]
                j = i + 1
                while j < end and df.iat[j, df.columns.get_loc(col)] == val:
                    j += 1
                run_len = j - i
                if run_len > 1:
                    col_spans[i] = run_len
                    for r in range(i + 1, j):
                        col_spans[r] = 0
                new_parent_ranges.append((i, j))
                i = j
        spans[col] = col_spans
        parent_ranges = new_parent_ranges
    return spans

def export_with_merged_cells(df: pd.DataFrame, path: Path, column_widths_pixels: Optional[Dict[str, int]] = None):
    # This list defines which columns are candidates for merging.
    all_possible_span_columns = [
        "CCIR Index", "CCIR", "Tactic ID", "Tactic Name",
        "Indicator Index", "Indicator", "Technique ID", "Technique Name"
    ]
    # Filter the list to only include columns that actually exist in the DataFrame.
    span_columns = [c for c in all_possible_span_columns if c in df.columns]

    df = df.copy()
    spans = _compute_hierarchical_spans(df, span_columns)

    wb = Workbook()
    ws = wb.active
    ws.title = "ASOM"
    cols = list(df.columns)

    # Styles
    header_font = Font(bold=True)
    header_alignment = Alignment(horizontal="center", vertical="center", wrap_text=True)
    # This alignment will be applied to all data cells.
    default_cell_alignment = Alignment(vertical="top", wrap_text=True)

    # Write Header
    for c_idx, col in enumerate(cols, start=1):
        cell = ws.cell(row=1, column=c_idx, value=col)
        cell.font = header_font
        cell.alignment = header_alignment

    # Write Data and apply default alignment
    for r_idx, (_, row) in enumerate(df.iterrows(), start=2):
        for c_idx, col in enumerate(cols, start=1):
            cell = ws.cell(row=r_idx, column=c_idx, value="" if pd.isna(row[col]) else row[col])
            cell.alignment = default_cell_alignment

    # Apply Merges (the alignment is already set, so we just merge)
    for col in span_columns:
        c_idx = cols.index(col) + 1
        r = 0
        while r < len(df):
            span_len = spans[col][r]
            excel_row_start = r + 2
            if span_len > 1:
                ws.merge_cells(
                    start_row=excel_row_start,
                    start_column=c_idx,
                    end_row=excel_row_start + span_len - 1,
                    end_column=c_idx
                )
            r += max(span_len, 1)
    
    # Set column widths
    for c_idx, col_name in enumerate(cols, start=1):
        col_letter = get_column_letter(c_idx)
        if column_widths_pixels and col_name in column_widths_pixels:
            ws.column_dimensions[col_letter].width = _pixels_to_width(column_widths_pixels[col_name])
        else:
            max_len = df[col_name].astype(str).map(len).max()
            max_len = max(len(col_name), max_len if pd.notna(max_len) else 0)
            ws.column_dimensions[col_letter].width = min(max_len + 2, 60)

    wb.save(path)


================================================
FILE: src/processing.py
================================================
import copy
import json
import logging
import re
from pathlib import Path
from typing import Any, Dict, List, Set, Tuple

import pandas as pd

logger = logging.getLogger(__name__)

TECHNIQUE_ID_PATTERN = re.compile(r"^(?P<tech_id>T\d{4}(?:\.\d{3})?|D3-[A-Z]+)")

def _normalize_tactic_key(tactic: str) -> Tuple[str, str]:
    if " - " in tactic:
        tid, name = tactic.split(" - ", 1)
        return tid.strip(), name.strip()
    return tactic.strip(), ""

def _normalize_technique_id(tech: str) -> str:
    m = TECHNIQUE_ID_PATTERN.match(tech.strip())
    return m.group("tech_id") if m else ""

def _load_json_safely(path: Path) -> Any:
    text = path.read_text(encoding="utf-8").strip()
    if text.startswith("```"):
        text = re.sub(r"^```(?:json)?\s*", "", text)
        text = re.sub(r"\s*```$", "", text)
    return json.loads(text)

def _is_new_schema_object(obj: dict) -> bool:
    required = {"information_requirement", "tactic_id", "tactic_name", "indicators"}
    return isinstance(obj, dict) and required.issubset(obj.keys())

def _filter_indicators(ir_obj: dict, allowed_ids: Set[str]) -> dict:
    if not allowed_ids:
        return ir_obj

    new_obj = copy.deepcopy(ir_obj)
    new_indicators = [
        ind for ind in new_obj.get("indicators", [])
        if _normalize_technique_id(ind.get("technique_id", "")) in allowed_ids
    ]
    new_obj["indicators"] = new_indicators
    return new_obj

def build_asom(
    detect_chain: Dict[str, List[str]],
    attack_chain: Dict[str, List[str]],
    directories: List[Path],
    filter_indicators: bool = True,
    deduplicate: bool = True
) -> List[dict]:
    """
    Builds an ASOM by processing detect and attack chains against analytic plan files.

    The function processes all JSON files in the given list of directories, filters
    them based on the combined tactics and techniques from both chains, and then sorts
    the results to ensure that items from the 'detect_chain' appear first.
    """
    # Combine chains and create a master map of all tactics and techniques
    full_chain = {**detect_chain, **attack_chain}
    tactic_map: Dict[str, Set[str]] = {}
    for tactic_str, technique_list in full_chain.items():
        tactic_id, _ = _normalize_tactic_key(tactic_str)
        norm_tecs = {_normalize_technique_id(t) for t in technique_list}
        norm_tecs = {t for t in norm_tecs if t}
        tactic_map.setdefault(tactic_id, set()).update(norm_tecs)

    # This list preserves the order of detect tactics for final sorting
    detect_tactic_ids_ordered = [_normalize_tactic_key(t)[0] for t in detect_chain.keys()]

    # Gather all .json files from all specified directories
    all_files: List[Path] = []
    for directory in directories:
        if directory.is_dir():
            all_files.extend(directory.glob("*.json"))
        else:
            logger.warning(f"Directory '{directory}' specified in config does not exist. Skipping.")

    # Process all JSON files and collect all matching IR objects
    all_results: List[dict] = []
    for path in sorted(all_files):
        try:
            data = _load_json_safely(path)
        except Exception as e:
            logger.warning(f"Could not parse {path.name}: {e}")
            continue

        if not isinstance(data, list):
            continue

        for obj in data:
            if not _is_new_schema_object(obj):
                logger.debug(f"Object in {path.name} does not conform to schema. Skipping.")
                continue

            tactic_id = obj.get("tactic_id", "").strip()
            if tactic_id not in tactic_map:
                continue

            if filter_indicators:
                filtered_obj = _filter_indicators(copy.deepcopy(obj), tactic_map[tactic_id])
                if tactic_map[tactic_id] and not filtered_obj.get("indicators"):
                    continue
                all_results.append(filtered_obj)
            else:
                all_results.append(copy.deepcopy(obj))

    # Sort results: detect chain tactics first (in order), then attack chain tactics
    def sort_key(ir_obj):
        tactic_id = ir_obj.get("tactic_id")
        if tactic_id in detect_tactic_ids_ordered:
            # Primary sort key 0 for detect, secondary is its position in the config
            return (0, detect_tactic_ids_ordered.index(tactic_id))
        else:
            # Primary sort key 1 for attack, secondary is alphabetical by ID
            return (1, tactic_id)

    all_results.sort(key=sort_key)
    
    # Deduplicate after sorting to ensure the first-occurring item is kept
    if deduplicate:
        seen = set()
        unique_results: List[dict] = []
        for obj in all_results:
            key = (obj.get("information_requirement"), obj.get("tactic_id"))
            if key in seen:
                continue
            seen.add(key)
            unique_results.append(obj)
        return unique_results
    
    return all_results


def format_asom(asom_input_list: List[dict], joiner: str = "; ") -> pd.DataFrame:
    table_rows = []
    ir_index = 0

    if not isinstance(asom_input_list, list):
        raise TypeError("Expected asom_input_list to be a list.")

    for ir_obj in asom_input_list:
        if not isinstance(ir_obj, dict):
            logger.warning(f"Skipping non-dict IR object: {ir_obj}")
            continue

        required_ir_keys = {"information_requirement", "tactic_id", "tactic_name", "indicators"}
        if not required_ir_keys.issubset(ir_obj.keys()):
            logger.warning(f"IR object missing required keys: {ir_obj.keys()}")
            continue

        indicators = ir_obj.get("indicators", [])
        if not indicators:
            logger.info(f"IR '{ir_obj.get('information_requirement')}' has no indicators; skipping.")
            continue

        ir_index += 1
        tactic_id = ir_obj.get("tactic_id", "")
        tactic_name = ir_obj.get("tactic_name", "")
        ir_text = ir_obj.get("information_requirement", "")
        information_requirement = f"{ir_text} ({tactic_id} - {tactic_name})"

        tech_sub_index = 0
        for indicator in indicators:
            technique_id = indicator.get("technique_id", "")
            technique_name = indicator.get("name", "")
            evidence_list = indicator.get("evidence", [])
            tech_sub_index += 1
            indicator_index_str = f"{ir_index}.{tech_sub_index}"

            if not evidence_list:
                logger.info(f"Indicator '{technique_id} - {technique_name}' has no evidence entries.")
                continue

            evidence_sub_index = 0
            for evidence in evidence_list:
                evidence_sub_index += 1
                evidence_index_str = f"{indicator_index_str}.{evidence_sub_index}"

                row = {
                    "CCIR Index": ir_index,
                    "CCIR": information_requirement,
                    "Tactic ID": tactic_id,
                    "Tactic Name": tactic_name,
                    "Indicator Index": indicator_index_str,
                    "Indicator": f"{technique_id} - {technique_name}",
                    "Technique ID": technique_id,
                    "Technique Name": technique_name,
                    "Evidence Index": evidence_index_str,
                    "Evidence Description": evidence.get("description", ""),
                    "Data Sources": joiner.join(evidence.get("data_sources", [])),
                    "Data Platforms": joiner.join(evidence.get("data_platforms", [])),
                    "NAI": evidence.get("nai", ""),
                    "Action": evidence.get("action", ""),
                }
                table_rows.append(row)

    df = pd.DataFrame(table_rows)
    column_order = [
        "CCIR Index", "CCIR", "Tactic ID", "Tactic Name", "Indicator Index",
        "Indicator", "Technique ID", "Technique Name", "Evidence Index",
        "Evidence Description", "Data Sources", "Data Platforms", "NAI", "Action"
    ]
    for col in column_order:
        if col not in df.columns:
            df[col] = pd.NA
    
    if not df.empty:
        df = df.sort_values(by=["CCIR Index", "Indicator Index", "Evidence Index"], ignore_index=True)

    return df[column_order]

def renumber_formatted_df(formatted_df: pd.DataFrame, d3_tactic_id: str = "D3-D") -> pd.DataFrame:
    if formatted_df.empty:
        return formatted_df
        
    df = formatted_df.copy()

    df["_tactic_primary_key"] = (df["Tactic ID"] != d3_tactic_id).astype(int)
    df["_orig_row"] = range(len(df))
    df["_CCIR_norm"] = df["CCIR"].apply(lambda s: re.sub(r"\s+", " ", s).strip())

    df.sort_values(
        by=["_tactic_primary_key", "Tactic ID", "_CCIR_norm", "Indicator", "Evidence Description", "_orig_row"],
        kind="stable",
        inplace=True
    )

    ccir_index_map = {}
    next_ccir_idx = 0
    new_ccir_indices = []
    for ccir_norm in df["_CCIR_norm"]:
        if ccir_norm not in ccir_index_map:
            next_ccir_idx += 1
            ccir_index_map[ccir_norm] = next_ccir_idx
        new_ccir_indices.append(ccir_index_map[ccir_norm])
    df["CCIR Index"] = new_ccir_indices

    indicator_index_map = {}
    ccir_indicator_counters = {}
    new_indicator_indices = []
    for ccir_norm, ccir_idx, indicator in zip(df["_CCIR_norm"], df["CCIR Index"], df["Indicator"]):
        key = (ccir_norm, indicator)
        if key not in indicator_index_map:
            ccir_indicator_counters.setdefault(ccir_norm, 0)
            ccir_indicator_counters[ccir_norm] += 1
            indicator_index_map[key] = ccir_indicator_counters[ccir_norm]
        sub_idx = indicator_index_map[key]
        new_indicator_indices.append(f"{ccir_idx}.{sub_idx}")
    df["Indicator Index"] = new_indicator_indices
    
    indicator_sub_lookup = {key: sub for key, sub in indicator_index_map.items()}

    evidence_index_map = {}
    evidence_counters = {}
    new_evidence_indices = []
    for ccir_norm, indicator, evidence_desc in zip(df["_CCIR_norm"], df["Indicator"], df["Evidence Description"]):
        parent_key = (ccir_norm, indicator)
        evidence_key = (ccir_norm, indicator, evidence_desc)
        if evidence_key not in evidence_index_map:
            evidence_counters.setdefault(parent_key, 0)
            evidence_counters[parent_key] += 1
            evidence_index_map[evidence_key] = evidence_counters[parent_key]
        
        ccir_idx = ccir_index_map[ccir_norm]
        indicator_sub_idx = indicator_sub_lookup[parent_key]
        evidence_sub_idx = evidence_index_map[evidence_key]
        new_evidence_indices.append(f"{ccir_idx}.{indicator_sub_idx}.{evidence_sub_idx}")
    df["Evidence Index"] = new_evidence_indices

    df.sort_values(by=["CCIR Index", "Indicator Index", "Evidence Index"], kind="stable", inplace=True)
    df.drop(columns=["_tactic_primary_key", "_orig_row", "_CCIR_norm"], inplace=True)
    df.reset_index(drop=True, inplace=True)

    return df

