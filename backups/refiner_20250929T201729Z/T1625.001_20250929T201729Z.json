[
  {
    "information_requirement": "Is an adversary maintaining persistence on mobile devices by hijacking system runtime APIs? (PIR)",
    "tactic_id": "TA0028",
    "tactic_name": "Persistence",
    "indicators": [
      {
        "technique_id": "T1625.001",
        "name": "System Runtime API Hijacking",
        "evidence": [
          {
            "description": "An MDM/MTD log event contains a 'compliance_status' field with the value 'failed' and a 'reason' field indicating 'File Integrity Mismatch' or 'OS Tampering'. The event must also contain the device ID, the path of the modified file (e.g., '/system/lib/libart.so'), and the measured file hash that deviates from the expected baseline hash.",
            "data_sources": [
              "MDM event logs",
              "MTD alert logs"
            ],
            "data_platforms": [
              "TBD"
            ],
            "nai": "The MDM/MTD management platform and the population of enrolled Android devices.",
            "action": "1. (Symbolic) Ingest MDM/MTD logs into a central SIEM. Create a detection rule that triggers a high-severity alert for any event where 'compliance_status' is 'failed' and 'reason' contains 'integrity' or 'tampering'. If the event provides a file hash, automatically query a threat intelligence platform (TIP) API to check if the hash is associated with known malware. 2. (Statistical) For all devices, establish a 30-day rolling baseline of the hourly attestation failure rate, segmented by device model and OS version. Calculate the mean and standard deviation for each hour of the day. Trigger an alert if the failure count for any segment in a given hour exceeds 3 standard deviations above its historical mean for that hour, suggesting a targeted or widespread issue. 3. (Machine Learning) Train a time-series forecasting model (e.g., SARIMA) on the historical count of fleet-wide attestation failures per hour. In real-time, compare the observed failure count to the model's prediction. If the observed count exceeds the upper bound of the model's 99% confidence interval for a sustained period (e.g., two consecutive hours), generate an alert for an anomalous trend potentially indicating a new campaign."
          },
          {
            "description": "A series of network connections from a single mobile device IP address exhibits multiple anomalies simultaneously: 1) The destination IP or domain has a reputation score below a defined threshold (e.g., < 20/100) in threat intelligence feeds. 2) Connection intervals to a single destination have a standard deviation near zero (e.g., < 1 second) over a 5-minute window, indicating automated beaconing. 3) The Shannon entropy of requested DNS domains is high (e.g., > 3.5), suggesting DGA. 4) The TLS client fingerprint (JA3) is rare, appearing in less than 1% of connections across the mobile device fleet.",
            "data_sources": [
              "Zeek conn.log",
              "Zeek dns.log",
              "Zeek ssl.log"
            ],
            "data_platforms": [
              "TBD"
            ],
            "nai": "Network sensors at key traffic chokepoints, such as wireless LAN controllers, VPN gateways, and internet egress firewalls.",
            "action": "1. (Symbolic) Ingest Zeek logs. Create a real-time rule to check the 'id.resp_h' field from conn.log and the 'query' field from dns.log against a threat intelligence feed of known malicious IPs and domains. Generate a high-severity alert on any match originating from a mobile device IP range. 2. (Statistical) For each mobile device IP ('id.orig_h'), perform two calculations over a 15-minute window: first, calculate the standard deviation of timestamps for connections to each unique destination ('id.resp_h'); second, calculate the Shannon entropy for all unique DNS queries ('query'). Create an alert if a device exhibits a connection with a time standard deviation below the 5th percentile (indicating beaconing) and a DNS query entropy above the 95th percentile (indicating DGA) when compared to the fleet-wide baseline. 3. (Machine Learning) For each connection, create a feature vector including protocol, destination port, connection duration, bytes transferred, and a one-hot encoded representation of the JA3 hash. Train a DBSCAN clustering model on a 30-day baseline of network traffic to identify normal clusters. In production, classify new connections; any connection labeled as noise (label -1) is considered an outlier. Escalate a device for investigation if it generates more than 10 outlier connections within a 5-minute window, a threshold derived from the 99th percentile of the baseline outlier rate."
          }
        ]
      }
    ],
    "version": "1.2",
    "date_created": "2025-09-26",
    "last_updated": "2025-09-29",
    "contributors": [
      "Zachary Szewczyk",
      "Ask Sage"
    ]
  }
]