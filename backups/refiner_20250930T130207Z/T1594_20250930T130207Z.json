[
  {
    "information_requirement": "Is the adversary conducting reconnaissance by searching our public-facing websites?",
    "tactic_id": "TA0043",
    "tactic_name": "Reconnaissance",
    "indicators": [
      {
        "technique_id": "T1594",
        "name": "Search Victim-Owned Websites",
        "evidence": [
          {
            "description": "One or more HTTP/S sessions to public-facing web servers originate from an IP address or contain a User-Agent string present on a threat intelligence feed of known malicious indicators.",
            "data_sources": [
              "Zeek http.log",
              "Zeek conn.log"
            ],
            "data_platforms": [
              "TBD"
            ],
            "nai": "Public-facing web servers, Network perimeter firewalls/proxies, Web Application Firewalls (WAFs)",
            "action": "1. (Symbolic) Ingest threat intelligence feeds (e.g., known malicious IPs, TOR exit nodes, suspicious User-Agents) into the SIEM. Create a rule that cross-references the source IP from Zeek conn.log and the User-Agent from Zeek http.log against this feed. Generate a high-severity alert on any match. 2. (Statistical) Calculate the historical daily volume of connections from IP addresses associated with anonymizing services (e.g., commercial VPNs, proxies). Establish a baseline and alert when the volume of sessions from these sources exceeds the 95th percentile of the daily average. 3. (Machine Learning) Develop a logistic regression classifier trained on labeled historical data (IP address, User-Agent, ASN, geolocation, time of day, referer) to predict the probability of a session being part of a malicious reconnaissance scan. Trigger an alert for sessions with a probability score above a predetermined threshold (e.g., 0.85) and feed confirmed incidents back into the model for retraining."
          },
          {
            "description": "A client sends one or more HTTP requests to a public web server targeting URIs, file types, or query parameters indicative of reconnaissance scanning, or uses a User-Agent string associated with automated scanning tools.",
            "data_sources": [
              "Zeek http.log"
            ],
            "data_platforms": [
              "TBD"
            ],
            "nai": "Public-facing web servers, Content Delivery Network (CDN) edge nodes",
            "action": "1. (Symbolic) Maintain a regex-based list of suspicious URI patterns (e.g., /.git/, /wp-admin/, /.env), file extensions (.sql, .bak, .config), and User-Agents (sqlmap, nikto, dirbuster). Create a rule in the SIEM to scan Zeek http.log for requests matching these patterns and alert on any hits. 2. (Statistical) For each source IP, calculate the entropy of requested file extensions over a 5-minute window. A high entropy score, exceeding the 90th percentile of the baseline, suggests probing for various file types. Additionally, monitor the rate of HTTP 404 responses per source IP; a rate exceeding 3 standard deviations above the mean indicates likely directory/file enumeration. 3. (Machine Learning) Use a time-series forecasting model (e.g., ARIMA) to predict the expected request volume for sensitive but public files like robots.txt and sitemap.xml. Generate an alert when the observed request volume significantly deviates from the forecasted range, indicating a potential coordinated scanning campaign."
          },
          {
            "description": "A single external source IP generates a volume of requests, rate of requests, or diversity of unique URIs accessed that is statistically anomalous compared to a baseline of typical user behavior.",
            "data_sources": [
              "Zeek http.log",
              "Zeek conn.log"
            ],
            "data_platforms": [
              "TBD"
            ],
            "nai": "Public-facing web servers, Load balancers, Web Application Firewalls (WAFs)",
            "action": "1. (Symbolic) Whitelist the User-Agent strings of known, legitimate web crawlers (e.g., Googlebot, Bingbot). Create a rule to flag any source IP that exhibits a high request rate (e.g., >100 requests/minute) but does not use a whitelisted User-Agent. 2. (Statistical) For each source IP, compute the request rate (requests/minute) and the count of unique URIs accessed over a 10-minute sliding window. Establish a baseline for these metrics across all users. Generate an alert if an IP's metrics exceed the 98th percentile of the established baseline. 3. (Machine Learning) Employ a clustering algorithm (e.g., DBSCAN) on web session features (session duration, number of requests, average time between requests, diversity of URIs). This will group sessions into clusters representing 'normal users,' 'benign bots,' and 'anomalous scanners.' Sessions falling into the 'anomalous scanners' cluster, or those identified as noise by the algorithm, trigger an alert for investigation."
          },
          {
            "description": "An external source IP systematically accesses a high number of unique pages within predefined sensitive sections of the website (e.g., employee directories, investor relations) in a manner inconsistent with normal user browsing.",
            "data_sources": [
              "Zeek http.log"
            ],
            "data_platforms": [
              "TBD"
            ],
            "nai": "Public-facing web servers hosting corporate, HR, or investor relations content",
            "action": "1. (Symbolic) Define a set of URL patterns for sensitive sections (e.g., /about/team/*, /investors/financials/*). Create a rule that alerts if a single source IP accesses more than 15 unique URLs matching these patterns within a 5-minute window. 2. (Statistical) For each source IP, calculate the ratio of sensitive pages visited to total pages visited in a session. Establish a baseline for this ratio across all user sessions. Generate an alert when an IP's ratio exceeds the 95th percentile, indicating an unusual focus on reconnaissance-rich content. 3. (Machine Learning) Model user navigation paths as a Markov chain, where states are page categories (e.g., 'Home', 'Product', 'Sensitive/Team', 'Contact'). Calculate the transition probabilities between states from baseline traffic. Alert on any user session whose path has a very low probability under the trained model, suggesting scripted enumeration of sensitive pages."
          },
          {
            "description": "HTTP requests to deep-linked or sensitive pages are observed with missing, malformed, or unexpected external referer headers, indicating access that bypasses the intended site navigation flow.",
            "data_sources": [
              "Zeek http.log"
            ],
            "data_platforms": [
              "TBD"
            ],
            "nai": "Public-facing web servers, Application servers",
            "action": "1. (Symbolic) Create a rule to alert on any request to a page matching a 'deep/sensitive' pattern (e.g., /admin/config.php, /private/*) that has a Referer header from a different top-level domain or is entirely blank, excluding known valid exceptions. 2. (Statistical) For each sensitive page on the website, establish a baseline of its top 10 most common referrers. Calculate the percentage of requests to that page that come from 'other' (non-top-10) or null referrers. Alert if this percentage, over a 1-hour window, exceeds 3 standard deviations above the mean. 3. (Machine Learning) Train a sequence-to-sequence autoencoder on normal user navigation paths (sequences of URIs visited in a session). During inference, a user's session path is fed into the autoencoder. A high reconstruction error indicates the navigation sequence is anomalous and does not conform to learned patterns, triggering an alert for review."
          }
        ]
      }
    ],
    "last_updated": "2025-09-29",
    "version": "2.2",
    "date_created": "2025-05-04",
    "contributors": [
      "Zachary Szewczyk"
    ]
  }
]