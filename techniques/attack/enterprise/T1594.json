[
  {
    "information_requirement": "Is the adversary conducting reconnaissance by searching our public-facing websites?",
    "tactic_id": "TA0043",
    "tactic_name": "Reconnaissance",
    "indicators": [
      {
        "technique_id": "T1594",
        "name": "Search Victim-Owned Websites",
        "evidence": [
          {
            "description": "An HTTP/S session to a public-facing web server originates from a source IP address or contains a User-Agent string matching an entry on a threat intelligence feed of known malicious indicators (e.g., TOR exit nodes, known scanning infrastructure, malicious user agents).",
            "data_sources": [
              "Zeek http.log",
              "Zeek conn.log"
            ],
            "data_platforms": [
              "TBD"
            ],
            "nai": "Public-facing web servers, Network perimeter firewalls/proxies, Web Application Firewalls (WAFs), Cloud-hosted web applications, Threat Intelligence Platforms",
            "action": [
              "1. (Symbolic) Ingest threat intelligence feeds (e.g., known malicious IPs, TOR exit nodes, suspicious User-Agents) into the SIEM. Create a rule that cross-references the `id.orig_h` field from Zeek conn.log and the `user_agent` field from Zeek http.log against the ingested intelligence feeds. Generate a high-severity alert on any match.",
              "2. (Statistical) Calculate the historical daily volume of connections from IP addresses associated with anonymizing services (e.g., commercial VPNs, proxies) using ASN and geolocation data. Establish a baseline mean and standard deviation for this volume. Generate an alert when the current daily volume exceeds three standard deviations from the mean or surpasses the 95th percentile of the historical distribution.",
              "3. (Machine Learning) Develop and train a logistic regression classifier on labeled historical data, using features such as source IP, User-Agent, ASN, geolocation, time of day, and referer to predict the probability of a session being part of a malicious reconnaissance scan. Trigger an alert for sessions with a probability score above a predetermined threshold (e.g., 0.85). Periodically retrain the model with newly confirmed malicious and benign session data to maintain accuracy."
            ]
          },
          {
            "description": "A single source IP sends HTTP requests to a public web server targeting URIs (e.g., /.git/, /wp-admin/, /.env), sensitive file types (e.g., .sql, .bak, .config), or uses a User-Agent string associated with automated scanning tools (e.g., sqlmap, nikto, dirbuster).",
            "data_sources": [
              "Zeek http.log"
            ],
            "data_platforms": [
              "TBD"
            ],
            "nai": "Public-facing web servers, Content Delivery Network (CDN) edge nodes, Web Application Firewalls (WAFs)",
            "action": [
              "1. (Symbolic) Create and maintain a list of regular expressions matching suspicious URI patterns, sensitive file extensions, and known scanner User-Agent strings. Configure a SIEM rule to scan the `uri`, `filename`, and `user_agent` fields in Zeek http.log for matches. Trigger an alert on any hit, noting the specific pattern matched.",
              "2. (Statistical) For each source IP over a 5-minute window, calculate the Shannon entropy of the requested file extensions found in the `uri` field. Establish a baseline entropy distribution for typical user traffic. Alert if a source IP's entropy score exceeds the 90th percentile of the baseline, indicating probing for diverse file types. Concurrently, monitor the rate of HTTP 404 responses per source IP; a rate exceeding 3 standard deviations above the mean for all IPs indicates likely directory or file enumeration.",
              "3. (Machine Learning) Implement a time-series forecasting model (e.g., ARIMA or Prophet) to predict the expected request volume for sensitive but public files like robots.txt and sitemap.xml on an hourly basis. Generate an alert when the observed request volume significantly deviates from the forecasted confidence interval, which could indicate a coordinated scanning campaign."
            ]
          },
          {
            "description": "A single external source IP generates a volume of requests, rate of requests, or diversity of unique URIs accessed that is statistically anomalous compared to a baseline of typical user behavior.",
            "data_sources": [
              "Zeek http.log",
              "Zeek conn.log"
            ],
            "data_platforms": [
              "TBD"
            ],
            "nai": "Public-facing web servers, Load balancers, Web Application Firewalls (WAFs), Network Intrusion Detection Systems (NIDS)",
            "action": [
              "1. (Symbolic) Maintain a whitelist of User-Agent strings for known, legitimate web crawlers (e.g., Googlebot, Bingbot). Create a rule to flag any source IP (`id.orig_h`) that generates a high request rate (e.g., >10 requests per second) and does not have a User-Agent string matching the whitelist. This helps filter out benign scanning activity.",
              "2. (Statistical) For each source IP, compute the request rate (requests/minute) and the count of unique URIs accessed over a 10-minute sliding window. Establish a baseline for these metrics across all users during a representative period. Generate an alert if an IP's metrics for both request rate and unique URI count simultaneously exceed the 98th percentile of their respective baselines.",
              "3. (Machine Learning) Apply a density-based clustering algorithm (e.g., DBSCAN) to web session data. Use features such as session duration from conn.log (`duration`), number of requests, average time between requests, and diversity of URIs from http.log. This will group sessions into clusters representing 'normal users,' 'benign bots,' and 'anomalous scanners.' Sessions identified as noise or falling into a pre-identified 'anomalous' cluster should trigger an alert for investigation."
            ]
          },
          {
            "description": "A single external source IP accesses an anomalously high number of unique pages within predefined sensitive sections of the website (e.g., /about/team/, /investors/, /careers/) in a short time frame, suggesting enumeration of personnel or business data.",
            "data_sources": [
              "Zeek http.log"
            ],
            "data_platforms": [
              "TBD"
            ],
            "nai": "Public-facing web servers hosting corporate, HR, or investor relations content, Content Management System (CMS) logs",
            "action": [
              "1. (Symbolic) Define a list of URL path patterns corresponding to sensitive content sections (e.g., /about/team/*, /investors/financials/*). Create a SIEM rule that triggers an alert if a single source IP (`id.orig_h`) accesses more than 20 unique URLs matching these patterns within a 5-minute window.",
              "2. (Statistical) For each user session, calculate the ratio of sensitive pages visited to total pages visited. Establish a baseline distribution for this ratio across all historical user sessions. Generate an alert when a session's ratio exceeds the 95th percentile, indicating an unusual focus on reconnaissance-rich content compared to normal browsing behavior.",
              "3. (Machine Learning) Model user navigation paths as a Markov chain, where states are page categories (e.g., 'Home', 'Product', 'Sensitive-Team', 'Contact'). Calculate the transition probabilities between states from baseline traffic. Alert on any user session whose navigation path has a very low probability under the trained model, suggesting scripted or non-human enumeration of sensitive pages."
            ]
          },
          {
            "description": "HTTP requests to sensitive or deep-linked pages (i.e., not the homepage) are observed with a null, malformed, or non-organizational Referer header, suggesting direct access that bypasses the intended website navigation.",
            "data_sources": [
              "Zeek http.log"
            ],
            "data_platforms": [
              "TBD"
            ],
            "nai": "Public-facing web servers, Application servers, Web Application Firewalls (WAFs)",
            "action": [
              "1. (Symbolic) Define a list of sensitive or deep-linked URI patterns (e.g., /admin/config.php, /private/*). Create a rule to alert on any request to a matching URI where the `referer` field in Zeek http.log is either null or does not contain the organization's domain name, while whitelisting known legitimate external referrers (e.g., search engines).",
              "2. (Statistical) For each sensitive page on the website, establish a baseline of its top 10 most common referrers. Over a 1-hour window, calculate the percentage of requests to that page that originate from null referrers or referrers not in the top-10 list. Alert if this percentage exceeds 3 standard deviations above its historical mean, indicating a potential shift in access patterns.",
              "3. (Machine Learning) Train a sequence-to-sequence autoencoder on normal user navigation paths (sequences of URIs visited in a session). Feed a user's session path into the autoencoder during inference. A high reconstruction error indicates the navigation sequence is anomalous and does not conform to learned patterns (e.g., jumping directly to a deep page without preceding navigation). Trigger an alert on sessions with a reconstruction error above a dynamically tuned threshold."
            ]
          }
        ]
      }
    ],
    "last_updated": "2025-09-30",
    "version": "2.3",
    "date_created": "2025-05-04",
    "contributors": [
      "Zachary Szewczyk"
    ]
  }
]