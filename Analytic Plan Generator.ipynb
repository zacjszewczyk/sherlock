{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bMTjh8St9lq"
   },
   "source": [
    "# Analytic Plan Generator\n",
    "\n",
    "This notebook uses artificial intelligence to generate analytic plans.\n",
    "\n",
    "## Background\n",
    "\n",
    "As described in TC 3-12.2.4.1, \"The Analytic Scheme of Maneuver is the plan to collect and analyze technical data to meet specific information requirements. It identifies what data to analyze, how to analyze it, and why it is being analyzed.\" The analytic scheme of maneuver, or ASOM, consists of the following components:\n",
    "\n",
    "* **Priority information requirement**: \n",
    "* **Indicator**: \n",
    "* **Evidence**: \n",
    "* **Data**: \n",
    "* **NAI**: \n",
    "* **Analytic**:\n",
    "\n",
    "\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "This section sets up the environment. It installs packages necessary to generate the analytic plans, imports modules, initializes helper functions, and finally defines global variables. This section also mounts Google Drive to the runtime and moves into the project folder.\n",
    "\n",
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oEPZ2nM0t9ls",
    "outputId": "92a5351f-67b0-4d9e-953c-28f859af0048",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U -q \"google\"\n",
    "!pip install -U -q \"google.genai\"\n",
    "!pip install mitreattack-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6lw6d8_usYi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "from google.colab import drive\n",
    "import requests\n",
    "from mitreattack.stix20 import MitreAttackData\n",
    "import json\n",
    "import base64\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCIRqedzUoF7"
   },
   "outputs": [],
   "source": [
    "def log(message, end=\"\\n\", flush = True):\n",
    "    \"\"\"\n",
    "    Logs a message to the console, prepended with the current timestamp\n",
    "    in ISO 8601 format.\n",
    "    \n",
    "    Args:\n",
    "    message (str): The string message to log.\n",
    "    \"\"\"\n",
    "    global verbose\n",
    "    \n",
    "    # Get the current date and time\n",
    "    current_time = datetime.datetime.now()\n",
    "    \n",
    "    # Format the timestamp in ISO 8601 format\n",
    "    timestamp = current_time.isoformat()\n",
    "    \n",
    "    # Construct the final log string using an f-string for clean formatting\n",
    "    log_string = f\"[{timestamp}] {message}\"\n",
    "    \n",
    "    # Print the log string to the console if logging is turned on (verbose = True)\n",
    "    if (verbose == True):\n",
    "        print(log_string, end = end, flush = flush)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attack_id_from_stix(stix_obj):\n",
    "    \"\"\"Extracts the MITRE ATT&CK ID (e.g., T1548, TA0002) from a STIX object.\n",
    "\n",
    "    Iterates through the 'external_references' list of a STIX object (like a technique,\n",
    "    tactic, or software object) and returns the 'external_id' associated with the\n",
    "    'mitre-attack' source name.\n",
    "\n",
    "    Args:\n",
    "        stix_obj (dict): A dictionary representing a STIX object, expected to have\n",
    "                         an 'external_references' key.\n",
    "\n",
    "    Returns:\n",
    "        str | None: The MITRE ATT&CK external ID if found, otherwise None.\n",
    "    \"\"\"\n",
    "    # Ensure 'external_references' exists and is iterable; default to empty list if not found.\n",
    "    for ref in stix_obj.get(\"external_references\", []):\n",
    "        # Check if the reference source is specifically 'mitre-attack'.\n",
    "        if ref.get(\"source_name\") == \"mitre-attack\":\n",
    "            # Return the associated external ID.\n",
    "            return ref.get(\"external_id\")\n",
    "    # Return None if no 'mitre-attack' external reference was found.\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle logging on (verbose = True)/off (verbose = False)\n",
    "verbose = True\n",
    "# verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-74quC1cM8Mq"
   },
   "outputs": [],
   "source": [
    "# Rate limits: https://ai.google.dev/gemini-api/docs/rate-limits\n",
    "# Pricing: https://ai.google.dev/gemini-api/docs/pricing\n",
    "# Usage: https://console.cloud.google.com/apis/api/generativelanguage.googleapis.com/metrics?project=gen-lang-client-0497172401\n",
    "# Note that this notebook is designed to be run in Google Colab. The line below reads the Gemini API key for AI Studio,\n",
    "# which is configured in the Secrets tab on the left side of the Colab window.\n",
    "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
    "log(\"Gemii API key loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QbeB5obluk-_",
    "outputId": "73c2ac5b-204f-4c1b-915d-f830df42df6a"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive and move into the Google AI Studio folder\n",
    "DRIVE_PATH = \"/content/drive\"\n",
    "TECHNIQUES_PATH = \"/content/drive/MyDrive/Google AI Studio/techniques\"\n",
    "\n",
    "drive.mount(DRIVE_PATH)\n",
    "log(f\"Google Drive mounted to {DRIVE_PATH}\")\n",
    "\n",
    "os.chdir(TECHNIQUES_PATH)\n",
    "log(f\"Changed directory to {TECHNIQUES_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQokR-C_5Qng"
   },
   "source": [
    "## ATT&CK Matrix Retrieval\n",
    "\n",
    "This section first retrieves the latest MITRE ATT&CK Matrix for Enterprise from MITRE's GitHub repository. Then, it builds a data structure that stores each technique as they key with it's parent tactic, the technique's description, and detection suggestions as a nested dictionary as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFs785QVxZEa"
   },
   "outputs": [],
   "source": [
    "# Define the local filename for the MITRE ATT&CK data\n",
    "attack_data_filename = \"enterprise-attack.json\"\n",
    "# Define the source URL for the MITRE ATT&CK Enterprise data\n",
    "attack_data_url = \"https://raw.githubusercontent.com/mitre/cti/master/enterprise-attack/enterprise-attack.json\"\n",
    "\n",
    "# Check if the MITRE ATT&CK data file already exists locally.\n",
    "# This avoids redundant downloads on subsequent runs, saving time and bandwidth.\n",
    "# It acts as a simple caching mechanism.\n",
    "if not os.path.exists(attack_data_filename):\n",
    "    log(f\"Local file '{attack_data_filename}' not found.\")\n",
    "    log (f\"Downloading MITRE ATT&CK data from {attack_data_url}\")\n",
    "    try:\n",
    "        # Attempt to download the latest enterprise ATT&CK framework definition from the official MITRE CTI GitHub repository.\n",
    "        attack_response = requests.get(attack_data_url)\n",
    "        # Raise an exception if the download request was unsuccessful (e.g., 404 Not Found, 500 Server Error).\n",
    "        attack_response.raise_for_status()\n",
    "        # Get the text content of the successful response.\n",
    "        attack_text = attack_response.text\n",
    "\n",
    "        # Parse the downloaded JSON text into a Python dictionary/list structure.\n",
    "        # This allows programmatic access to the ATT&CK data.\n",
    "        attack_json = json.loads(attack_text)\n",
    "\n",
    "        # Save the downloaded and parsed data to the local file.\n",
    "        # Using 'w' mode ensures that if the file somehow existed but os.path.exists returned False (e.g., race condition, though unlikely here),\n",
    "        # it would be overwritten. Opening with 'utf-8' encoding is generally good practice for JSON.\n",
    "        # The 'indent=4' argument makes the saved JSON file human-readable (pretty-printed).\n",
    "        with open(attack_data_filename, \"w\", encoding='utf-8') as outfile:\n",
    "            json.dump(attack_json, outfile, indent=4)\n",
    "        log(f\"Successfully downloaded and saved data to '{attack_data_filename}'.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle potential network errors during download (e.g., connection error, timeout, bad HTTP status).\n",
    "        log(f\"Error downloading ATT&CK data: {e}\")\n",
    "        # Depending on the application's needs, might want to exit, retry, or proceed without the data.\n",
    "        # For now, we'll just print the error and potentially fail later when MitreAttackData is initialized.\n",
    "    except json.JSONDecodeError as e:\n",
    "        # Handle cases where the downloaded content is not valid JSON.\n",
    "        log(f\"Error parsing downloaded JSON data: {e}\")\n",
    "    except IOError as e:\n",
    "        # Handle potential errors during file writing (e.g., permissions denied, disk full).\n",
    "        log(f\"Error writing data to file '{attack_data_filename}': {e}\")\n",
    "\n",
    "# Initialize the MitreAttackData object using the local JSON file.\n",
    "# This object provides methods to easily query and navigate the ATT&CK framework data\n",
    "# (e.g., find techniques, tactics, mitigations, relationships).\n",
    "# It encapsulates the logic for interacting with the raw JSON data structure.\n",
    "# Ensure the file exists before attempting to initialize, or handle potential FileNotFoundError within MitreAttackData.\n",
    "if os.path.exists(attack_data_filename):\n",
    "    try:\n",
    "        mad = MitreAttackData(attack_data_filename)\n",
    "        log(f\"MitreAttackData object initialized successfully from '{attack_data_filename}'.\")\n",
    "        # Now the 'mad' object can be used to work with the ATT&CK data.\n",
    "        # Example (assuming methods exist):\n",
    "        # techniques = mad.get_techniques()\n",
    "        # tactic_details = mad.get_tactic_by_id('TA0001')\n",
    "    except Exception as e:\n",
    "        # Catch any potential errors during MitreAttackData initialization (e.g., file format issues not caught earlier, internal errors).\n",
    "        log(f\"Error initializing MitreAttackData from '{attack_data_filename}': {e}\")\n",
    "        # Set mad to None or handle appropriately to indicate failure.\n",
    "        mad = None\n",
    "else:\n",
    "    # This branch is reached if the download/file writing failed in the 'if not os.path.exists' block.\n",
    "    log(f\"Cannot initialize MitreAttackData: File '{attack_data_filename}' is missing and could not be created.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the script has retrieved the MITRE ATT&CK Matrix for Enterprise, transform it into a new data structure for ease of reference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcTpHcfGyBcb"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary mapping lowercase tactic names (e.g., \"initial access\")\n",
    "# to their corresponding MITRE ATT&CK Tactic IDs (e.g., \"TA0001\").\n",
    "# This provides a quick lookup for tactic IDs based on names found in technique data.\n",
    "# It assumes the first external reference in a tactic object contains the desired ID.\n",
    "tactic_name_to_id = {\n",
    "    # Convert tactic name to lowercase for case-insensitive matching later.\n",
    "    tactic.name.lower(): tactic.external_references[0][\"external_id\"]\n",
    "    # Iterate through all tactic objects provided by the MitreAttackData instance.\n",
    "    for tactic in mad.get_tactics()\n",
    "}\n",
    "\n",
    "# Initialize the dictionary that will store the processed technique/sub-technique information.\n",
    "# The keys will be formatted strings like \"T1548.002 - Unsecured Credentials\",\n",
    "# and the values will be dictionaries containing tactic, description, and detection details.\n",
    "technique_dict = {}\n",
    "\n",
    "# Retrieve both techniques and sub-techniques from the MitreAttackData instance.\n",
    "# Exclude any techniques/sub-techniques marked as revoked or deprecated in the source data.\n",
    "# Combine them into a single list for uniform processing.\n",
    "all_techniques = mad.get_techniques(remove_revoked_deprecated=True) + \\\n",
    "                 mad.get_subtechniques(remove_revoked_deprecated=True)\n",
    "\n",
    "# Iterate through each technique or sub-technique object in the combined list.\n",
    "for tech in all_techniques:\n",
    "    # Extract the primary ATT&CK ID (e.g., \"T1548\" or \"T1548.002\") using the helper function.\n",
    "    tid = extract_attack_id_from_stix(tech)\n",
    "    # Skip processing if a valid ATT&CK ID couldn't be extracted.\n",
    "    if not tid:\n",
    "        continue # Or add logging/error handling\n",
    "\n",
    "    # Extract the name, remove leading/trailing whitespace, and replace slashes\n",
    "    # (which might cause issues in file paths or keys) with hyphens.\n",
    "    name = tech.get(\"name\", \"\").strip().replace(\"/\", \"-\")\n",
    "\n",
    "    # Create a unique, human-readable key for the technique_dict.\n",
    "    full_key = f\"{tid} - {name}\"\n",
    "\n",
    "    # Extract the description and detection details, removing leading/trailing whitespace.\n",
    "    # Use .get() with a default empty string to prevent errors if keys are missing.\n",
    "    description = tech.get(\"description\", \"\").strip()\n",
    "    # 'x_mitre_detection' is a custom STIX property used by MITRE for detection guidance.\n",
    "    detection = tech.get(\"x_mitre_detection\", \"\").strip()\n",
    "\n",
    "    tactic_names = []\n",
    "    # Iterate through the kill chain phases associated with the technique/sub-technique.\n",
    "    # Default to an empty list if 'kill_chain_phases' is missing.\n",
    "    for phase in tech.get(\"kill_chain_phases\", []):\n",
    "        # Ensure we are looking at phases belonging to the 'mitre-attack' framework\n",
    "        # (as opposed to potential other frameworks like 'mitre-pre-attack').\n",
    "        if phase.get(\"kill_chain_name\") == \"mitre-attack\":\n",
    "            # Extract the tactic's phase name (e.g., \"initial-access\").\n",
    "            # Convert it to lowercase and replace hyphens with spaces (e.g., \"initial access\")\n",
    "            # to match the keys created for the 'tactic_name_to_id' dictionary.\n",
    "            phase_name_lookup_key = phase.get(\"phase_name\", \"\").lower().replace(\"-\", \" \")\n",
    "\n",
    "            # Look up the Tactic ID (e.g., \"TA0001\") using the prepared lookup key.\n",
    "            tactic_id = tactic_name_to_id.get(phase_name_lookup_key)\n",
    "\n",
    "            # If a corresponding tactic ID was found...\n",
    "            if tactic_id:\n",
    "                # Format the tactic information string (e.g., \"TA0001 - Initial Access\").\n",
    "                # Use .title() for consistent capitalization of the tactic name.\n",
    "                tactic_display_name = phase.get(\"phase_name\", \"\").replace(\"-\", \" \").title()\n",
    "                tactic_names.append(f\"{tactic_id} - {tactic_display_name}\")\n",
    "\n",
    "    # Join the collected tactic strings into a single comma-separated string.\n",
    "    # Use set() to remove potential duplicates and sorted() for consistent ordering.\n",
    "    tactic_str = \", \".join(sorted(set(tactic_names)))\n",
    "    # --- End Tactic Processing ---\n",
    "\n",
    "    # Store the extracted and formatted information in the main dictionary.\n",
    "    technique_dict[full_key] = {\n",
    "        \"tactic\": tactic_str,\n",
    "        \"description\": description,\n",
    "        \"detection\": detection,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0cuxRiPRyWk"
   },
   "outputs": [],
   "source": [
    "# Debugging block\n",
    "# 'technique_dict' now contains the processed data, ready for further use\n",
    "# (e.g., writing to a file, displaying in a UI, further analysis).\n",
    "# print(json.dumps(technique_dict[\"T1133 - External Remote Services\"], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoaTfH2A3j3a"
   },
   "source": [
    "## Build Prompts to Generate Initial Analytic Plans\n",
    "\n",
    "This section generates a prompt to build the initial analytic plan for each technique in the MITRE ATT&CK matrix. It first defines a base prompt that provides context to the model, and then generates a specific prompt crafted for each individual technique. The complete prompts are stored in `prompt_library`, where the key is the technique ID and name in the form \"T1190 - Exploit Public-Facing Application\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P991nceT3mxP"
   },
   "outputs": [],
   "source": [
    "# This prompt generates the initial analytic plan.\n",
    "base_prompt = \"\"\"\\\n",
    "Generate an Analytic Scheme of Maneuver (ASOM) based on the following definitions of its components. For each component, provide specific examples relevant to a scenario where we suspect a sophisticated external threat is attempting to gain unauthorized access to our organization's sensitive data or negatively impact the network.\n",
    "\n",
    "ASOM Component Definitions:\n",
    "\n",
    "1.  Information Requirement (IR): These identify the information about the enemy or the terrain that the commander considers most important. For example, \"Has the adversary gained initial access to the network?\" These should be tagged with MITRE ATT&CK tactic numbers; for example, (TA0001 - Initial Access). A complete PIR looks like this: \"Has the adversary gained initial access to the network? (TA0001 - Initial Access)\".\n",
    "\n",
    "2.  Indicators: These are positive or negative evidence of threat activity pertaining to one or more information requirements. They are observable clues related to a specific information requirement. For the IR \"Has the adversary gained initial access to the network? (TA0001 - Initial Access)\", the indicator should be a technique within that tactic; for example, \"T1190 - Exploit Public Facing Application\".\n",
    "\n",
    "3.  Evidence: This is the concrete information that supports or refutes an indicator. It provides the \"proof\" and can vary in complexity. For the IR \"Has the adversary gained initial access to the network?\" and the indicator \"T1190 - Exploit Public Facing Application\" beneath it, appropriate evidence could be:\n",
    "    * \"Anomalous login attempts from unusual geographic locations.\"\n",
    "    * \"Network traffic involving known malicious command and control (C2) infrastructure.\"\n",
    "\n",
    "4.  Data: This describes the precise data necessary to identify evidence. Specificity here is key (e.g., Zeek Conn logs, Sysmon event ID 4624, Active Directory security logs). For the evidence, focus your ASOM on the following data sources: network logs, specifically Zeek logs; host logs, specifically Windows Event IDs. Write only the data name. For example, Windows Event ID 4688; Zeek conn.log\n",
    "\n",
    "5. Data Platform: This describes the type of system from which the data can be collected. It should be one of the following options: \"Endpoints\", \"Servers\", \"Network devices\".\n",
    "\n",
    "6.  Named Areas of Interest (NAIs): These are areas where data that will satisfy a specific information requirement can be collected. For all NAIs, use a dummy value of \"Insert site-specific NAI here\".\n",
    "\n",
    "7.  Actions: These are high-level instructions that guide the analysts' search for evidence. Focus mostly on simple detections, but also look for opportunities to incorporate basic statistical methods data science techniques here, such as descriptive statistics, Inter‑quartile range & box‑plots, entropy measures, correlation analysis, linear regression, time series analysis, and other, similar methods. For the evidence above, appropriate actions could be:\n",
    "    * \"Identify all source IP addresses for failed and successful login events. Geolocate logain sources, then identify rare source countries by low frequency of occurence or percentiles to flag anomalous login events from unusual geographic locations.\"\n",
    "    * \"Inner join a list of source and destination IP addresses from public-facing devices with a list of known malicious command and control (C2) servers from a cyber threat intelligence provider updated within at least 30 days. Investigate all sessions where connections involved C2 servers.\"\n",
    "\n",
    "Based on these definitions, please generate a detailed ASOM in the JSON format. The keys in the JSON object should correspond to the following ASOM components in this order: IR, Indicators, Evidence, Data Platform, Data, NAIs, Actions. Note that the key for IR should be replaced by the actual IR based on the description above. Also note that the key for evidence should be replaced with the actual form of evidence based on the description above. Here is an example template:\n",
    "\n",
    "{\n",
    "    \"(Insert IR here)\": {\n",
    "        \"Indicators\": {\n",
    "            \"(Insert form of evidence here)\": {\n",
    "                \"Data\": \"\",\n",
    "                \"Data Platform\": \"\",\n",
    "                \"NAI\": \"\",\n",
    "                \"Action\": \"\"\n",
    "            },\n",
    "            \"(Insert form of evidence here)\": {\n",
    "                \"Data\": \"\",\n",
    "                \"Data Platform\": \"\",\n",
    "                \"NAI\": \"\",\n",
    "                \"Action\": \"\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "For each MITRE technique below, generate one PIR (a general question tagged with the parent tactic's T-code, in the format \"Has the adversary gained initial access to the network? (TA0001 - Initial Access)\") for each tactic. For example, if there is one parent tactic, generate one PIR; if there are two parent tactics, generate two PIRs. For each PIR, generate two indicators (the MITRE technique provided). For each indicator, generate one form of evidence. Each form of evidence should have Data, NAI, and Actions. I have also provided a helpful description of the technique labeled \"Description:\", and a potential idea for detecting it, labeled \"Detection:\".\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Adb__dv3uGz"
   },
   "outputs": [],
   "source": [
    "prompt_library = {}\n",
    "\n",
    "for technique in technique_dict:\n",
    "    prompt_library[technique] = base_prompt + f\"\\n\\nTechnique: {technique}\\n\\nTactic(s): {technique_dict[technique]['tactic']}\\n\\nDescription: {technique_dict[technique]['description']}\\n\\nDetection: {technique_dict[technique]['detection']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGOJP-skQZRA"
   },
   "outputs": [],
   "source": [
    "# print(prompt_library[\"T1190 - Exploit Public-Facing Application\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0WI4hfst9lt"
   },
   "source": [
    "## Generate the Initial Analytic Plans\n",
    "\n",
    "This section generates one PIR and two indicators (with evidence, data, data platform, NAI, and action) for each technique in the MITRE ATT&CK matrix for Enterprise. Note that if a technique may be used at multiple points in an operation, as part of more than one tactic, a unique PIR is generated for each instance of that technique.\n",
    "\n",
    "First, define a function with embeded context for generating the analytic plans. This function is unique to the initial generation of analytic plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYbX1wmst9lt"
   },
   "outputs": [],
   "source": [
    "def generate_initial_plan(prompt, target_model):\n",
    "    \"\"\"Generates an analytic plan in JSON format using a generative AI model.\n",
    "\n",
    "    This function constructs a detailed, multi-turn prompt including definitions, examples,\n",
    "    and constraints to guide the target generative model in creating an analytic plan tailored\n",
    "    to a specific cyber threat scenario and MITRE ATT&CK technique. It leverages\n",
    "    few-shot prompting by providing a complete example (for T1133) within the prompt context.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): A string containing the specific details of the MITRE ATT&CK\n",
    "                      technique for which the ASOM should be generated. This typically\n",
    "                      includes Technique ID, Name, Tactics, Description, and Detection guidance.\n",
    "        target_model (str): The name or identifier of the generative AI model to use\n",
    "                            (e.g., \"models/gemini-1.5-flash\").\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the generated ASOM in JSON format. Returns the raw text\n",
    "             response from the model. Error handling for API calls or invalid JSON\n",
    "             responses is not explicitly included here.\n",
    "    \"\"\"\n",
    "    # Initialize the Generative AI client using the API key stored in environment variables.\n",
    "    # Ensure the 'GEMINI_API_KEY' environment variable is set before running.\n",
    "    client = genai.Client(\n",
    "        api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
    "    )\n",
    "\n",
    "    # Set the target model for the generation request.\n",
    "    model = target_model\n",
    "\n",
    "    # Construct the conversation history ('contents') for the generative model.\n",
    "    # This uses a multi-turn approach to provide detailed instructions and examples (few-shot learning).\n",
    "    contents = [\n",
    "        # Turn 1: User provides detailed instructions, definitions, constraints, and a full example.\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[\n",
    "                # This large text block defines the ASOM structure, components (IR, Indicators, etc.),\n",
    "                # the desired JSON output format, data source constraints (Zeek, Win Events),\n",
    "                # NAI placeholder requirements, action guidance (including data science methods),\n",
    "                # and crucially, provides a complete example scenario based on T1133.\n",
    "                # --- START OF DETAILED PROMPT TEXT (DO NOT MODIFY CONTENT) ---\n",
    "                types.Part.from_text(text=\"\"\"Generate an Analytic Scheme of Maneuver (ASOM) based on the following definitions of its components. For each component, provide specific examples relevant to a scenario where we suspect a sophisticated external threat is attempting to gain unauthorized access to our organization's sensitive data or negatively impact the network.\n",
    "\n",
    "ASOM Component Definitions:\n",
    "\n",
    "1.  Information Requirement (IR): These identify the information about the enemy or the terrain that the commander considers most important. For example, \\\"Has the adversary gained initial access to the network?\\\" These should be tagged with MITRE ATT&CK tactic numbers; for example, (TA0001 - Initial Access). A complete PIR looks like this: \\\"Has the adversary gained initial access to the network? (TA0001 - Initial Access)\\\".\n",
    "\n",
    "2.  Indicators: These are positive or negative evidence of threat activity pertaining to one or more information requirements. They are observable clues related to a specific information requirement. For the IR \\\"Has the adversary gained initial access to the network? (TA0001 - Initial Access)\\\", the indicator should be a technique within that tactic; for example, \\\"T1190 - Exploit Public Facing Application\\\".\n",
    "\n",
    "3.  Evidence: This is the concrete information that supports or refutes an indicator. It provides the \\\"proof\\\" and can vary in complexity. For the IR \\\"Has the adversary gained initial access to the network?\\\" and the indicator \\\"T1190 - Exploit Public Facing Application\\\" beneath it, appropriate evidence could be:\n",
    "    * \\\"Anomalous login attempts from unusual geographic locations.\\\"\n",
    "    * \\\"Network traffic involving known malicious command and control (C2) infrastructure.\\\"\n",
    "\n",
    "4.  Data: This describes the precise data necessary to identify evidence. Specificity here is key (e.g., Zeek Conn logs, Sysmon event ID 4624, Active Directory security logs). For the evidence, focus your ASOM on the following data sources: network logs, specifically Zeek logs; host logs, specifically Windows Event IDs. Write only the data name. For example, Windows Event ID 4688; Zeek conn.log\n",
    "\n",
    "5. Data Platform: This describes the type of system from which the data can be collected. It should be one of the following options: \\\"Endpoints\\\", \\\"Servers\\\", \\\"Network devices\\\".\n",
    "\n",
    "6.  Named Areas of Interest (NAIs): These are areas where data that will satisfy a specific information requirement can be collected. For all NAIs, use a dummy value of \\\"Insert site-specific NAI here\\\".\n",
    "\n",
    "7.  Actions: These are high-level instructions that guide the analysts' search for evidence. Focus mostly on simple detections, but also look for opportunities to incorporate basic statistical methods data science techniques here, such as descriptive statistics, Inter‑quartile range & box‑plots, entropy measures, correlation analysis, linear regression, time series analysis, and other, similar methods. For the evidence above, appropriate actions could be:\n",
    "    * \\\"Identify all source IP addresses for failed and successful login events. Geolocate logain sources, then identify rare source countries by low frequency of occurence or percentiles to flag anomalous login events from unusual geographic locations.\\\"\n",
    "    * \\\"Inner join a list of source and destination IP addresses from public-facing devices with a list of known malicious command and control (C2) servers from a cyber threat intelligence provider updated within at least 30 days. Investigate all sessions where connections involved C2 servers.\\\"\n",
    "\n",
    "Based on these definitions, please generate a detailed ASOM in the JSON format. The keys in the JSON object should correspond to the following ASOM components in this order: IR, Indicators, Evidence, Data Platform, Data, NAIs, Actions. Note that the key for IR should be replaced by the actual IR based on the description above. Also note that the key for evidence should be replaced with the actual form of evidence based on the description above. Here is an example template:\n",
    "\n",
    "{\n",
    "    \\\"(Insert IR here)\\\": {\n",
    "        \\\"Indicators\\\": {\n",
    "            \\\"(Insert form of evidence here)\\\": {\n",
    "                \\\"Data\\\": \\\"\\\",\n",
    "                \\\"Data Platform\\\": \\\"\\\",\n",
    "                \\\"NAI\\\": \\\"\\\",\n",
    "                \\\"Action\\\": \\\"\\\"\n",
    "            },\n",
    "            \\\"(Insert form of evidence here)\\\": {\n",
    "                \\\"Data\\\": \\\"\\\",\n",
    "                \\\"Data Platform\\\": \\\"\\\",\n",
    "                \\\"NAI\\\": \\\"\\\",\n",
    "                \\\"Action\\\": \\\"\\\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "For each MITRE technique below, generate one PIR (a general question tagged with the parent tactic's T-code, in the format \\\"Has the adversary gained initial access to the network? (TA0001 - Initial Access)\\\") for each tactic. For example, if there is one parent tactic, generate one PIR; if there are two parent tactics, generate two PIRs. For each PIR, generate two indicators (the MITRE technique provided). For each indicator, generate one form of evidence. Each form of evidence should have Data, NAI, and Actions. I have also provided a helpful description of the technique labeled \\\"Description:\\\", and a potential idea for detecting it, labeled \\\"Detection:\\\".\n",
    "\n",
    "Technique: T1133 - External Remote Services\n",
    "\n",
    "Tactic: TA0001 - Initial-Access, TA0003 - Persistence\n",
    "\n",
    "Description: Adversaries may leverage external-facing remote services to initially access and/or persist within a network. Remote services such as VPNs, Citrix, and other access mechanisms allow users to connect to internal enterprise network resources from external locations. There are often remote service gateways that manage connections and credential authentication for these services. Services such as [Windows Remote Management](https://attack.mitre.org/techniques/T1021/006) and [VNC](https://attack.mitre.org/techniques/T1021/005) can also be used externally.(Citation: MacOS VNC software for Remote Desktop)\n",
    "\n",
    "Access to [Valid Accounts](https://attack.mitre.org/techniques/T1078) to use the service is often a requirement, which could be obtained through credential pharming or by obtaining the credentials from users after compromising the enterprise network.(Citation: Volexity Virtual Private Keylogging) Access to remote services may be used as a redundant or persistent access mechanism during an operation.\n",
    "\n",
    "Access may also be gained through an exposed service that doesn’t require authentication. In containerized environments, this may include an exposed Docker API, Kubernetes API server, kubelet, or web application such as the Kubernetes dashboard.(Citation: Trend Micro Exposed Docker Server)(Citation: Unit 42 Hildegard Malware)\n",
    "\n",
    "Detection: Follow best practices for detecting adversary use of [Valid Accounts](https://attack.mitre.org/techniques/T1078) for authenticating to remote services. Collect authentication logs and analyze for unusual access patterns, windows of activity, and access outside of normal business hours.\n",
    "\n",
    "When authentication is not required to access an exposed remote service, monitor for follow-on activities such as anomalous external use of the exposed API or application.\"\"\"),\n",
    "                # --- END OF DETAILED PROMPT TEXT ---\n",
    "            ],\n",
    "        ),\n",
    "        # Turn 2: Model provides its internal plan for generating the example (T1133).\n",
    "        # This helps guide the model's reasoning process for the actual prompt later.\n",
    "        types.Content(\n",
    "            role=\"model\",\n",
    "            parts=[\n",
    "                # This text simulates the model outlining its step-by-step plan to generate the ASOM for T1133.\n",
    "                # --- START OF MODEL PLAN TEXT (DO NOT MODIFY CONTENT) ---\n",
    "                types.Part.from_text(text=\"\"\"The user wants an Analytic Scheme of Maneuver (ASOM) in JSON format.\n",
    "The ASOM should follow a specific structure: IR -> Indicators -> Evidence -> Data, Data Platform, NAI, Action.\n",
    "The IR key should be the actual Information Requirement string.\n",
    "The Evidence key should be the actual evidence string.\n",
    "The scenario involves detecting a sophisticated external threat attempting unauthorized access or network impact.\n",
    "I need to generate PIRs, Indicators, Evidence, Data, Data Platform, NAI, and Actions based on the provided technique T1133 - External Remote Services.\n",
    "\n",
    "**Plan:**\n",
    "\n",
    "1.  **Identify Tactics:** T1133 belongs to TA0001 (Initial Access) and TA0003 (Persistence).\n",
    "2.  **Formulate PIRs:** Create one PIR for each tactic, using the general question format provided and the tactic T-code.\n",
    "    * PIR 1 (TA0001): \\\"Has the adversary gained initial access to the network via external remote services? (TA0001 - Initial Access)\\\"\n",
    "    * PIR 2 (TA0003): \\\"Is the adversary maintaining persistence using external remote services? (TA0003 - Persistence)\\\"\n",
    "3.  **Define Indicators:** For each PIR, use the provided technique T1133 as the indicator. Since the user asked for *two* indicators per PIR, but only provided *one* technique (T1133), I will use T1133 twice for each PIR, but focus on different aspects or detection methods derived from the description and detection notes.\n",
    "    * PIR 1 -> Indicator 1: T1133 - External Remote Services (focus on login anomalies)\n",
    "    * PIR 1 -> Indicator 2: T1133 - External Remote Services (focus on exposed services)\n",
    "    * PIR 2 -> Indicator 1: T1133 - External Remote Services (focus on persistent unusual access)\n",
    "    * PIR 2 -> Indicator 2: T1133 - External Remote Services (focus on redundant access mechanisms)\n",
    "4.  **Develop Evidence:** For each indicator, create a specific form of evidence based on the detection notes.\n",
    "    * PIR1/Indicator1 (T1133 - Login Anomalies): \\\"Anomalous login attempts to external remote services (VPN, Citrix, RDP) from unusual geographic locations or outside normal business hours.\\\"\n",
    "    * PIR1/Indicator2 (T1133 - Exposed Services): \\\"Anomalous external usage patterns targeting potentially exposed remote services (e.g., Docker API, Kubernetes API) that do not require authentication.\\\"\n",
    "    * PIR2/Indicator1 (T1133 - Persistent Unusual Access): \\\"Successful logins to external remote services exhibiting unusual access patterns (e.g., multiple failed logins followed by success, logins at odd hours consistently over time) matching known Valid Accounts.\\\"\n",
    "    * PIR2/Indicator2 (T1133 - Redundant Access): \\\"Connections via external remote services originating from IP addresses associated with other known suspicious activities or previously compromised accounts.\\\"\n",
    "5.  **Specify Data Sources:** Identify the specific logs needed for each piece of evidence, focusing on Windows Event IDs and Zeek logs as requested.\n",
    "    * Evidence 1 (Login Anomalies): Windows Event ID 4624, Windows Event ID 4625, VPN Logs, Citrix Logs, Zeek conn.log\n",
    "    * Evidence 2 (Exposed Services): Zeek conn.log, Web Server Access Logs (if applicable for K8s dashboard etc.)\n",
    "    * Evidence 3 (Persistent Unusual Access): Windows Event ID 4624, Windows Event ID 4625, VPN Logs, Citrix Logs\n",
    "    * Evidence 4 (Redundant Access): Zeek conn.log, VPN Logs, Citrix Logs, Threat Intel Feeds\n",
    "6.  **Define Data Platform:** Assign the appropriate platform (\\\"Endpoints\\\", \\\"Servers\\\", \\\"Network devices\\\").\n",
    "    * Evidence 1: Servers (for AD/Event Logs), Network devices (VPN/Citrix/Zeek)\n",
    "    * Evidence 2: Network devices (Zeek), Servers (Web server logs)\n",
    "    * Evidence 3: Servers (for AD/Event Logs), Network devices (VPN/Citrix)\n",
    "    * Evidence 4: Network devices (Zeek/VPN/Citrix), Servers (Threat Intel platform)\n",
    "7.  **Set NAI:** Use the placeholder \\\"Insert site-specific NAI here\\\".\n",
    "8.  **Create Actions:** Define actions to find the evidence, incorporating basic analysis and statistical methods where applicable.\n",
    "    * Evidence 1 Action: \\\"Collect remote service authentication logs (Windows Event ID 4624, 4625, VPN/Citrix). Identify source IP addresses for successful and failed logins. Geolocate source IPs. Calculate frequency distribution of source countries/regions and login times. Flag logins from low-frequency locations or occurring outside typical business hours (e.g., using IQR or percentile analysis on login times).\\\"\n",
    "    * Evidence 2 Action: \\\"Monitor Zeek conn.log for connections to known ports associated with exposed services (e.g., Docker API 2375/2376, K8s API 6443/443, kubelet 10250). Analyze connection volume, source IPs, and request patterns. Use time series analysis to detect sudden spikes or changes in traffic patterns from external IPs to these services.\\\"\n",
    "    * Evidence 3 Action: \\\"Correlate successful logins (Event ID 4624, VPN/Citrix logs) with preceding failed attempts (Event ID 4625). Analyze login timestamps for patterns inconsistent with normal user behavior (e.g., logins consistently occurring late at night or on weekends). Use descriptive statistics to establish baseline login behavior per user/group and identify deviations.\\\"\n",
    "    * Evidence 4 Action: \\\"Cross-reference source IP addresses from remote service logs (Zeek conn.log, VPN/Citrix logs) against a list of IP addresses identified in other security alerts or known compromised host lists. Inner join connection logs with Threat Intelligence feeds for known malicious IPs connecting via remote services.\\\"\n",
    "9.  **Assemble JSON:** Construct the final JSON object following the specified structure and order. Use the PIRs as the top-level keys. Use the evidence strings as the keys under \\\"Indicators\\\".\"\"\"),\n",
    "                # --- END OF MODEL PLAN TEXT ---\n",
    "                # This provides the expected JSON output for the T1133 example, acting as a strong few-shot example.\n",
    "                # --- START OF MODEL EXAMPLE JSON OUTPUT (DO NOT MODIFY CONTENT) ---\n",
    "                types.Part.from_text(text=\"\"\"[\n",
    "  {\n",
    "    \\\"Has the adversary gained initial access to the network via external remote services? (TA0001 - Initial Access)\\\": {\n",
    "      \\\"Indicators\\\": {\n",
    "        \\\"T1133 - External Remote Services\\\": {\n",
    "          \\\"Anomalous login attempts to external remote services (VPN, Citrix, RDP) from unusual geographic locations or outside normal business hours.\\\": {\n",
    "            \\\"Data\\\": \\\"Windows Event ID 4624, Windows Event ID 4625, Zeek conn.log\\\",\n",
    "            \\\"Data Platform\\\": \\\"Servers, Network devices\\\",\n",
    "            \\\"NAI\\\": \\\"Insert site-specific NAI here\\\",\n",
    "            \\\"Action\\\": \\\"Collect remote service authentication logs (Windows Event ID 4624, 4625, VPN/Citrix). Identify source IP addresses for successful and failed logins. Geolocate source IPs. Calculate frequency distribution of source countries/regions and login times. Flag logins from low-frequency locations or occurring outside typical business hours (e.g., using IQR or percentile analysis on login times).\\\"\n",
    "          },\n",
    "          \\\"Anomalous external usage patterns targeting potentially exposed remote services (e.g., Docker API, Kubernetes API) that do not require authentication.\\\": {\n",
    "            \\\"Data\\\": \\\"Zeek conn.log\\\",\n",
    "            \\\"Data Platform\\\": \\\"Network devices\\\",\n",
    "            \\\"NAI\\\": \\\"Insert site-specific NAI here\\\",\n",
    "            \\\"Action\\\": \\\"Monitor Zeek conn.log for connections to known ports associated with exposed services (e.g., Docker API 2375/2376, K8s API 6443/443, kubelet 10250). Analyze connection volume, source IPs, and request patterns. Use time series analysis to detect sudden spikes or changes in traffic patterns from external IPs to these services.\\\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \\\"Is the adversary maintaining persistence using external remote services? (TA0003 - Persistence)\\\": {\n",
    "      \\\"Indicators\\\": {\n",
    "        \\\"T1133 - External Remote Services\\\": {\n",
    "          \\\"Successful logins to external remote services exhibiting unusual access patterns (e.g., multiple failed logins followed by success, logins at odd hours consistently over time) matching known Valid Accounts.\\\": {\n",
    "            \\\"Data\\\": \\\"Windows Event ID 4624, Windows Event ID 4625\\\",\n",
    "            \\\"Data Platform\\\": \\\"Servers\\\",\n",
    "            \\\"NAI\\\": \\\"Insert site-specific NAI here\\\",\n",
    "            \\\"Action\\\": \\\"Correlate successful logins (Event ID 4624, VPN/Citrix logs) with preceding failed attempts (Event ID 4625). Analyze login timestamps for patterns inconsistent with normal user behavior (e.g., logins consistently occurring late at night or on weekends). Use descriptive statistics to establish baseline login behavior per user/group and identify deviations.\\\"\n",
    "          },\n",
    "          \\\"Connections via external remote services originating from IP addresses associated with other known suspicious activities or previously compromised accounts.\\\": {\n",
    "            \\\"Data\\\": \\\"Zeek conn.log\\\",\n",
    "            \\\"Data Platform\\\": \\\"Network devices\\\",\n",
    "            \\\"NAI\\\": \\\"Insert site-specific NAI here\\\",\n",
    "            \\\"Action\\\": \\\"Cross-reference source IP addresses from remote service logs (Zeek conn.log, VPN/Citrix logs) against a list of IP addresses identified in other security alerts or known compromised host lists. Inner join connection logs with Threat Intelligence feeds for known malicious IPs connecting via remote services.\\\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "]\"\"\"),\n",
    "                # --- END OF MODEL EXAMPLE JSON OUTPUT ---\n",
    "            ],\n",
    "        ),\n",
    "        # Turn 3: User provides the actual prompt containing the target technique details.\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[\n",
    "                # The 'prompt' variable passed into the function contains the technique details\n",
    "                # for which the ASOM should be generated this time.\n",
    "                types.Part.from_text(text=prompt),\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Configure the generation request.\n",
    "    generate_content_config = types.GenerateContentConfig(\n",
    "        # Specify that the model's response should be formatted as JSON.\n",
    "        response_mime_type=\"application/json\",\n",
    "        # Provide system instructions, reinforcing the desired JSON structure.\n",
    "        # This repeats the template definition from the initial user message for emphasis.\n",
    "        system_instruction=[\n",
    "            # --- START OF SYSTEM INSTRUCTION JSON TEMPLATE (DO NOT MODIFY CONTENT) ---\n",
    "            types.Part.from_text(text=\"\"\"Based on these definitions, please generate a detailed ASOM in the JSON format. The keys in the JSON object should correspond to the following ASOM components in this order: IR, Indicators, Evidence, Data Platform, Data, NAIs, Actions. Note that the key for IR should be replaced by the actual IR based on the description above. Also note that the key for evidence should be replaced with the actual form of evidence based on the description above. Here is an example template:\n",
    "\n",
    "{\n",
    "    \\\"(Insert IR here)\\\": {\n",
    "        \\\"Indicators\\\": {\n",
    "            \\\"(Insert form of evidence here)\\\": {\n",
    "                \\\"Data\\\": \\\"\\\",\n",
    "                \\\"Data Platform\\\": \\\"\\\",\n",
    "                \\\"NAI\\\": \\\"\\\",\n",
    "                \\\"Action\\\": \\\"\\\"\n",
    "            },\n",
    "            \\\"(Insert form of evidence here)\\\": {\n",
    "                \\\"Data\\\": \\\"\\\",\n",
    "                \\\"Data Platform\\\": \\\"\\\",\n",
    "                \\\"NAI\\\": \\\"\\\",\n",
    "                \\\"Action\\\": \\\"\\\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\"\"\"),\n",
    "            # --- END OF SYSTEM INSTRUCTION JSON TEMPLATE ---\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Send the request to the generative model.\n",
    "    response = client.models.generate_content(\n",
    "        model=model,              # The specified target model\n",
    "        contents=contents,        # The constructed multi-turn conversation history\n",
    "        config=generate_content_config, # Configuration including response format and system instructions\n",
    "    )\n",
    "\n",
    "    # Return the text content of the model's response, which should be the generated ASOM JSON.\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will go through multiple iterations of generating and refining these analytic plans, we'll create a wrapper function `prompt_model` that makes it easy to map a collection of prompts to a specific generator function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_model(requests_per_minute: int,\n",
    "                 max_iterations: int,\n",
    "                 prompts_collection: dict,\n",
    "                 generation_function,\n",
    "                 output_directory: str = \".\",\n",
    "                 model_name: str = \"gemini-2.5-flash-preview-04-17\"): # Default model, can be overridden\n",
    "    \"\"\"\n",
    "    Processes a collection of prompts using a specified generation function,\n",
    "    while respecting rate limits and iteration caps.\n",
    "\n",
    "    Args:\n",
    "        requests_per_minute: The target maximum number of requests allowed per minute.\n",
    "        max_iterations: The maximum number of successful generation requests to perform.\n",
    "        prompts_collection: A dictionary where keys are identifiers (e.g., technique names)\n",
    "                            and values are the prompts themselves.\n",
    "        generation_function: A function reference that takes a prompt and a model_name\n",
    "                             and returns the generated content.\n",
    "                             Example: def my_generator(prompt, target_model): ... return response\n",
    "        output_directory: The directory where output files will be saved. Defaults to current dir.\n",
    "        model_name: The name or identifier of the model to be used by the generation_function.\n",
    "    \"\"\"\n",
    "    if requests_per_minute <= 0:\n",
    "        min_seconds_between_requests = 0\n",
    "    else:\n",
    "        min_seconds_between_requests = 60 / requests_per_minute\n",
    "\n",
    "    last_request_timestamp = 0  # Timestamp of the last request start\n",
    "    successful_requests_count = 0 # Counter for successful requests\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "        log(f\"Created output directory: {output_directory}\")\n",
    "\n",
    "    log(f\"Starting prompt processing. Max iterations: {max_iterations}, RPM: {requests_per_minute}.\")\n",
    "\n",
    "    for item_key, prompt_content in prompts_collection.items():\n",
    "        # Construct the expected output filename based on the item key.\n",
    "        # Ensures that the filename is safe for filesystems.\n",
    "        safe_item_key = \"\".join(c if c.isalnum() or c in (' ', '.', '_') else '_' for c in str(item_key)).rstrip()\n",
    "        output_filename = f\"{safe_item_key}.json\" # Assuming JSON output, adjust as needed\n",
    "        output_file_path = os.path.join(output_directory, output_filename)\n",
    "\n",
    "        # Check if a file with this name already exists in the output directory.\n",
    "        if os.path.exists(output_file_path):\n",
    "            log(f\"Skipping '{item_key}': {output_file_path} already exists.\")\n",
    "            continue  # Skip to the next item\n",
    "\n",
    "        # Rate limiting check\n",
    "        if min_seconds_between_requests > 0:\n",
    "            current_time = time.time()\n",
    "            elapsed_time_since_last_request = current_time - last_request_timestamp\n",
    "\n",
    "            if elapsed_time_since_last_request < min_seconds_between_requests:\n",
    "                wait_time = min_seconds_between_requests - elapsed_time_since_last_request\n",
    "                log(f\"Rate limit check: Waiting for {wait_time:.2f} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "\n",
    "        log(f\"Processing item #{successful_requests_count + 1}/{max_iterations}). Generating content for '{item_key}'...\")\n",
    "\n",
    "        # Record the time just before starting the generative API call.\n",
    "        last_request_timestamp = time.time()\n",
    "        try:\n",
    "            # Call the external function to generate content\n",
    "            response_content = generation_function(prompt_content, target_model=model_name)\n",
    "\n",
    "            # Increment the successful request counter *only after* the API call\n",
    "            # returns without raising an exception.\n",
    "            successful_requests_count += 1\n",
    "\n",
    "            # Open the target output file in write mode ('w').\n",
    "            with open(output_file_path, \"w\") as f:\n",
    "                f.write(response_content) # Assumes response_content is a string\n",
    "\n",
    "            log(f\"Successfully generated content for '{item_key}' and saved as {output_file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log(f\"Error generating content for '{item_key}': {e}\")\n",
    "            # Option to stop the entire loop: break\n",
    "            # Option to continue with the next item: continue\n",
    "            # Current implementation stops on error:\n",
    "            log(\"Stopping processing due to an error.\")\n",
    "            break\n",
    "\n",
    "        # Check if the number of *successful requests* has reached the set limit.\n",
    "        if successful_requests_count >= max_iterations:\n",
    "            log(f\"Generation limit of {max_iterations} successful requests reached. Exiting.\")\n",
    "            break\n",
    "\n",
    "    log(\"Prompt processing finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, generate the analytic plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_model(\n",
    "    requests_per_minute = 10,\n",
    "    max_iterations = 3,\n",
    "    prompts_collection = prompt_library,\n",
    "    generation_function = generate_initial_plan,\n",
    "    output_directory = \"./\",\n",
    "    model_name = \"gemini-2.5-flash-preview-04-17\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a33sJPiKNlI8",
    "outputId": "689b7a0d-65c0-450d-cb50-0c319295f6c1"
   },
   "outputs": [],
   "source": [
    "# # Defines the target maximum number of requests allowed per minute.\n",
    "# REQUESTS_PER_MINUTE = 10\n",
    "\n",
    "# # Calculates the minimum required time separation (in seconds) between the start\n",
    "# # of consecutive requests to stay within the defined rate limit.\n",
    "# MIN_SECONDS_BETWEEN_REQUESTS = 60 / REQUESTS_PER_MINUTE\n",
    "\n",
    "# # Stores the timestamp (seconds since epoch) of when the last request started.\n",
    "# # Initialized to 0 to ensure the very first request in the script's run\n",
    "# # isn't unnecessarily delayed by the rate-limiting check.\n",
    "# last_request_time = 0\n",
    "# # Counts the number of successful requests made within this specific run\n",
    "# # of the script. This is used to enforce the `limit`.\n",
    "# request_count_in_loop = 0\n",
    "\n",
    "# # Sets a maximum limit on the number of successful generation requests\n",
    "# # (`generate_initial_plan` calls that don't raise an immediate exception)\n",
    "# # that this script run will perform. This is set to 500 due to API request\n",
    "# # limits for the Gemini free tier.\n",
    "# limit = 500\n",
    "\n",
    "# # Iterate through each technique name (key) in the prompt library.\n",
    "# for technique in prompt_library:\n",
    "#     # Construct the expected output filename based on the technique name.\n",
    "#     output_filename = f\"{technique}.json\"\n",
    "#     # Check if a file with this name already exists in the current directory.\n",
    "#     if os.path.exists(output_filename):\n",
    "#         # If the file exists, log a message indicating it's being skipped.\n",
    "#         log(f\"Skipping {technique}: {output_filename} already exists.\")\n",
    "#         continue # Skip to the next technique\n",
    "\n",
    "#     # Get the current time before potentially making a request.\n",
    "#     current_time = time.time()\n",
    "#     # Calculate the time elapsed since the start of the last request.\n",
    "#     elapsed_time = current_time - last_request_time\n",
    "\n",
    "#     # Check if the elapsed time is less than the minimum required separation.\n",
    "#     if elapsed_time < MIN_SECONDS_BETWEEN_REQUESTS:\n",
    "#         # If too soon, calculate the necessary waiting time.\n",
    "#         wait_time = MIN_SECONDS_BETWEEN_REQUESTS - elapsed_time\n",
    "#         # Log that the script is pausing for rate limiting.\n",
    "#         log(f\"Rate limit check: Waiting for {wait_time:.2f} seconds...\")\n",
    "#         # Pause the script's execution for the calculated duration.\n",
    "#         time.sleep(wait_time)\n",
    "\n",
    "#     # Log the attempt to process the current technique, showing its progress\n",
    "#     # relative to the `limit` for this run.\n",
    "#     log(f\"Processing #{request_count_in_loop + 1}/{limit}). Started generating plan for {technique} ...\")\n",
    "\n",
    "#     # Record the time just before starting the generative API call.\n",
    "#     # This ensures the rate limit is based on request initiation frequency.\n",
    "#     last_request_time = time.time()\n",
    "#     try:\n",
    "#         # Call the external function to generate the plan using the prompt\n",
    "#         # associated with the current technique and the specified model.\n",
    "#         response = generate_initial_plan(prompt_library[technique], target_model = \"gemini-2.5-flash-preview-04-17\") # Example model\n",
    "\n",
    "#         # Increment the successful request counter *only after* the API call\n",
    "#         # returns without raising an exception.\n",
    "#         request_count_in_loop += 1\n",
    "\n",
    "#         # Open the target output file in write mode ('w').\n",
    "#         with open(output_filename, \"w\") as f:\n",
    "#             # Write the content received from the `generate_initial_plan` function to the file.\n",
    "#             f.write(response)\n",
    "        \n",
    "#         # Log the successful generation and saving of the plan.\n",
    "#         log(f\"Successfully generated plan for '{technique}' and saved as {output_filename}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         # If any exception occurs during the `generate_initial_plan` call:\n",
    "#         # Log the error message, including the technique that failed.\n",
    "#         log(f\"Error generating plan for {technique}: {e}\")\n",
    "#         # Stop the entire loop immediately upon encountering an error.\n",
    "#         # Alternative strategies could be to `continue` (skip the failed technique)\n",
    "#         # or implement retry logic.\n",
    "#         break # Stop processing further techniques on error\n",
    "\n",
    "#     # Check if the number of *successful requests* made in this run has reached the set limit.\n",
    "#     if request_count_in_loop >= limit:\n",
    "#         # If the limit is reached, log a message explaining why the loop is ending.\n",
    "#         log(f\"Generation limit of {limit} reached. Exiting.\")\n",
    "#         # Stop the loop.\n",
    "#         break\n",
    "\n",
    "# # Log a final message indicating the main processing loop has finished, either by\n",
    "# # completing all techniques, hitting the generation limit, or encountering an error.\n",
    "# log(\"Processing finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoaTfH2A3j3a"
   },
   "source": [
    "## Build Prompts to Improve the Analytic Plans\n",
    "\n",
    "This section generates a prompt to build the initial analytic plan for each technique in the MITRE ATT&CK matrix. It first defines a base prompt that provides context to the model, and then generates a specific prompt crafted for each individual technique. The complete prompts are stored in `prompt_library`, where the key is the technique ID and name in the form \"T1190 - Exploit Public-Facing Application\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tHf0bsg3dW8"
   },
   "outputs": [],
   "source": [
    "base_prompt = \"\"\"\\\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t5OpEIvJHRd8",
    "outputId": "6dfda881-179c-4a7c-a423-3ff8938e98bc"
   },
   "outputs": [],
   "source": [
    "refine_prompt_library = {}\n",
    "\n",
    "for technique in technique_dict:\n",
    "    refine_prompt_library[technique] = base_prompt[0] + f\"\\n\\nTechnique: {technique}\\n\\nTactic(s): {technique_dict[technique]['tactic']}\\n\\nDescription: {technique_dict[technique]['description']}\\n\\nDetection: {technique_dict[technique]['detection']}\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the Analytic Plans\n",
    "\n",
    "This section iterates over the existing analytic plans, then uses generative artificial intelligence to improve them.\n",
    "\n",
    "First, define a function with embeded context for improving the analytic plans. This function is unique to the improvement of analytic plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_improved_plan(prompt, target_model):\n",
    "    \"\"\"Generates an improved analytic plan in JSON format using a generative AI model.\n",
    "\n",
    "    This function constructs a detailed, multi-turn prompt including definitions, examples,\n",
    "    and constraints to guide the target generative model in creating an analytic plan tailored\n",
    "    to a specific cyber threat scenario and MITRE ATT&CK technique. It leverages\n",
    "    few-shot prompting by providing a complete example (for T1133) within the prompt context.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): A string containing the specific details of the MITRE ATT&CK\n",
    "                      technique for which the ASOM should be generated. This typically\n",
    "                      includes Technique ID, Name, Tactics, Description, and Detection guidance.\n",
    "        target_model (str): The name or identifier of the generative AI model to use\n",
    "                            (e.g., \"models/gemini-1.5-flash\").\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the generated ASOM in JSON format. Returns the raw text\n",
    "             response from the model. Error handling for API calls or invalid JSON\n",
    "             responses is not explicitly included here.\n",
    "    \"\"\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, generate the improved analytic plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a33sJPiKNlI8",
    "outputId": "689b7a0d-65c0-450d-cb50-0c319295f6c1"
   },
   "outputs": [],
   "source": [
    "prompt_model(\n",
    "    requests_per_minute = 10,\n",
    "    max_iterations = 3,\n",
    "    prompts_collection = refine_prompt_library,\n",
    "    generation_function = generate_improved_plan,\n",
    "    output_directory = \"./\",\n",
    "    model_name = \"gemini-2.5-flash-preview-04-17\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0AqnybE-2yMA"
   },
   "outputs": [],
   "source": [
    "# Halt execution before the code below is run\n",
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZcDLXXqK6pAV"
   },
   "outputs": [],
   "source": [
    "def update_json_files_structure(root_dir, dict_to_add_or_update):\n",
    "    \"\"\"\n",
    "    Recursively finds JSON files in root_dir and adds or updates keys\n",
    "    from dict_to_add_or_update into the dictionary value of the primary\n",
    "    key within each item of the main list.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): The path to the directory to start searching from.\n",
    "        dict_to_add_or_update (dict): A dictionary containing key-value pairs\n",
    "                                      to add or update.\n",
    "    \"\"\"\n",
    "    if not isinstance(dict_to_add_or_update, dict):\n",
    "        log(\"Error: 'dict_to_add_or_update' must be a dictionary.\")\n",
    "        return\n",
    "\n",
    "    if not dict_to_add_or_update:\n",
    "        log(\"Warning: 'dict_to_add_or_update' is empty. No changes will be made.\")\n",
    "        # Optionally return here if desired, or proceed to scan files anyway.\n",
    "        # return\n",
    "\n",
    "    if not os.path.isdir(root_dir):\n",
    "        log(f\"Error: Directory '{root_dir}' not found.\")\n",
    "        return\n",
    "\n",
    "    log(f\"Starting scan in directory: {root_dir}\")\n",
    "    log(f\"Data to add/update: {dict_to_add_or_update}\")\n",
    "    files_processed = 0\n",
    "    files_modified = 0\n",
    "    errors_encountered = 0\n",
    "    error_files = []\n",
    "    warning_files = []\n",
    "\n",
    "    for subdir, _, files in os.walk(root_dir):\n",
    "        for filename in files:\n",
    "            if filename.lower().endswith('.json'):\n",
    "                file_path = os.path.join(subdir, filename)\n",
    "                log(f\"Processing file: {file_path}\")\n",
    "                files_processed += 1\n",
    "                modified_in_this_file = False\n",
    "                try:\n",
    "                    # Read the JSON file with UTF-8 encoding\n",
    "                    # Use a variable for the file handle to ensure it's closed\n",
    "                    # even if errors occur before json.load() finishes\n",
    "                    read_f = None\n",
    "                    try:\n",
    "                        read_f = open(file_path, 'r', encoding='utf-8')\n",
    "                        data = json.load(read_f)\n",
    "                    finally:\n",
    "                        if read_f:\n",
    "                            read_f.close()\n",
    "\n",
    "\n",
    "                    # Ensure the top level is a list\n",
    "                    if isinstance(data, list):\n",
    "                        # Iterate through each item (dictionary) in the list\n",
    "                        for item in data:\n",
    "                            if isinstance(item, dict):\n",
    "                                # Iterate through the primary keys in the item dictionary\n",
    "                                # (Assuming one primary key per item as per example)\n",
    "                                for primary_key in list(item.keys()): # Use list() for safe iteration\n",
    "                                    # Check if the value associated with the primary key is a dictionary\n",
    "                                    if isinstance(item[primary_key], dict):\n",
    "                                        target_dict = item[primary_key]\n",
    "                                        # Iterate through the keys and values to add/update\n",
    "                                        for key_to_update, value_to_update in dict_to_add_or_update.items():\n",
    "                                            # Check if the key needs to be added or if the value is different\n",
    "                                            if key_to_update not in target_dict or target_dict[key_to_update] != value_to_update:\n",
    "                                                target_dict[key_to_update] = value_to_update\n",
    "                                                modified_in_this_file = True # Mark that a change occurred\n",
    "                                    else:\n",
    "                                        log(f\"  Warning: Value for key '{primary_key}' in {file_path} is not a dictionary. Skipping update for this key.\")\n",
    "                                        warning_files.append(file_path)\n",
    "                            else:\n",
    "                                 log(f\"  Warning: Found an item in the list that is not a dictionary in {file_path}. Skipping this item.\")\n",
    "                                 warning_files.append(file_path)\n",
    "\n",
    "                    else:\n",
    "                        log(f\"  Warning: Root object in {file_path} is not a list. Skipping this file.\")\n",
    "                        warning_files.append(file_path)\n",
    "                        continue # Skip to the next file\n",
    "\n",
    "                    # Write the modified data back to the file if changes were made\n",
    "                    if modified_in_this_file:\n",
    "                        write_f = None\n",
    "                        try:\n",
    "                            write_f = open(file_path, 'w', encoding='utf-8')\n",
    "                            # Use indent for pretty printing, adjust as needed (e.g., indent=4)\n",
    "                            json.dump(data, write_f, indent=2, ensure_ascii=False)\n",
    "                            log(f\"  Successfully updated: {file_path}\")\n",
    "                            files_modified += 1\n",
    "                        finally:\n",
    "                             if write_f:\n",
    "                                 write_f.close()\n",
    "                    else:\n",
    "                        log(f\"  No changes needed for: {file_path}\")\n",
    "\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    log(f\"  Error: Invalid JSON format in {file_path}. Skipping.\")\n",
    "                    error_files.append(file_path)\n",
    "                    errors_encountered += 1\n",
    "                except IOError as e:\n",
    "                    log(f\"  Error: Could not read/write file {file_path}. Reason: {e}. Skipping.\")\n",
    "                    error_files.append(file_path)\n",
    "                    errors_encountered += 1\n",
    "                except Exception as e:\n",
    "                    # Log the type of exception for better debugging\n",
    "                    log(f\"  Error: An unexpected {type(e).__name__} occurred processing {file_path}. Reason: {e}. Skipping.\")\n",
    "                    error_files.append(file_path)\n",
    "                    errors_encountered += 1\n",
    "\n",
    "    print(\"\\n--- Processing Summary ---\")\n",
    "    print(f\"Total files scanned: {files_processed}\")\n",
    "    print(f\"Files successfully modified: {files_modified}\")\n",
    "    # Corrected calculation for skipped/unchanged files\n",
    "    skipped_unchanged = files_processed - files_modified - errors_encountered\n",
    "    print(f\"Files skipped, unchanged, or with warnings: {skipped_unchanged}\")\n",
    "    print(f\"Errors encountered during processing: {errors_encountered}\")\n",
    "    print(\"--------------------------\")\n",
    "    print(\"Error files:\\n\",error_files)\n",
    "    print()\n",
    "    print(\"Warning files:\\n\",warning_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixIytOLMiKhS"
   },
   "outputs": [],
   "source": [
    "# target_directory = \"./\"\n",
    "\n",
    "# data_to_add = {\n",
    "#     \"version\": \"1.0\",\n",
    "#     \"last_updated\": \"2025-05-04\"\n",
    "# }\n",
    "\n",
    "# # Call the generalized function\n",
    "# update_json_files_structure(target_directory, data_to_add)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
